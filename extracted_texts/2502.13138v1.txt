



AIDE: AI-Driven Exploration in the Space of Code




18 Feb 2025





ZhengyaoJiang
zhengyao@weco.ai

Dhruv Srikanth Weco AI



WecoAi

Dhruv Srikanth Weco AI


Dhruv Srikanth Weco AI


Dhruv Srikanth Weco AI


Dhruv Srikanth Weco AI


Dhruv Srikanth Weco AI



DominikSchmidt

Dhruv Srikanth Weco AI



RunwayMl

Dhruv Srikanth Weco AI



DixingXu

Dhruv Srikanth Weco AI



IanKaplan

Dhruv Srikanth Weco AI



DenissJacenko

Dhruv Srikanth Weco AI



YuxiangWu
yuxiang@weco.ai.

Dhruv Srikanth Weco AI


AIDE: AI-Driven Exploration in the Space of Code



18 Feb 2025


180CBF08CDCB4431DCEB58AE175AD7AE
arXiv:2502.13138v1[cs.AI]






GROBID - A machine learning software for extracting information from scholarly documents






Machine learning, the foundation of modern artificial intelligence, has driven innovations that have fundamentally transformed the world. Yet, behind advancements lies a complex and often tedious process requiring labor and compute intensive iteration and experimentation. Engineers and scientists developing machine learning models spend much of their time on trial-and-error tasks instead of conceptualizing innovative solutions or research hypotheses. To address this challenge, we introduce AI-Driven Exploration (AIDE), a machine learning engineering agent powered by large language models (LLMs). AIDE frames machine learning engineering as a code optimization problem, and formulates trial-anderror as a tree search in the space of potential solutions. By strategically reusing and refining promising solutions, AIDE effectively trades computational resources for enhanced performance, achieving state-of-the-art results on multiple machine learning engineering benchmarks, including our Kaggle evaluations, OpenAI's MLE-Bench and METR's RE-Bench. The implementation of AIDE is publicly available at https://github.com/WecoAI/aideml.





IntroductionMachine learning engineering supports many modern AI achievements, from basic regression on tabular data to the recent surge in large generative models. However, building a high-performance machine learning model is always time consuming. Due to the inherent stochasticity of both the data and the optimization process, engineers and scientists rely heavily on trial-and-error. Researchers have long sought to automate these iterative processes, leading to advancements in fields like Au-toML (Feurer et al., 2015(Feurer et al., , 2020;;LeDell and Poirier, 2020a;Olson and Moore, 2016;Jin et al., 2023Jin et al., , 2019;;Thornton et al., 2013a,b;Mueller and et al., 2024), Neural Architecture Search (Zoph and Le, 2017;Pham et al., 2018;Liu et al., 2019;Real et al., 2019;Elsken et al., 2019), and hyperparameter optimization (Falkner et al., 2018;Yang and Shami, 2020). These methods typically require a predefined search space of configurations, such as hyperparameters and network architectures, within which the algorithm explores potential solutions (Elsken et al., 2019;Yang and Shami, 2020;White et al., 2023). Defining this space often requires significant domain expertise. Furthermore, search algorithms for hyperparameter tuning are often somewhat brute force compared to human experts, resulting in lower compute efficiency and a risk of overfitting to the validation set.The emergence of advanced coding capabilities in large language models (LLMs) (OpenAI, 2023;Jimenez et al., 2024;Anthropic, 2024;Google, 2024;Jain et al., 2025;OpenAI, 2025a,b) has introduced an exciting new possibility: searching directly within the space of code rather than the space of predefined configurations. Code-space optimization offers greater flexibility and leverages the extensive domain-specific knowledge inherent in LLMs, effectively narrowing the search to more promising solutions and thus boosting sample efficiency. This gives it the potential to address compute-bound tasks like deep learning or, presumably, even optimizing LLMs themselves.Here, we introduce AI-Driven Exploration (AIDE), a LLM-powered agent * that automates the trialand-error process of machine learning engineering. Unlike the ReACT (Yao et al., 2023) style agent, which appends historical observations to the LLM's context and relies on the model's capabilities to solve a monolithic optimization problem, AIDE organizes all historical solutions in a tree structure. It then asks the LLM to propose improvements based on individual tree nodes. A hard-coded treesearch algorithm accumulates these incremental improvements, guided by automated evaluations.We benchmarked AIDE on a set of Kaggle tasks focusing on tabular machine learning and released these initial results together in April 2024 (Weco AI, 2024). Subsequently, OpenAI released MLE-Bench (Chan et al., 2024), further showing that AIDE can be applied to even more challenging deep learning tasks from Kaggle while achieving state-of-the-art performance. Most notably, AIDE achieves twice the number of medals compared to a follow-up agent (Wang et al., 2024) when both use GPT-4o; with o1-preview, the gap widens even further. In parallel, METR assessed AIDE on AI research tasks against human experts under time constraints, showing that AIDE can outperform expert-crafted solutions in limited time windows (METR, 2024). Moreover, for tasks with a robust evaluation signal like Triton Kernel optimization, AIDE's final solution surpasses that of human experts, even when the latter had extended development time.The first half of this paper provides a formal specification of AIDE for the research community. In the second half, we present and analyze empirical evaluations of AIDE, drawing on both our own experiments and independent benchmark results.
PreliminariesMany general-purpose LLM agents, including ReACT (Yao et al., 2023), frame their tasks as Partially Observable Markov Decision Processes (POMDPs) (Kaelbling et al., 1998), a widely used framework in reinforcement learning. In a POMDP, the agent tries to maximize a cumulative reward by choosing actions based on all past observations, essentially treating the entire interaction history as the state. While this approach is flexible and unifies a range of tasks, it lacks a principled way to break down the problem when there is a clear structure available. Moreover, for LLM-based agents, continually appending all historical data can lead to oversized prompts and limit scalability, because the model's context window eventually fills up.In this work, we adopt an alternative framework for LLM-driven iterative problem solving by modeling the task as an optimization problem: Let S be a space of possible solutions (e.g., Python scripts), and let h : S → R be a stateless objective function (for example, validation accuracy or loss). The goal is to find an optimal solution:s * = arg max s∈S h(s).(1) Each candidate solution s can be evaluated independently via an objective function h(s). This perspective simplifies the problem considerably: rather than unrolling a single, long-horizon decision process , we can directly evaluate and compare solutions. It also aligns naturally with existing optimization methods, like tree search, which depend on standalone evaluations of candidate solutions.
MethodologyIn this section, we introduce our approach to automating machine learning engineering with AIDE. By employing the tree search method, AIDE systematically explores solutions that optimize validation metrics, breaking down the monolithic optimization task into atomic improvement steps. We begin by outlining the high-level optimization algorithm. And then delve into key implementation details, such as the search policy and specialized prompts that drive the iterative generation and refinement of machine learning code.Algorithm 1 AI-Driven Exploration (AIDE) 1: Initialize: solution tree T 0 ← ∅ 2: Initialize: base solution s ← s 0 3: for n = 1, 2, ..., N do 4:s n ← f s, Σ(T n−1 ) ▷ Propose a new solution 5: v n ← h(s n ) ▷ Evaluate the solution 6: T n ← T n−1 ∪ {node (s n , v n ), edge (s → s n )}▷ Record node and its score 7:s ← π(T n ) ▷ Select the next base node 8: end for 9: return argmax s ′ ∈{s0,...,s N } h(s ′ ) ▷ Best solution found
AI-Driven Exploration in the Space of SolutionsIn AIDE, a solution s is the code to be optimized, with s 0 denoting the empty root solution. An evaluator, h : S → R, evaluates the code and provides a scalar score. All discovered solutions are stored in a solution tree, T , whose nodes correspond to scripts and edges represent an improvement attempt (e.g., s → s ′ is an improvement of s). A search policy, π(T ), selects which solution s ∈ T will serve as the base solution to be improved. To keep language model prompts concise while being aware of the historical attempts, a summarization operator, Σ(T ), extracts relevant information from the tree, such as the high level idea of each improvement attempt and its corresponding performance metrics. Finally, a coding operator, f s, Σ(T ) , proposes new scripts by drafting an initial version from s 0 , fixing bugs, or refining a promising solution based on the summarized context.With these components in place, AIDE can systematically explore the code solution space, as shown in Algorithm 1.
AIDE for Machine LearningHere we present more implementation details of AIDE for machine learning engineering, providing a concrete instantiation of the core components from Section 3.1. In particular, we build upon the following design elements:Search Policy (π). In AIDE, the search policy π ( algorithm 1, line 7) follows a simple hard-coded rule, determining whether to draft, debug, or improve based on an existing solution. Specifically, it selects:• Drafting if we have not yet reached the desired number of initial solutions.• Debugging if a buggy node remains within a certain debug depth.• Improving otherwise, typically targeting the best (non-buggy) solution.This policy imposes practical heuristics, such as 1) first exploring a set of diverse initial solutions and continuously improving the best one, and 2) constraining the number of debug attempts for a broken solution.Coding Operator (f ). The coding operator has three main entry points, each with its own specialized prompts:• Drafting, which is invoked when we need a completely new solution from scratch. It prompts an LLM to outline a brief plan for a model (e.g., specifying a particular network architecture or feature-engineering idea), then emits a single-file Python program implementing that plan. • Debugging, which focuses on repairing buggy solutions. By inspecting error logs and execution traces, it attempts to rectify issues in the code like broken imports, incorrect tensor dimensions, or other coding errors while preserving the overall approach. • Improving, which is called when a valid, non-buggy solution already exists but could benefit from data preprocessing, architectural or optimization modifications. Here, the LLMs 0 s 1 s 2 s 3 s 4 s 5 s 6 s 7 f : draft f : draft f : draft f : fix f : fix f : improve f : fix Empty Solution Bug DetectedValid Solution
Optimal SolutionFigure 1: A sample solution tree T for AIDE, where each node is a Python script. Arrows represent transitions proposed by the coding operator f . Some branches terminate in a bug, while others lead to improved or optimal solutions.proposes exactly one "atomic" change, such as switching optimizers or adding a regularization technique, so that its effect on performance is directly measurable.Combining these three operations keeps the solution tree structured and ensures that each new node arises from a well-defined modification of a parent node.Summarization Operator (Σ(T )). Despite the flexibility to generate arbitrarily large numbers of solutions, we avoid saturating the LLM's prompt by applying a context summarization operator, Σ(T ). Instead of appending all historical logs, Σ(T ) selectively extracts:• Performance metrics (e.g., accuracy, AUC-ROC, test set loss).• Hyperparameter settings if a solution involves a hyperparameter sweep.• Relevant hints for debugging (e.g., misaligned array shapes in tracebacks).A concise summary is crucial to maintaining a stateless perspective: each code revision stands on its own, but Σ(T ) uses prior information to guide subsequent proposals. This design offers much of the benefit of incremental reasoning without exploding the prompt size.Data Preview in Coding Prompts. In addition to dynamic updates from Σ(T ), AIDE for machine learning includes a small static "data preview" in each prompt, giving the LLM basic knowledge of dataset size or feature layouts. In practice, we store relevant metadata (e.g., number of rows, column names, or data splits) in the workspace and insert it into the coding operator's prompt. Although not a complete EDA pipeline, this lightweight approach helps AIDE guide key code decisions. These decisions include selecting a validation split or scaling hyperparameters, without repeatedly including extensive dataset context.Putting It All Together. Figure 1 illustrates how AIDE's instantiation for machine learning uses (i) a search policy π to select which solution to refine next, (ii) a coding operator f for generating code by drafting, debugging, or improving solutions, and (iii) a summarization operator Σ(T ) to keep the LLM prompts concise and targeted. By combining these components under a stateless optimization framework, AIDE can systematically search within the space of possible code solutions for machine learning tasks, avoiding an ever-increasing prompt history while retaining the relevant knowledge needed to achieve high performance.
EvaluationIn this section we report empirical evaluations of AIDE. We did our own evaluation on Kaggle competitions with a focus on tabular machine learning tasks (Weco AI, 2024). On the other hand, after the open sourcing of the AIDE in April 2024, the community has done larger scale independent evaluations showing promising results on deep learning (Chan et al., 2024) and AI R&D (METR, 2024) tasks. We therefore also aggregate relevant results here to provide a better understanding of the AIDE's performance. Readers interested in the extended evaluations are encouraged to read and cite the papers from OpenAI (Chan et al., 2024) and METR (2024) respectively.
Weco Kaggle BenchmarkWe curated a diverse set of Kaggle competitions to build Weco's internal Kaggle benchmark, called Weco-Kaggle, for evaluating AIDE's performance in machine learning. This set consists of 63 competitions of varied complexity and data size, spanning domains such as tabular machine learning, image classification, and time-series prediction. Some of these competitions require a GPU to solve. Full details of the competitions in Weco-Kaggle are provided in Appendix C. From Weco-Kaggle, we selected a subset of 16 tabular machine learning tasks with relatively lower complexity and primarily CPU-based runtime requirements. This subset, referred to as Weco-Kaggle Lite, is shown in Table 2.Evaluation Protocol. We evaluate the performance of AIDE by comparing its results to that of human competitors in each Kaggle competition, and averaging across competitions. We follow the evaluation protocol below to evaluate AIDE's and other frameworks' performance:1. Before running the agent on a competition, we split the competition's training data into an agent train set and a holdout test set. This split is defined manually for each competition following similar parameters as Kaggle's official private test set (e.g. similar train-test percentages), but is not necessarily the same, since Kaggle's test set is not released publicly for most competitions. Note that our holdout test set is also distinct from the train-validation split that AIDE itself generates as part of its internal node evaluation protocol.2. During code generation, AIDE is given access to the holdout test inputs (but not labels) and prompted to evaluate its model on this data. In particular, we prompt AIDE to generate a submission.csv file, analogously to how human competitors submit their competition results.3. We define an Exceeds % of Human metric as 100(1 − q), where q is the quantile of AIDE's score on the official Kaggle leaderboard. This metric represents the percentage of human competitors whose performance AIDE surpasses. Whenever possible, we use Kaggle's private leaderboard because it is less prone to overfitting by competitors; if a private leaderboard is unavailable, we default to the public leaderboard. In addition, we report the Above Median metric, originally proposed by Chan et al. (2024), which indicates how frequently AIDE outperforms the median Kaggler performance across competitions.4. This metric is then averaged across all competitions.We chose our evaluation protocol based on leaderboard-quantiles since, unlike each competition's included metric, these scores are similarly distributed between competitions, making it possible to simply average across competitions to obtain aggregated scores. Leaderboard quantiles are also more fine-grained, allowing us to evaluate, for example, the performance of a single run on a single task, unlike medal-counts (Chan et al., 2024) which collapse to a binary metric in this case. Finally, our scores are interpretable and useful in assessing AIDE's performance relative to humans. Baselines. To evaluate AIDE's effectiveness, we compare it against three baselines that illustrate different approaches to automated or assisted machine learning:1. Conventional H2O AutoML. We select H2O, one of the leading AutoML platforms, to exemplify traditional AutoML tools. In each competition, the data is split into an 80%/20% train/validation set, and model selection is performed within a 600-second search window.
2.AutoGPT. A workflow automation framework that surged in popularity in early 2024. It generates a plan and automatically executes the necessary steps to complete a task. We adapt its task descriptor to produce solutions for our competitions.3. Human Assisted with ChatGPT. An increasingly common scenario involves human engineers leveraging ChatGPT to assist with coding tasks. We adopt this baseline to understand how AIDE performs relative to a human engineer directing ChatGPT to develop solutions.These baselines collectively provide a robust comparative foundation for evaluating AIDE against both traditional AutoML workflows and modern LLM-assisted strategies. Further details about the baselines' configuration can be found in Appendix A.  Table 3: Full MLE-Bench results (pass@1) reported by (Chan et al., 2024) comparing AIDE to other agent frameworks. Valid Subm. (%) is the fraction of all competitions (not just those with a submission) where the submission passed validity checks. Above Median (%) is the fraction of competitions where the score was strictly above the median of human Kaggle participants. Any Medal (%) is the fraction awarded a bronze, silver, or gold medal (the primary success metric).Each experiment is repeated with 3 seeds, except for AIDE+o1-preview and AIDE+GPT-4o, which use 16 and 36 seeds respectively. Scores represent the mean ± one standard error of the mean.Potential Limitations. Despite the advantages discussed above, our protocol has some limitations. First, because our test set may differ from Kaggle's private test set, scores may not always be directly comparable, which can result in variance in percentiles. Second, there is a risk of contamination since some of the language models used in this work may have been trained on competition-related data. Although we found no significant correlation between agent performance and competition recency, the only way to fully ensure no data contamination would be to submit the agent's solutions to live competitions.
AIDE in MLE-BenchMLE-Bench (Chan et al., 2024) is an offline evaluation framework comprising 75 real Kaggle competitions. Here, we present the results related to AIDE reported by Chan et al. (2024) and encourage readers to check and cite the original paper if they are interested in the results presented here. In these evaluations, AIDE emerged as the top-performing agent framework when paired with state-ofthe-art large language models. Other agent frameworks such as ResearchAgent from MLAB (Huang et al., 2024) and OpenHands (Wang et al., 2024) tended to terminate early or struggle with iterative refinement. AIDE's optimization-centric approach led to better scalability in terms of trial-and-error interactions, therefore higher valid-submission rates and ultimately more competition medals.  AIDE's Key Advantages. By explicitly implementing a solution tree search strategy, AIDE keeps node-level code concise and focuses each language model call on a localized problem (e.g. debugging only the most promising script). This design helps avoid oversized prompts, preserves a clear performance record for each node, and repeatedly refines partial solutions over the entire 24-hour timeframe. Consequently, AIDE systematically addresses coding bugs and suboptimal hyperparameters rather than abandoning failed solutions. As shown in Table 3, these iterative improvements translate into higher medal acquisition rates in comparison to generic agents.Moreover, when given additional attempts per competition (increasing k in pass@k), AIDE significantly increases its success rate; for instance, GPT-4o and o1-preview nearly double their medals from pass@1 to pass@6 (Chan et al., 2024). These observations underscore the specialized nature of AIDE, which often outperforms other agents through persistent, Kaggle-style iteration, highlighting the efficacy of a competition-targeted design in real-world ML tasks.The impact of AIDE becomes particularly evident when comparing performance on the MLE-bench Lite subset, as shown in Figure 3. Using o1-preview with AIDE significantly improved performance across all metrics compared to using o1-preview alone. The valid submission rate increased from 63.6% ± 4.5% to 92.4% ± 2.6%, demonstrating AIDE's effectiveness in guiding the model through the submission process. More importantly, the fraction of solutions scoring above the median human performance increased dramatically from 13.6% to 59.1% ± 4.5%, and both medal-related metrics showed substantial improvements: the gold medal achievement rate more than tripled from 6.1% ± 2.6% to 21.2% ± 6.9%, while the overall medal achievement rate increased nearly fivefold from 7.6% ± 2.6% to 36.4% ± 7.9%. These improvements are statistically significant (p < 0.01 for all metrics, two-tailed t-test). The dramatic performance gains across all metrics demonstrate that AIDE's iterative optimization approach substantially enhances the model's problem-solving capabilities, enabling more reliable and higher-quality solutions through systematic refinement.
AIDE in RE-BenchWhile AIDE is designed for building machine learning pipelines, METR applied it to much more challenging AI R&D tasks by formulating these tasks into optimization tasks. The tasks range from optimizing a Triton Kernel to finetuning GPT-2 for QA. Surprisingly, AIDE performs quite well on these tasks, and is even comparable with the top human AI scientists from Google DeepMind, Google, Anthropic, OpenAI, FAR Labs, Redwood Research, University of California Berkeley, Carnegie Mellon University, Stanford University, or Massachusetts Institute of Technology (METR, 2024).Figure 4: Average score achieved by AIDE+o1-preview and top human scientists on 7 AI R&D tasks, as report by METR (2024). AIDE managed to surpass human scientists within six hours by enabling faster experiment iterations. However, human scientists eventually caught up, as AIDE adopts a simple greedy policy that may lead to local optima on challenging R&D tasks.Figure 4 illustrates AIDE's average performance over time across the seven RE-Bench environments. Since LLMs can implement solutions much faster, allowing for more iteration cycles, AIDE managed to outperform humans within the six-hour time limit. Notably, the agent exceeded human performance in Optimize a Kernel, discovering a custom Triton-based solution faster than any of the nine human experts did within 64 hours. However, AIDE fell short in environments that required handling larger codebases or where a single improvement involved multiple steps of interaction. For example, in Agent for Rust CodeContests, AIDE was prone to repeating local patches instead of discovering new strategies.5 Related Work
LLM AgentsRecent advances in large language models have spurred the development of agents that combine natural language reasoning with task execution. General-purpose agents such as ReAct (Yao et al., 2023) and HuggingGPT (Shen et al., 2023) interleave planning with action selection to perform tasks ranging from information retrieval to multi-modal processing. In contrast, specialized agents like Voyager (Wang et al., 2023) and AlphaCode (Li and et al., 2022) are tailored to specific domains such as embodied reasoning and competitive code generation. These systems integrate execution feedback into the LLM's reasoning process, enabling iterative refinement of candidate solutions.
Automated Machine LearningAutomated Machine Learning (AutoML) aims to eliminate manual intervention in model selection, hyperparameter tuning, and pipeline configuration. Early frameworks such as Auto-WEKA (Thornton et al., 2013b) and TPOT (Olson and Moore, 2016) employed Bayesian optimization and genetic programming, respectively, to search over predefined model spaces. Later systems like Auto-Sklearn (Feurer et al., 2020) and AutoGluon (Mueller and et al., 2024) have leveraged meta-learning and ensemble techniques to further improve performance. Despite their success, many conventional AutoML methods rely on static search spaces and lack the dynamic adaptability required for more complex problem settings.
Neural Architecture SearchNeural Architecture Search (NAS) focuses on automatically designing neural network topologies. Initial methods based on reinforcement learning (Zoph and Le, 2017) and evolutionary strategies (Real et al., 2019) demonstrated that automated search could yield competitive architectures. Differentiable approaches such as DARTS (Liu et al., 2019) have reduced the computational cost by enabling gradient-based optimization over a relaxed search space. However, NAS still faces challenges in computational expense and search space design. AIDE, on the other hand, avoids such problems above with code space search and efficient design exploration powered by LLMs.
ConclusionIn conclusion, we have presented AI-Driven Exploration (AIDE), an LLM Agent for machine learning engineering. By systematically drafting, debugging, and refining solutions, AIDE achieves superior performance on Kaggle tasks as well as on more research-oriented benchmarks. While developed for tabular machine learning tasks, third-party experiments show that this approach can generalize to challenges such as neural architecture search, Triton Kernel optimization, and other AI R&D tasks. We believe AIDE represents a promising step toward the future of automated ML engineering, offering a principled way to combine iterative LLM prompting with a tree-based exploration of code solutions. 6. Stacked Ensembles (including an ensemble of all base models and ensembles using subsets of the base models)
A Baseline SpecificationsIt then performs a random search over a predefined grid of hyperparameter combinations, avoiding the computational expense of an exhaustive grid search. After training individual models, H2O AutoML creates stacked ensembles by combining the predictions of the best-performing models from each algorithm. This ensemble method leverages the strengths of multiple models to improve overall performance. All trained models, including individual models and ensembles, are evaluated using cross-validation and ranked based on performance metrics such as accuracy, AUC, or RMSE, depending on the problem type. The configurations are shown in Table 4.
A.2 AutoGPT BaselineWe use the LangChain implementation of AutoGPT, which includes LangChain primitives such as PromptTemplates, VectorStores, and Embeddings. Inspired by Huang et al. (2024), we introduce a task descriptor for AutoGPT in each competition to provide a basic task planner and minimize human intervention. The task descriptor includes information retrieved from the Kaggle page, such as the dataset description, file details (train.csv, test.csv, sample_submission.csv), evaluation metrics, submission file format, and a sample training script. An example task descriptor is shown in Figure 5.We also provide the agent with tools to read and write files, list directories, and run Python REPL evaluations. The agent reads the task descriptor with predefined goals, as shown below:  Figure 3 :3Figure 3: Performance of o1-preview with and without AIDE on MLE-bench Lite (complexity=low) set.
Figure 5 :Figure 6 :56Figure 5: The task descriptor for bike-sharing-demand Task Descriptor Prompt

Table 1 :1Comparing AIDE to other agent frameworks on 16 tabular machine learning tasks from Kaggle. Exceeds % of humans indicates the percentage of human Kaggle participants being outperformed by the agents, averaged across the competitions. Above Median (%) is the fraction of competitions where the score was strictly above the median of human Kaggle participants.AgentModelExceeds % of humans ↑ Above Median (%) ↑AIDEGPT-4 Turbo51.3850.00AutoML (H2O)N/A35.3418.75AutoGPT (Langchain) GPT-4 Turbo32.340.00Human with ChatGPT GPT-4 Turbo41.1718.75
Table 2 :2AIDE vs. human performance comparison on Weco-Kaggle Lite. The submissions were made manually in February 2024. All rankings are actual rankings on the private/public Kaggle leaderboard, assessed in February 2024.AIDE's Results on Weco-Kaggle Lite. Table1compares AIDE against multiple baselines, including H2O AutoML, AutoGPT, and a human competitor utilizing ChatGPT, averaged over the 16 tabular Kaggle tasks of Weco-Kaggle Lite. AIDE achieves an Exceeds % of humans score of 51.38%, outperforming half of the Kaggle participants on average, and surpasses the human median in 50% of these tasks. By contrast, H2O AutoML and LangChain AutoGPT attain lower Exceeds % of humans scores (35.34% and 32.34%, respectively). Table2offers a detailed breakdown for each competition, indicating that AIDE's performance ranges from surpassing roughly 13% of human participants (for more challenging tasks) to nearly 92% (for tasks it handles more effectively). Across half of the competitions, AIDE ranks above the human median, underscoring its robustness in consistently delivering competitive results against a diverse set of real-world machine learning challenges. Figure 2: AIDE's performance distribution on full Weco-Kaggle benchmark. Exceeds % of Humans values are estimated from the leaderboard distribution.AIDE's Results on Full Weco-Kaggle. Figure 2 illustrates AIDE's performance distributionacross our extended set of Kaggle competitions, sorted by its Exceeds % of Humans value. Notably,AIDE achieves near-top-tier performance on several tasks, surpassing the vast majority of humanparticipants, while on other tasks it exceeds only a small fraction. Overall, the average Exceeds % ofHumans rate is 48.23%, and AIDE outperforms the human median in 49.21% of the competitions.These results underscore that AIDE can be highly competitive in certain domains, yet there remains variability in its performance depending on the dataset and task requirements.
Table 33highlights key results of AIDE compared to other agents. The reported Any Medal (%) column shows the fraction of competitions on which the agent and model combination achieved ao1-previewo1-preview + AIDE100%92.4% ±2.6%80%60%63.6% ±4.5%59.1% ±4.5%40%36.4% ±7.9%20%±0% 13.6%6.1% ±2.6% 21.2% ±6.9%±2.6% 7.6%0%Valid SubmissionsAbove MedianGold MedalAny Medalmedal (bronze, silver, or gold) in a single pass (i.e. pass@1). AIDE with o1-preview earned medals in 16.9% of competitions, nearly four times that of the follow-up agent OpenHands.
Table 4 :4Baseline hyperparameters.AutoGPTParameterValueagentLangChain AutoGPTmodelgpt-4-0125-previewseed1max_runtime 600Human with ChatGPTParameterValuemodelgpt-4-0125-previewA.1 H2O AutoML BaselineThe machine learning algorithm selection process of H2O AutoML LeDell and Poirier (2020b)proceeds as follows. First, it searches over a set of six algorithms:1. Distributed Random Forest (DRF) and Extremely Randomized Trees (XRT)2. Generalized Linear Model (GLM) with regularization3. XGBoost4. H2O Gradient Boosting Machines5. Fully connected multi-layer artificial neural network (DeepLearning)
* In this paper, "Agent" may refer either to the algorithm built on top of LLMs or to the entire system with LLMs included, depending on the context.



Goal PromptGo through task_descriptor . txt to understand the task and evaluation method . Iterate over different models or feature selections to get a better performance based on evaluation method . You can use following steps as reference : 1. Select a model and fill in the provided python snippet . 
A.3 ChatGPT with Human AssistanceA human operator is tasked with solving a Kaggle competition using only the information provided in the overview and data tabs, which include the available dataset. The operator is permitted to utilize the ChatGPT web interface. The LLM is set to gpt-4-0125-preview in comparison with Auto-GPT. Due to limitations in ChatGPT's capabilities, such as the potential for generating hallucinated results and occasionally using outdated packages, iterative interactions are required. The human operator will continue to issue instructions until a valid submission is produced. Upon completion, the operator submits the results to Kaggle, where the submission is ranked against the competition leaderboard.
B Analysis of AIDE B.1 Code Complexity GrowthIn Figure 6, we observe that the aggregated code complexity (combining LOC, LLOC, Volume, N1, and MI) exhibits an overall increasing trend as the number of iterative steps grows. Initially, there is a slight dip in complexity, but after the first step, the metrics begin a generally steady rise. This suggests that as AIDE (GPT-4 Turbo) produces successive iterations of code, the solutions tend to become more elaborate, with additional lines of code and logical structures contributing to higher values for traditional software complexity measures. The progressive increase implies that, over multiple generation steps, the model accumulates more intricate functionality-potentially reflecting deeper problem-solving processes or additional features-leading to an increasingly complex codebase by the final iteration.
B.2 Cost AnalysisFigure 7 illustrates the per-task LLM inference cost for AIDE across the Weco-Kaggle benchmark, using GPT-4 Turbo (gpt-4-0125-preview) with pricing data from early 2024. Although certain tasks incur higher costs due to more extensive prompting (up to approximately $2.50 per task), the majority remain under $1.50, reflecting moderate token usage and minimal manual intervention. Overall, these expenditures are much lower than the investment required for human experts or conventional AutoML services, especially when considering the significant performance gains achieved by AIDE's fully automated design. Moreover, as language model costs continue to decline, AIDE's approach becomes increasingly competitive in terms of both performance and budget.
C Weco Kaggle Benchmark 




Claude 3.5 sonnet model card addendum

Anthropic



2024
Anthropic


Technical report




JSChan


NChowdhury


OJaffe


JAung


DSherburn


EMays


GStarace


KLiu


LMaksin


TPatwardhan


LWeng


AĄdry

arXiv:2410.07095
MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering

2024


arXiv preprint



Neural architecture search: A survey

TElsken


JHMetzen


FHutter



Journal of Machine Learning Research

20
55

2019





BOHB: Robust and Efficient Hyperparameter Optimization at Scale

SFalkner


AKlein


FHutter



Proc. of ICML
of ICML

2018
35





Efficient and Robust Automated Machine Learning

MFeurer


AKlein


KEggensperger


JTSpringenberg


MBlum


FHutter



Proc. of NeurIPS
of NeurIPS

2015
28






MFeurer


KEggensperger


EBergman


FPfisterer


BBischl


FHutter

arXiv:2007.04074
Auto-Sklearn 2.0: Hands-free AutoML via Meta-Learning

2020


arXiv preprint



Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context

Google

arXiv:2403.05530

2024


arXiv preprint



Mlagentbench: Evaluating language agents on machine learning experimentation

QianHuang


JianVora


PercyLiang


JureLeskovec



Forty-first International Conference on Machine Learning

2024





Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code

NamanJain


KingHan


AlexGu


Wen-DingLi


FanjiaYan


TianjunZhang


SidaWang




The Thirteenth International Conference on Learning Representations

2025





SWE-bench: Can language models resolve real-world github issues?

CarlosEJimenez


JohnYang


AlexanderWettig


ShunyuYao


KexinPei


;Karthik R Narasimhan




The Twelfth International Conference on Learning Representations

Ofir Press
2024





Auto-Keras: An Efficient Neural Architecture Search System

HJin


QSong


XHu



Proc. of ACM SIGKDD (KDD)
of ACM SIGKDD (KDD)

2019





AutoKeras: An AutoML Library for Deep Learning

HJin


FChollet


QSong


XHu



Journal of Machine Learning Research

24
6

2023





Planning and acting in partially observable stochastic domains

LesliePack


KaelblingMichael L Littman


AnthonyRCassandra



Artificial Intelligence

101
1-2

1998





H2O AutoML: Scalable Automatic Machine Learning

ELedell


SPoirier



Proc. of the AutoML Conference
of the AutoML Conference

2020a






H2O AutoML: Scalable automatic machine learning

ErinLedell


SebastienPoirier




7th ICML Workshop on Automated Machine Learning (AutoML)

July 2020b





Competition-level Code Generation with AlphaCode

YLi

10.1126/science.abq1158


Science

378

2022





DARTS: Differentiable Architecture Search

HLiu


KSimonyan


YYang




Proc. of ICLR, 2019. METR. Evaluating frontier AI R&D capabilities of language model agents against human experts
of ICLR, 2019. METR. Evaluating frontier AI R&D capabilities of language model agents against human experts

November 2024





AutoGluon: AutoML for Text, Image, and Tabular Data

JMueller



Scientific Reports

14
1
72889
2024





TPOT: A Tree-based Pipeline Optimization Tool for Automating Machine Learning

RSOlson


JHMoore



ICML AutoML Workshop

2016





OpenAI. Openai o1 system card


2023. 2025a


OpenAI ; OpenAI ; OpenAI


Technical report
Gpt-4 technical report



Openai o3-mini system card

Openai



2025b


OpenAI


Technical report



Efficient Neural Architecture Search via Parameter Sharing

HPham


MYGuan


BZoph


QVLe


JDean



Proc. of ICML
of ICML

2018
35





Regularized Evolution for Image Classifier Architecture Search

EReal


AAggarwal


YHuang


QVLe



Proc. of AAAI
of AAAI

2019





HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face

YShen


KSong


XTan


DLi


WLu


YZhuang



Proc. of NeurIPS
of NeurIPS

2023





Auto-WEKA 2.0: Automatic Model Selection and Hyperparameter Optimization in WEKA

CThornton


FHutter


HHHoos


KLeyton-Brown



Journal of Machine Learning Research

14
1

2013a





Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms

CThornton


FHutter


HHHoos


KLeyton-Brown


;GWang


YXie


YJiang


AMandlekar


CXiao


YZhu


LFan


AAnandkumar


XWang


BLi


YSong


FFXu


XTang


MZhuge


JPan


YSong


BLi


JSingh

arXiv:2305.16291
arXiv:2407.16741


An open platform for AI software developers as generalist agents

2013b. 2023. 2024


arXiv preprint
Proc. of ACM SIGKDD (KDD)




AIWeco


Aide


Data Science Automation Technical Report

2024. February 3, 2025






ColinWhite


MahmoudSafari


RheaSukthanker


BinxinRu


ThomasElsken


ArberZela


DebadeeptaDey


FrankHutter

arXiv:2301.08727
Neural architecture search: Insights from 1000 papers

2023


arXiv preprint



On hyperparameter optimization of machine learning algorithms: Theory and practice

LYang


AShami



Neurocomputing

415

2020





ReAct: Synergizing Reasoning and Acting in Language Models

SYao


JZhao


DYu


NDu


IShafran


KNarasimhan


YCao



Proc. of ICLR
of ICLR

2023





Neural Architecture Search with Reinforcement Learning

BZoph


QVLe



Proc. of ICLR
of ICLR

2017







