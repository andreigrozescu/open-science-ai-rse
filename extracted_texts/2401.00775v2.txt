



Recent Advances in Text Analysis




February 8, 2024





ZhengTracyKe

Harvard University
University of Georgia
Carnegie Mellon University



PengshengJi

Harvard University
University of Georgia
Carnegie Mellon University



JiashunJin

Harvard University
University of Georgia
Carnegie Mellon University



WanshanLi

Harvard University
University of Georgia
Carnegie Mellon University


Recent Advances in Text Analysis



February 8, 2024


98B929EA212688B6A206F051BF1EEBF1
arXiv:2401.00775v2[stat.AP]






GROBID - A machine learning software for extracting information from scholarly documents







BERT
journal ranking
knowledge graph
neural network
SCORE
Stigler's model
Topic-SCORE
topic weight



Text analysis is an interesting research area in data science and has various applications, such as in artificial intelligence, biomedical research, and engineering. We review popular methods for text analysis, ranging from topic modeling to the recent neural language models. In particular, we review Topic-SCORE, a statistical approach to topic modeling, and discuss how to use it to analyze MADStat -a dataset on statistical publications that we collected and cleaned.The application of Topic-SCORE and other methods on MADStat leads to interesting findings. For example, 11 representative topics in statistics are identified. For each journal, the evolution of topic weights over time can be visualized, and these results are used to analyze the trends in statistical research. In particular, we propose a new statistical model for ranking the citation impacts of 11 topics, and we also build a cross-topic citation graph to illustrate how research results on different topics spread to one another.The results on MADStat provide a data-driven picture of the statistical research in 1975-2015, from a text analysis perspective.





Text analysis is an interdisciplinary research area in data science, computer science, and linguistics. It aims to use computers to process a large amount of natural language data and extract information or features. Research in text analysis and Natural Language Processing (NLP) is especially useful for developing auto-piloting cars, chatbots (e.g., chatGPT), and artificial intelligence in health care and biomedical engineering. In the past decades, numerous methods were proposed for text analysis. Two approaches are especially popular.• Topic modeling. This approach has a strong statistical flavor. Given a large collection of text documents, this approach assumes that all these documents only discuss a few topics (e.g., "finance", "politics", "sports", etc.). Each document discusses the topics with different weights, and given that a particular topic is being discussed, the words in the document are generated from a distribution specific to that topic.• Neural network modeling. This is a rapidly developing area. It models the generation of text documents via deep neural networks, and train the model with massive text corpora (e.g., English Wikipedia) and domain knowledge. The trained model will be used for different down-stream tasks.The neural network approach has proven effective in many NLP tasks (e.g., text classification and machine translation), and has gained immense popularity, particularly among technology titans such as Google and Meta. However, this approach is internally complex, expensive to train, and resource-intensive. These factors substantially restrict the use of the neural network approach, especially for some common NLP users such as social scientists who only have a few hundreds of text documents from a specific domain of interest. The topic modeling approach provides a valuable alternative and has the following benefits.• (Transparency and interpretability). Many users prefer an approach that is (a) not a blackbox but a more transparent step-by-step algorithm, (b) easy to understand and tune (so users can modify it as needed), and (c) where the results (e.g., the extracted features) are easy-to-interpret (see [9,10]).• (Analytical accessibility). Topic modeling approaches are relatively simple and allow for delicate theoretical analysis. Especially, some of these methods are shown to enjoy statistical optimality. In comparison, neural network approaches are much harder to analyze and often have no theoretical guarantee. Topic-SCORE [28] is an especially interesting topic modeling method. It is fast, effective, and enjoys nice theoretical properties. It is also a flexible idea and can adapt to several different settings. These characteristics make Topic-SCORE especially appealing when we analyze the MADStat data set (to be introduced below).One goal of this paper is to review popular topic modeling methods, from the rudimentary topic models in the 1990s to the more recent multi-gram topic models, with a focus on Topic-SCORE and related problems. In addition, we review the neural network approaches. Large neural language model is a rapidly developing area (with new research emerging on a weekly basis), making it hard to conduct a comprehensive review. Since the focus of this paper is on the topic modeling approach and the MADStat data set, we keep the review of neural network approaches relatively brief.Another goal of this paper is to analyze the MADStat dataset using text analysis techniques. MADStat [19] is a large-scale high-quality data set on statistical publications.We collected and cleaned the dataset, with substantial time and efforts. It consists of the bibtex (title, author, abstract, keywords, references) and citation information of 83,331 research papers published in 36 representative journals in statistics and related fields during 1975-2015. The dataset contains detailed citation, bibtex, and author information for each paper (aka. paper-level data). It can be used to study research problems that can not be addressed with other data resources that have only journal-level data or include no author information. Using MADStat, for instance, one can easily find the top 30 most-cited papers within our data range, whereas it is unclear how to do so using Google Scholar.Text analysis on MADStat yields several findings. First, we use Topic-SCORE to identify 11 representative research topics in statistics, and visualize the evolution of the overall weight of statistical publications on each topic. Second, we extend Topic-SCORE to TR-SCORE, a method for ranking research topics by their citation exchanges, and we also build a knowledge graph to visualize how the research results on one topic disseminate to others. Third, we rank all 36 journals and suggest that Annals of Statistics, Biometrika, JASA, and JRSS-B are the four most influential journals in statistics. Last, we find that the (perauthor) paper counts in statistics were steadily decreasing, suggesting that publishing in statistics has becoming more and more competitive. Our results provide an evidence-based picture of the whole statistics community, and so can be viewed as a data-driven review of statistical research, from a text analysis persective. The results may help administrators or committees for decision making (e.g., promotion and award) and help researchers make research plan and build networks. We use statistics as the object of study, but the same techniques can be used to study other fields (e.g., physics).Obtaining a large-scale, high-quality data set such as MADStat is a challenging and time-consuming task. Particularly, many public data (e.g., Google Scholar) are quite noisy, and many online resources do not permit large-volume downloads. The data set must also be carefully cleansed; we accomplish this through a combination of manual labor and custom-developed computer algorithms. See Section A of the supplement for more detailed discussion on data collection and cleaning.Below in Section 2, we review the recent advances on topic modeling. In Section 3, we briefly review neural network language models. In Section 4, we present some preliminary results about MADStat (paper counts, network centrality, journal ranking). In Section 5, we analyze the text data in MADStat using Topic-SCORE as the main tool. In Section 6, we propose TR-SCORE (an extension of Topic-SCORE) for ranking different topics, and we also construct a cross-topic knowledge graph. Section 7 contains a brief discussion.
Topic Models and their ApplicationsTopic model is one of the most popular models in text analysis. [7] proposed the latent semantic indexing (LSI) as an ad-hoc approach to word embedding. Later, [17] proposed a probabilistic model for LSI, which is nowadays known as the topic model. Hofmann's topic model can be described as follows. Given n documents written with a vocabulary of p words, let X ∈ R p×n be the word-document-count matrix where X(j, i) is the count of the jth vocabulary word in document i. Write X = [x 1 , x 2 , . . . , x n ] so x i ∈ R p is the vector of word counts for document i. Suppose document i has N i words. For a weight vector (all entries are non-negative with a unit sum) Ω i ∈ R p , we assumex i ∼ Multinomial(N i , Ω i ), 1 ≤ i ≤ n. (2.1)Here, Ω i is both the probability mass function (PMF) for x i and the vector of population word frequency; in addition, we implicitly assume the words are drawn independently from the vocabulary with replacement. Next, while there are a large number of documents, we assume there are only K "topics" discussed by these documents, and K is a relatively small integer. Fix 1 ≤ i ≤ n and consider document i. For a weight vector w i ∈ R K and PMFs (2.2)We call A and W the topic matrix and the topic weight matrix, respectively.From time to time, we may normalize X to the word-document-frequency matrix D = [d 1 , . . . , d n ] ∈ R p×n , where D(j, i) = X(i, j)/N i (N i : total number of words in document i as above). The primary goal of topic modeling is to estimate (A, W ) using X or D.
Anchor words and identifiability of the topic modelWe call a word an anchor word of a given topic if its occurrence almost always indicates that the topic is being discussed. Consider the Associated Press (AP) [15] data set for example.A pre-processed version of the data set consists of 2246 news articles discussing three topics "politics", "finance", and "crime" [28]. In this example, we may think "gunshot" and "Nasdaq" as anchor words for "crime", and "finance", respectively. In Model (2.1)-(2.2), we can make the concept more rigorously: we call word j an anchor word of topic kif A k (j) ̸ = 0 and A ℓ (j) = 0 for all ℓ ̸ = k.The notion of anchor word is broadly useful. First, it can be used to resolve the identifiability issue of the topic model. Without any extra conditions, Model (2.1)-(2.2) is non-identifiable (i.e., given an Ω, we may have multiple pairs of (A, W ) satisfying Ω = AW ).To make the model identifiable, we may assume rank(W ) = K and impose the anchorword condition (which requires that each of the K topics has at least one anchor word).The anchor-word condition was first proposed by [2] for topic models, which in turn was adapted from the separability condition [11] for nonnegative matrix factorization (NMF).Second, anchor words are useful in methodological developments: many topic modeling methods critically depend on the assumption that each topic has one or a few anchor words; for instance, see Section 2.2 and 2.3 for descreptions of Topic-SCORE and anchorword searching methods. Last but not the least, a challenge in real applications is that both the number of topics K and the meanings of each estimated topics are unknown;we can tackle this with the (estimated) anchor words. See Section 5 for our analysis of the MADStat data for example, where we use the estimated anchor words to decide K, interpret each estimated topic, and assign an appropriate label for each of them.
Topic-SCORE:A spectral approach to estimating the topic matrixA In Hofmann's topic model (2.1)-(2.2), we can view D = AW +(D−W ) = "signal"+"noise",where (typically) rank(AW ) = K ≪ min{n, p}. To estimate A in such a "low-rank signal matrix plus noise" scenario, it is preferable to emply a Singular Value Decomposition (SVD) approach, as SVD is effective in both dimension reduction and noise reduction.Topic-SCORE [28] is an SVD approach to topic modeling, consisting of two main ideas: SCORE normalization and utilizing a low-dimensional simplex structure in the spectral domain. In detail, [28] pointed out that a prominent feature of text data is the severe heterogeneity in word frequency: the chance of one word appears in the documents may be hundreds of times larger than that of another. This heterogeneity poses great challenges for textbook SVD approaches, so the vanilla SVD must be combined with proper normalizations. [28] proposed a pre-SVD approach, where for a diagonal matrix M they constructed, they mapped the data matrix D to M −1/2 D. Unfortunately, while the pre-SVD normalization may reduce the effects of severe heterogeneity to some extent, a major part of them persists. To overcome the challenge, [28] proposed a post-SVD normalization as follows. Let ξk be the k-th left singular vector of M −1/2 D. They normalized ξ2 , . . . , ξK by dividing each of them by ξ1 entry by entry. This gives rises to a matrix R ∈ R n,K−1 , where R(i, k) = ξk+1 (i)/ ξ1 (i) (by Perron's theorem [18], all entries of ξ1 are positive under a mild condition). [28] argued that, by combining the pre-SVD normalization and post-SVD normalizations, one can satisfactorily alleviate the effects of severe word-frequency heterogeneity. The post-SVD normalization was inspired by the SCORE normalization (proposed by [20] for analyzing network data with severe degree heterogeneity), thus the name Topic-SCORE.[28] discovered a low-dimensional simplex S with K vertices as follows. For 1 ≤ i ≤ p, let r′ i be the ith row of R, and view each ri as a point in R K−1 . They pointed out: (a) when word i is an anchor word, then (up to small noise; same in (b)) ri falls on one of the vertices of S; (b) when word i is a non-anchor word, ri is in the interior of S.This simplex structure reveals a direct relationship between R and A (A is the quantity of interest) and gives rise to the Topic-SCORE approach as follows. Let v1 , . . . , vK be the estimates of the vertices of S. We can write each ri uniquely as a convex linear combination of v1 , . . . , vK , with a barycentric coordinate vector πi ∈ R K . Topic-SCORE estimates A by Â = M 1/2 diag( ξ1 )[π 1 , . . . , πp ] ′ (subject to a column-wise renormalization), where diag( ξ1 ) is the diagonal matrix whose diagonal entries are from ξ1 . In a noiseless case where D = AW , [28] showed that Â = A, so the approach is valid. An interesting problem here is how to use the rows of R to estimate the vertices of S (i.e., Vertex Hunting (VH)). This problem was studied in hyperspectral unmixing and archetypal analysis, with many available algorithms. [28] recommended the sketched vertex search (SVS) algorithm [23] for its superior numerical performance. See [26] for more discussion on this.The major computation cost of Topic-SCORE comes from the SVD step, which can be excuted relatively fast. For this reason, Topic-SCORE is fast and can easily handle large corpora. For example, it takes only a minute to process the MADStat corpus in Section 5.Topic-SCORE is also theoretically optimal in a wide parameter regime [28].
2.3The anchor-word-searching methods for estimating A [2,1] proposed an anchor-word-searching approach which estimates A by finding anchor words from the word-word co-occurrence matrix Q = DD ′ . This method first normalizes each row of Q to have unit-ℓ 1 -norm, with the resulting matrix denoted by Q. It then applies a successive projection algorithm to rows of Q, to get a subset S ⊂ {1, 2, . . . , p} containing exactly one estimated anchor word per topic. The method then estimates A by either a direct reconstruction or minimizing some objective functions (e.g., KL-divergence). [2,1] are among the first works that utilize the anchor-word condition for topic modeling and provide explicit error rates. A challenge it faces is that the rows of Q are in a very highdimensional space. Similar to Topic-SCORE, their anchor-word-searching also relies on a K-vertex simplex, except for a major difference: this simplex is in R p while the simplex in Section 2.2 is in R K−1 (e.g., in the aforementioned AP dataset, K = 3, but p is a few thousands). This gives Topic-SCORE an important edge (in both theory and computation) when it comes to vertex hunting (VH) and subsequent steps of estimating A. In particular, Topic-SCORE improves the error rate in [2,1]. [4] proposed a different anchor-word-searching approach. Recall that W ∈ R K×n is the topic weight matrix; see Model (2.1)-(2.2). Lettingζ k = ∥W k ∥ 2 /∥W k ∥ 1 , where W k is kth row of W , they assumed W ′ k W ℓ ∥W k ∥∥W ℓ ∥ < ζ k ζ ℓ ∧ ζ ℓ ζ k , for 1 ≤ k ̸ = ℓ ≤ K.For the same Q as above, let S i be the set of indices j such that Q(i, j) attains the maximum value of row i. [4] proposed an approach and showed that if (a) the above assumption holds, and(b) the model is noiseless (i.e., D = AW ), then the approach can fully recover the set of anchor words from the index sets S 1 , S 2 , . . . , S n . Extending the idea to the real case (where D = AW + "noise"), they obtained an estimate for the set of anchor words, and then a procedure for estimating A.
Other approaches for estimating A: EM algorithm and NMF approachesThe EM algorithm is a well-known approach to fitting latent variable models. It was noted (e.g. [34]) that Model (2.1)-(2.2) is equivalent to a latent variable model, so we can estimate A using the EM algorithm. Such an approach is interesting but faces some challenges. First, it does not explicitly use any anchor-word condition, so the model being considered is in fact non-identifiable (see Section 2.1). Also, since min{n, p} is typically large, the convergence of the EM algorithm remains unclear; even when the EM algorithm converges, the local minimum it converges to is not necessarily the targeted (A, W ) (which is uniquely defined under a mild anchor-word condition; see Section 2.1).Also, note that Model (2.1)-(2.2) implies D = AW + "noise", where (D, A, W ) are all (entry-wise) non-negative matrices; hence, the problem of estimating (A, W ) can be recast as a non-negative matrix factorization (NMF) problem. There are many NMF algorithms (e.g., see [14]), which are proved to be successful in applications such as image processing [31], recommender systems, and bioinformatics. However, a direct use of them in topic modeling faces challenges. The noise in most NMF settings is additive and homoscedastic, but the noise matrix D − E[D] in the topic model is non-additive and severely heteroscedastic, as indicated by the multinomial distribution. In Model (2.1)-(2.2), the variance of D(j, i) is proportional to word j's frequency in document i. Because of severe word-frequency heterogeneity, the variances of D(j, i) may have different magnitudes, hence, a direct application of NMF algorithms often yields non-optimal error rates.
Estimating the topic weight matrix WIn Model (2.1)-(2.2), D = AW + "noise", and both A and W are unknown. While most existing works focused on estimating A, W is also of interest (e.g., see Section 5). To estimate W , a natural approach is to first obtain an estimate Â for A, and then estimateW by fitting the model D = ÂW + "noise". Recall that W = [w 1 , . . . , w n ]. [28] proposed a weighted least square approach, where for each 1 ≤ i ≤ n, it estimates w i by ŵi = argmin w ∥Θ(d i − Âw)∥ 2 , with Θ ∈ R p×p being a diagonal weight matrix (as w i ∈ R K and K is typically small, this is is a low-dimensional regression problem). To handle severe wordfrequency heterogeneity, [28] suggested Θ = M − 1 2 , with the same M as in Section 2.2. For our study on the MADStat data in Section 5, we find that taking Θ = I p also works fine, if a ridge regularization is added. Noting that the word count vector x i is distributed as Multinomial(N i , Aw i ), we can also estimate w i by some classical approaches, such as MLE, where we replace A by Â in the likelihood.The above raises a question: Since D = AW + "noise", can we first estimate W and then use W to estimate A? There are two concerns. First, in some settings, the optimal rate for estimating A is faster than that of estimating W (see Section 2.6). Therefore, if we first estimate W and then use W to estimate A, then we may achieve the optimal rate in estimating W but likely not in estimating A. If we first estimate A and then use A to estimate W , we have optimal rates in estimating both. Second, many approaches for estimating A rely on the assumption that each topic has some anchor words (see Sections 2.2-2.3). If we extend them to estimate W , we need to similarly assume that each topic has some pure documents (document i is pure if w i (k) = 1 and w i (ℓ) = 0 for ℓ ̸ = k). However, in many applications, it is more reasonable to assume the existence anchor words than the existence of pure documents (especially when documents are long). Therefore, though the roles of A and W may appear symmetrical to one other, they are not symmetrical in reality.
2.6The optimal rates for estimating (A, W )For simplicity, as in many theoretical works on topic modeling, we assume N 1 = . . . N n = N ; i.e., documents have the same length. We may have either a long-document (LD) case where N/p = O(1) or a short-document (SD) case where N/p = o(1) (p: size of the vocabulary).Consider the rate for estimating A. For any estimate Â, we measure the loss by the ℓ 1 -error: L( Â, A) = K k=1 ∥ Âk − A k ∥ 1 (subject to a permutation in the K columns of Â). The minimax rate is defined as R n = inf Â sup A EL( Â, A). In the LD case, when K is finite, R n ≍ p/(N n) up to a multi-log(p) factor (e.g., log(p)) [28]; when K grows with (n, p), R n ≍ K Kp/(N n), also up to a multi-log(p) factor [4]. In the SD case, the optimal rate is unclear. Some minimax upper bounds were derived [2,28], but they do not yet match the minimax lower bound. The difficulty of the SD case is that the majority of words have a zero count in most documents, which poses challenges in theoretical analysis.Consider the rate for estimating W . Similarly, for any estimate Ŵ , we measure the loss by L( Ŵ , W ) = 1 n n i=1 ∥ ŵi − w i ∥ 1 (up to a permutation in the K rows in Ŵ ) and define the minimax rate as R n = inf Ŵ sup W EL( Ŵ , W ). [45] showed that R n ≍ K/N . In an apparently parallel work, [29] considered the Frobenius loss n −1/2 ∥ W − W ∥ F and showed that the minimax rate is K 1/N . The minimax rates are flat in n: This is not surprising, because the number of free parameters in W is proportional to n.
Estimating the number of topics KAlmost all topic learning algorithms assume K as known a priori, but K is rarely known in real applications. How to estimate K is therefore a fundamental problem.To estimate K in such a "low-rank matrix plus noise" situation, a standard approach is to use the scree plot: for a threshold t, we estimate K as the number of singular values of X that exceed t. [28] showed that this estimator is consistent, under some regularity conditions. This method does not need topic model fitting and is fast and easy-to-use, but how to select a data-driven t is an open question. Alternatively, one may select K using BIC or other information criteria: for each candidate of K, we obtain ( Â, Ŵ ) by applying a topic learning algorithm, and estimate K by the candidate that minimizes BIC.Also, alternatively, one may use the cross validation (CV) approaches, by estimating a topic model for each candidate K and each training-validation split. A commonly-used validation loss is the perplexity. It measures the predictive power of a trained language model on the held-out test set. To use perplexity, we usually assume w i are iid generated, so the approach is more appropriate for the Bayesian version of the topic model to be introduced in Section 2.9; we can also use a full Bayesian approach by imposing a prior on K and selecting K to minimize the marginal likelihood [41]. In both the BIC and CV approaches, we need to fit the topic model many times, so the computational cost is high.In simulation studies, it has been noted that (a) none of these methods is uniformly better than others, and which method is the best depends on the data set, and (b) the popular perplexity approach often over-estimates K. For these reasons, in real applications, whenever some inside information is available, we hope to use them to help determine K.For example, in the study of MADStat (see Section 5), we investigate the estimated anchor words by Topic-SCORE for different K, and use our knowledge of the statistical community to choose the K with the most reasonable results. In some applications, what the best K is depends on the perspectives of the users, and even experts may differ in their opinions. In such a case, we may want to consider several different K. Such a flexibility may be helpful.
Global testing associated with topic modelsThe problem of global testing is closely related to the problem of estimating of K. The goal is to test H 0 : K = 1 versus H 1 : K > 1. Global testing is a fundamental problem: if no method can reliably tell between K = 1 and K > 1, it is merely impossible to estimate K or estimate the matrices (A, W ) in Model (2.1)-(2.2).Recall that x i ∼ Multinomial(N i , Aw i ), 1 ≤ i ≤ n, in Model (2.1)-(2.2). [6] proposed a test statistic ψ n called DELVE. They showed that when K = 1, although the model has many unknown parameters, ψ n → N (0, 1), and the limiting distribution does not depend on unknown parameters. This result is practically useful. For example, we can use it to compute an approximate p-value and use the p-value to measure the research diversity of different authors in the MADStat dataset; see Section 3.3 of [19] for a similar use of global testing in the network setting [21,22].
Denote by λ 2 the second largest (in magnitude) eigenvalue of ΣA = A ′ [diag(A1 K )] −1 A.Similar as in Section 2.6, we assume N i = N for 1 ≤ i ≤ N . Consider the DELVE test that rejects H 0 if |ψ n | ≥ t, for a threshod t > 0. [6] showed that this test achieves a sharp phase transition as follows. If |λ 2 |/ p/(N n) → ∞, for an appropriate t, the sum of the Type I and Type II errors of the DELVE test converges to 0 as p → ∞. If |λ 2 |/ p/(N n) → 0, for any test, the sum of the Type I and Type II errors converges to 1. Compared with earlier works (e.g., [28,4]), such a result is more satisfying. In earlier works, we usually assume all eigenvalues of Σ A are at the order of O(1). Here, we may have λ 2 = o(1), especially when p ≪ N n.
The latent Dirichlet topic model and its estimationThe latent Dirichlet allocation (LDA) model by [5] is one of the most popular topic models, and it can be viewed as a Bayesian version of the Hofmann's topic model. In the LDA model, we start with Model (2.1)-(2.2) and further assume that the topic weight vectors w 1 , w 2 , . . . , w n are i.i.d. drawn from a Dirichlet distribution with parameters α = (α 1 , . . . , α K ), where α k ≥ 0 and K k=1 α k = 1. The LDA model has parameters (A, α) and treats w i 's as latent variables. In such a setting, (A, α) are estimated by a variational EM algorithm, and the posterior of w i 's can be obtained using MCMC. This is essentially the approach proposed by [5]. Compared to Model (2.1)-(2.2), LDA does not assume any structure on the topic matrix A. Therefore, if our goal is to estimate A, all those methods in Sections 2.2-2.3 are still applicable. In particular, compared to the variational EM approach of [5], Topic-SCORE in Section 2.2 is not only faster but also provides desired theoretical guarantees [28]. On the other hand, LDA puts a Dirichlet prior on the topic weights w i . This allows us to learn the posterior distribution of w and may provide additional insights. Recall that in Section 2.5, we have proposed a regression approach to estimating W (without any priors on W ). The regression approach is still useful for the LDA model (e.g., we can use this method to estimate the parameter α in the LDA model, and plug the estimated value to the variational EM algorithm).
The m-gram topic modelsHofmann's topic model and the LDA are so-called bag-of-word or uni-gram models, as they only model the counts of single words, neglecting word orders and word context. There are several ideas about extending these models to incorporate word orders and word context.One idea is to simply expand the vocabulary to include phrases. For example, we may include all possible m-grams in the vocabulary (an m-gram is a sequence of m words).Unfortunately, even for a small m, the size of this vocabulary is too large, making topic estimation practically infeasible. To address the issue, we may only include a subset of carefully selected m-grams. For example, we may exclude low-frequency phrases or apply a phrase retrieval algorithm [13]. Once the vocabulary is determined, we treat each item in the vocabulary as a "word" and model them by (2.1)-(2.2) same as before; the resulting model is still a uni-gram model in flavor.Another idea is the bigram topic model [44]. For each 1 ≤ i ≤ n, document i is modeled as an ordered sequence of words satisfying a Markov chain with a transition matrix M i ∈ R p×p (p: vocabulary size), where M i (j, ℓ) is the probability of drawing word ℓ when the word immediately preceding it is word j. For transition matrices A 1 , A 2 , . . . , A K ∈ R p×p ,M i = K k=1 w i (k)A k ,where each A k is treated as a "topic" and w i ∈ R K is the topic weight vector as before. [44] proposed a Gibbs EM algorithm for estimating the parameters and showed that, compared to the unigram topic model, this bigram model led to a better predictive performance and more meaningful topics on two real-world datasets.
Supervised topic modelsIn many applications, we observe not only text documents but also some response variables associated with documents. For example, many online customer reviews contain numeric ratings; we treat a review as a text document and the corresponding rating as the response.We would like to build a joint model for text and response, to help predict future ratings.The model in [27] is a supervised topic model of this kind. This paper studied the problem of how to use news articles to improve financial models. They focused on the news articles in Dow Jones Newswire. These articles are tagged with the identifier of a firm (the study excluded articles tagged with multiple firms). They model the news article with Model (2.1)-(2.2) and K = 2 (so there are only two topics), where the two topics are "positive sentiment" and "negative sentiment", respectively. In such a simple case, for any1 ≤ i ≤ n, let w i = (a i , 1 − a i ) ′be the topic weight of document i as before (w i captures the "sentiment" level of article i). Meanwhile, let y i be the stock return of the firm being tagged with document i. They assume that P(y i > 0) = f (a i ) for an (unknown) function f that is monotone increasing. This model jointly models text and return data, allowing for a better estimation of w i (which in turn may lead to a better prediction of stock returns).Compared with other approaches that also estimate news sentiment and use it to predict returns, this approach has a substantial improvement on real-data performance. Moreover, see [33] for other supervised topic models with a similar flavor.
Deep neural network approaches to natural language processingThe deep neural network approaches to natural language processing (DNN-NLP) have become very popular recently, with successes observed in a variety of NLP tasks such as text classification, question answering, machine translation, among others [36].In statistics, a "model" is a generative model with some unknown parameters we need to estimate. In DNN-NLP, researchers use the term "model" slightly differently: a neural language model usually refers to a pre-trained neural network equipped with estimated parameters. A neural language model usually consists of three components as follows.• A neural network architecture. This is the core of a neural language model. It specifies how an input text is processed to generate the desirable output. The encoder-decoder structure is commonly used: the encoder is a neural network that maps the input text into a numeric vector (a.k.a., the encoder state), and the decoder converts the encoder state to the targeted output (e.g., a variable-length sequence of tokens). Many neural network models were inspired by new architectures proposed in the literature.• The NLP tasks used to train the neural networks. A neural language model usually targets on one specific task (e.g., machine translation) or several specific NLP tasks (e.g., the BERT model [8] outputs document embeddings, which can be used in various downstream tasks). In either case, pre-training the neural networks (i.e., estimating the parameters) must use specific NLP tasks to define the objective function. Hence, the same architecture may lead to different neural language models if they are pre-trained using different NLP tasks.• The text corpora and domain knowledge used in training. Even with the same architecture and the same NLP tasks in training, the resulting neural language model still varies with the training corpora. One strategy is selecting training corpora to obtain a domain-specific language model. For example, BERT has variants such as BioBERT [32] trained using publications in biomedicine. Besides domain-specific corpora, other knowledge such as a domain-specific vocabulary can also be employed.The research on DNN-NLP has multiple goals, including but not limited to (a) Prediction of the next word given the previous words in a sentence (e.g., GPT family [37]), (b) Extraction of numeric features from text (e.g., BERT family [8]), and (c) modeling the (synatic and semantic) relationships of words (e.g., word2vec [35]). DNN-NLP is a fast-developing area, which is hard to review comprehensively (especially as our focus is on the topic modeling approaches and the MADStat data). For these reasons, we select a few interesting topics in DNN-NLP to review, focusing on (a) popular DNN architectures for NLP, (b) BERT, a powerful feature extraction tool developed by Google Inc. We also discuss word embedding and how to apply a neural language model (e.g., BERT) to a text corpus in our own research (see Remarks 1-2).
Commonly used neural network architecturesSome well-known network architectures for NLP include the convolutional neural networks (CNNs), recursive neural networks (RNNs), and transformers. CNNs and RNNs are more traditional, and transformers have become very popular in recent years.CNNs use structural layers (e.g., convolutional layers and pooling layers) to capture the spacial patterns in the input, and are extensively used in signal (speech, image, video)processing. In processing a text document, sometimes it is not important whether certain words appear, but rather whether or not they appear in particular localities. Hence, CNNs are also useful for NLP tasks such as sentence modeling [24] and sentiment analysis [12].RNNs are especially useful for sequence data with variable-lengths, making them suitable for text analysis. The long short-term memory (LSTM) network [16] is the most popular variant of RNNs. In the vanilla RNNs, information may be diluted with successive iterations, preventing the model to "remember" important information from the distant past. LSTMs add neurons (called "gates") to retain, forget, or expose specific information, so it can better capture the dependence between two far-apart words in the sequence. The standard LSTMs are unidirectional (i.e., text is processed left-to-right). It is preferred to process text bidirectionally, as a word may depend on the words behind it. The bidirectional LSTMs combine outputs from left-to-right layers and right-to-left layers.The transformers [43] are a type of architectures based on the attention mechanism [3].In a traditional encoder-decoder pair, the encoder maps the input sequence into a fixedlength vector, and the decoder has access to this vector only. The attention mechanism allows the encoder to pass all the hidden states (not just the final encoded vector) to the decoder, along with annotation vectors and attention weights to tell the decoder which part of information to "pay attention to". The attention mechanism was shown to be much more effective than RNNs in processing long documents. [43] proposed a special architecture called transformer that uses self-attention within each of the encoder and decoder and cross-attention between them. The transformer has become the most popular architecture in NLP. For example, the encoder part of the transformer is the building block of models like BERT (see below), and the decoder part of the transformer is the building block of models like GPT [37] for text generation.
BERTThe bidirectional encoder representations from transformers (BERT) is a state-of-the-art language model developed by Google AI Language [8], which provides a numerical representation for each sentence. As mentioned before, a neural language model consists of three components: architecture, pre-training tasks, and training corpora. For architecture, BERT uses the transformer encoder with bi-directional self-attention. For training corpora, BERT uses the BooksCorpus (800M words) [47] and English Wikipedia (2,500M words). The main innovation of BERT is in the pre-training tasks it used: BERT was pre-trained using two tasks, the masked language modeling (MLM) and next sentence prediction (NSP). In MLM, some tokens of the input sequence are randomly masked, and the objective is to predict those masked tokens from their left and right contexts. In NSP, the input are two sentences A and B from a corpus, and the objective is to tell if B is the next sentence of A. These tasks do not require manual labeling of text.BERT has been applied to different downstream NLP tasks, with superior performances.Numerous language models have been created based on BERT, such as modifications of the architecture (e.g., ALBERT and DistillBERT) and pre-training tasks (e.g., RoBERTa and ELECTRA), adaptation to other languages (e.g., XLM and ERNIE), and inclusion of domain-specific corpora (e.g., BioBERT and UmlsBERT). See [38] for a comprehensive survey.Remark 1. Another major goal of NLP is to learn the syntactic and semantic relation-ships between words. To do this, a standard approach is word embedding (i.e., find vector representations of words). Despite the fact that word embedding is frequently used in neural language models (often as the first layer), its primary purpose is to understand or mimic various syntactic and semantic regularities in natural languages. A frequently mentioned example is that vector("king") − vector("man") + vector("woman") ≈ vector("queen").Word2vec [35] is a popular word embedding model. It was trained using a Google News corpus, and its performance was tested on a semantic-syntactic relationship question set manually created by the authors.Remark 2. Many modern DNN-NLP tools (such as BERT) are owned by high-tech companies. They were trained with a huge amount of data and efforts, and many parts of them are not publicly available. A typical NLP user has his/her own (domain-specific) text corpus (1K to 10K documents), which are not large enough to re-train BERT (say).To help these users to apply modern DNN-NLP tools, there are two approaches: transfer learning and fine tuning. In the first approach, the user inputs his/her own documents to the BERT (say) and obtain an embedded vector for each document. The embedded vectors can then be used as features for downstream analysis. In the second approach, a user may alter the parameters of the pre-trained model. By adding additional layers to the neural networks, one can convert the output of a pre-trained neural language model to the targeted output of a downstream task (e.g., document classification). Next, all the parameters-those in the pre-trained model and those for the added layers-are updated together (this can done by running stochastic gradient descents starting from parameters of the pre-trained models).
MADStat basics: paper counts, journal ranking, and network centralityThe multi-attribute dataset on statisticians (MADStat) contains the bibtex (e.g., author, title, abstract, journal, year, references, etc.) and citation information of 83,331 papers from 47,311 authors, spanning 41 years . We collected and cleaned the dataset with substantial time and efforts and have made it publicly available (the links to download the dataset can be found in [19]). In the supplementary material, we present (a) details on data collection and cleaning, (b) the list of the 36 journals and their abbreviations, and (c) supplementary results of the text analysis conducted in this paper (such as selection of K for Topic-SCORE). In this section, we discuss some basic findings on the data set, including paper counts, network centrality, and journal ranking.
Paper countsThe paper counts provide valuable information for studying how the productivity of statisticians evolve over time. In the left panel of Figure 1, the red curve presents the number of papers per year and the blue curve presents the number of active authors per year (an author is active in a given year if he/she publishes at least 1 paper in that year). In both  
Network centralityNetwork centrality (e.g., most-collaborative authors) provides information for the leadership and trends in statistical research. Table 1 presents the top 10 authors who have the most coauthors, the most citers (a citer for any given author is any other author who has cited this author), and the most citations, respectively. Table 4 (Appendix, Section E) presents the top 10 most-cited papers. Note that the numbers of coauthors, citers, and citations here are all counted using only the papers in our data range, so there may be some biases in our ranking. For example, in  on LARS. See Figure 9, where for each pattern we present the yearly citation curve of a representative paper. • • • • • • • • • • • • • • • • • • • •1995• • • • • • • • • • • • • • • • • • • • •• • • • • • • • • • •• • • ••• • •• •••• •• • • • •• 1985 1995 2005 2015 0 2 4 6 8 10 •• • • • • • • • • • •• • • ••• • •• •••• •• • • • •• • • • • • •• • • • •• • • • • • ••• • • • • • • • • • • • •• • • • • • • 1980 1990 2000 2010 0 20 60 100 140 • • • • • •• • • • •• • • • • • ••• • • • • • • • • • • • •• • • • • • • •• • • • • • • • • • •• • • • • • • • • • •• • •• • • • 1985 1995 2005 2015 0 20 40 60 •• • • • • • • • • • •• • • • • • • • • • •• • •• • • • Figure 9:Yearly citation curve for 4 papers. Left to right: "sleeping beauty" (Tibshirani (1996)on Lasso), "transient", "steadily increasing" (Dempster, Laird and Rubin (1977) on EM algorithm), and "sudden fame" (Liang and Zeger (1986) on GLM). Compare Figure 7.Sleeping beauty. The "sleeping beauty" pattern is especially interesting. To identify papers with such a pattern, we need a metric. We adapt the approach in [5]. Fix a paper i.Suppose T i years (or months/quarters) have passed since its publication by the end of 2015.Let n i (t), 1  t  T i , be the number of citations the paper receives in year t. Suppose the 30 The "sleeping beauty" pattern is especially interesting. To identify the sleeping beautifies in our data range, we use the metric suggested by [25]. It outputs a measure B i for each paper i (the details are in the supplementary material); the larger B i , the more likely this paper is a sleeping beauty. We select the 300 papers with the largest maximum number of yearly citations and arrange them in the descending order of B i . Table 5 and 
Journal rankingJournal ranking has been widely used in appointing to academic positions, awarding research grants and ranking universities and departments. A common approach is the Impact Factor (IF), but IF is known to have some issues [42]. We instead use the Stigler's model [40] for journal ranking: Given N journals, let µ 1 , . . . , µ N ∈ R be their export scores; for two papers i and j published in journal ℓ and m, respectively, let C ij be the indicator of a citation from i to j. We assumeP(C ij = 1|C ij +C ji = 1) = exp(µ ℓ −µ m )/[1+exp(µ ℓ −µ m ))].We fit this model using the quasi-likelihood approach in [42]. For comparison, we also consider the PageRank approach (with the same tuning parameter α as suggested in [42]).Among the 36 journals (see Table 3), there are relatively few citation exchanges between the 3 journals focusing on probability and the other 33 journals, so we exclude these 3 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0 5 10 same as in Section 2, x i ∈ R p contains the word counts of the ith paper abstract.
Anchor words and the 11 identified topicsTo apply Topic-SCORE, we need to decide the number of topics. This is a hard problem (see Section 2.7) and we tackle it by combining the scree plot, substantial manual efforts, and our knowledge of the statistical community (see Section H of the supplementary material).We find that that K = 11 is the most reasonable choice.Since K = 11, there are 11 discovered research topics by Topic-SCORE. To interpret and label these topics, we introduce a rule for selecting 'representative' words and papers for each topic. The anchor words (see Section 2.1) appear only in one topic. For example, "lasso" and "prior" may be anchor words for the topics of "variable selection" and "Bayes", respectively. Given Â, define the topic loading vector a j ∈ R K for each word j by a j (k) = Âk (j)/[ K ℓ=1 Âℓ (j)], 1 ≤ k ≤ K. Note that 0 ≤ a j (k) ≤ 1 and in theory a j (k) = 1 if and only if word j is an anchor word of topic k. Fix 1 ≤ k ≤ K. The most frequent anchor word in topic k is the word ĵ where ĵ = argmax j {a j (k) : 1 ≤ j ≤ p}. Similarly, we can define the m-th most frequent anchor word for any m ≥ 1. Figure 4 shows the 20 most frequent anchor words for each of the 11 estimated topics. Based on these words, we suggest a name for each topic as in the second column of in Table 2. To check if the proposed labels are reasonable and get more insight of each topic, we also use Ŵ to identify representative papers. For each 1 ≤ k ≤ 11, we pull out the top 300 papers with the largest ŵi (k) (the titles of top-3 within each topic is given in Table 8 of the supplementary material). We manually review the titles of these papers and come up with a list of suggested research Our topic learning results are based on abstract similarity (i.e., the research areas covered by the same topic have similar word counts in their abstracts). Such a similarity does not necessarily imply the similarity in the intellectual content of the paper. Also, our goal here is to use statistical methods to identify a few interpretable topics, and it is possible that some research topics in the data set are not well represented here.
Topic weights for representative authorsHow to estimate the research interests of an author is an interesting problem. It helps us understand an author's research profile and may be useful in decision making (e.g., award, funding, promotion); it may also help this author to plan for future research. We estimate  the research interest of an author as follows. For an author a, let N a ⊂ {1, 2, . . . , n} be the collection of papers he/she published in our data range. Each paper i has an estimated topic weight vector ŵi for its abstract. A reasonable metric of author a's interest on topick is wa (k) = 1 |Na| i∈Na ŵi (k), 1 ≤ k ≤ 11.Let w(k) be the average of ŵi (k) over all 56,500 abstracts. We define the centered topic interest vector of author a by z a = wa − w ∈ R 11 .The entries of z a sum to 0, so it has both positive and negative entries. We are interested in its positive entries, since z a (k) > 0 indicates a greater-than-average weight on topic k.We can compute the vector z a for almost every author in our data range. Table 9 of the supplementary contains the results of 80 selected authors. Figure 5 5 suggests that the research interests of Peter Bickel, David Donoho, and Kathryn Roeder are relatively diverse, covering many topics; these are consistent with our impression of these authors and the information of 11 topics in Table 2.
Topic trendsHow to characterize the evolvements of statistical research over time is an interesting problem [30]. We tackle it by combining the estimated topic weights and the time and journal information of each paper. q q q q q q q q q q q q q q q q q q q q q q q q q q 1990 1995 2000 2005 2010 2015 0.04 0.06 0.08 0.10 0.12 0.14 q q q q q q q q q q q q q q q q q q q q q q q q q q q q Math.Stats.  First, we study how the yearly average topic weights change over time. Recall that ŵi is the estimated topic weight vector for paper i by Topic-SCORE. For each year, we compute the average topic weight for all papers published in this year, smoothed by a weighted moving average in a 3-year window (weights: 0.25, 0.50, and 0.25). See Figure 6.We observe that the 5 topics, Math.Stat., Regression, Bio./Med., Bayes, and Hypo.Test, We observe that in some time periods, some journals are clearly in favor of some topics.When this happens, we say that this journal is "friendly" to this topic. In Figure 7, we list the "friendliest" journals for 11 topics. Note that the short label of a topic may not be accurate for all research topics it covers, and it is preferable to consult 
Math.Stats. AoSIn Section 4.4, we have discussed journal ranking, in which OCA is a good choice. For topic ranking, TWCA is more appropriate. Under TWCA, we view µ ′ w i = K k=1 µ k w i (k) as the export score of paper i and assume the Bernoulli variables C ij and C ji satisfyP(C ij = 1|C ij + C ji ≥ 1) = exp(µ ′ w i − µ ′ w j ) 1 + exp(µ ′ w i − µ ′ w j ) . (6.1)This gives the model of the citation exchange matrix C. To model the word-documentcount matrix X, we use the same model as in Section 2:x i ∼ Multinomial(N i , Aw i ), A ∈ R p×K , w i ∈ R K , (6.2)where A is the topic matrix as in Section 2 and N i is the size (total word count) of document i. For identifiability, we assume median(µ 1 , . . . , µ K ) = 0. Also, for simplicity, we assume X and C are independent (but their distributions are related by w i 's), and this can be relaxed. We call (6.1)-( 6.2) the Hofmann-Stigler model.
Topic-Ranking SCORE (TR-SCORE)We propose TR-SCORE for topic ranking. The input are X, C, and the number of topics K, and the output is an estimated export score vector μ. TR-SCORE has three steps.1. (Topic matrix estimation). Apply Topic-SCORE (e.g., Section 2.2) to get Â ∈ R p×K .
(Topic weight estimation).For 1 ≤ i ≤ n, estimate w i by ŵi = ( Â′ Â + λI K ) −1 Â′ d i ,where λ > 0 is a regularization parameter which we usually fix at λ = 0.3.
(Topic ranking). Plug ŵ1 , . . . , ŵn into (6.1) and obtain an estimate μ for the export score vector µ. Rank topics according to the descending order of μ1 , μ2 , . . . , μK .We discuss Step 3 in detail. We use a quasi-likelihood method with over-dispersion to obtain μ. Recall that C is the adjacency matrix of between-paper citations.Write C = C + C ′ (i.e., Cij = C ij + C ji ). Recall that W = [w 1 , w 2 , . . . , w n ] ∈ R K,nis the topic weight matrix.Let τ (x) = e x /(1 + e x ) denote the logistic function. We now slightly modify (6.1) to assumeE[C| C] = C • Ω, Var(C| C) = ϕ[Ω • (1 − Ω)], with Ω = τ (1 n µ ′ W − W ′ µ1 ′ n ), (6.3)where • is the Hadamard product, Var(C| C) and (1 − Ω) are both element-wise operations, and ϕ > 0 is the dispersion parameter. Model (6.1) corresponds to fixing ϕ = 1, but a better strategy is to estimate ϕ from data, as commonly used in fitting count data (e.g., see [42] for a similar strategy for fitting the Stigler's model). When W is known, we estimate µ 1 , µ 2 , . . . , µ K by maximizing the quasi-likelihood, which is equivalent to maximizing the likelihood of model (6.1). This is done by first fixing µ 1 = 0 and treating (6.1) as a generalized linear model with (K − 1) predictors and N := i,j 1{ Cij = 1} samples, so that it can be solved by a standard package. We then re-center μ1 , μ2 , . . . , μK so that their median is 0. The dispersion parameter is estimated by φ =1 N −K+1 (i,j):i<j, Cij ≥1 (C ij − Cij Ωij ) 2 /[ Cij Ωij (1 − Ωij )],where Ωij = τ (μ ′ w i − μ′ w j ). So far, W is assumed known. For unknown W , we use the same procedure, except that W is replaced by the Ŵ from Step 2.
Topic-ranking and a cross-citation graphIn Section 5, we have applied Topic-SCORE to a set of 56,500 (pre-processed) abstracts and identified 11 representative research topics in statistics. We now use TR-SCORE to the same set of abstracts and rank all 11 topics. We also build a cross-topic citation graph (as a type of knowledge graph) to visualize the dissemination of knowledge across areas (an important research topic in the area of modern knowledge discovery [39]).We first build a cross-topic citation graph. This is a weighted and directed graph with 11 nodes, each being a discovered topic. We propose two definitions of edge weights. In the first one, let N k,ℓ = n i,j=1 ŵi (k) ŵj (ℓ)C ij and P kℓ = N kℓ /( K m=1 N km ), for 1 ≤ k, ℓ ≤ 11, where C is the between-paper citation adjacency matrix and ŵi is the topic weight vector of abstract i. Here N kℓ is the (allocated) citation counts from topic k to topic ℓ, and P kℓ is the proportion of citations to topic ℓ among all citations from topic k. We use P ∈ R 11×11 as the weighted adjacency matrix of this graph. In the second definition, we group all papers based on the 'dominant topic' -the topic with the largest weight in ŵi (if there is a tie, pick the smaller k). Let w * i ∈ {e 1 , e 2 , . . . , e K } denote the group label of abstract i.Define N * k,ℓ = n i,j=1 ŵ * i (k) ŵ * j (ℓ)C ij and P * kℓ = N * kℓ /( K m=1 N * km ).We then use P * ∈ R 11×11 as the weighted adjacency matrix. This definition uses "winner takes all" to allocate each      Firth, 2016). We instead use the Stigler's model for journal ranking, which takes in the randomness and skewness of citation counts. Given N journals, let µ 1 , µ 2 , . be their export scores. For two papers i and j published in journal and m, resp C ij be the indicator of a citation from i to j. We assume (2.4)P(C ij = 1|C ij + C ji = 1) = exp(µ ` µ m )/[1 + exp(µ ` µ m ))].This model is in the same spirit of (2.2) but uses OCA for citation contribution uses TWCA for citation contribution). We fit this model using the quasi-likelihoo in Varin, Cattelan and Firth (2016). For comparison, we also consider the PageRan (with the same tuning parameter as suggested in Varin, Cattelan and Firth (201 To apply the two methods to our data set, we construct a between-journal cita G as follows. First, among the 36 journals (see Table S1 of the supplement for journals), there are relatively few citation exchanges between the 3 journals f probability and the other 33 journals, so we exclude the 3 probability journals fo here. Second, for each pair of journals, we count the between-journal citations year time window. For instance, if 2014 is the "current year," then we count one ci journal i to journal j if and only if a paper published in journal i in 2014 has ci published in journal j between 2005 and 2014. This gives rise to a 33 ⇥ 33 betw citation matrix for 2014. Last, for stability and reliability of the rankings, we take the two matrices for 2014 and 2015. This is our final data matrix fed into either methods. The results are in Figure 4, where each solid black circle represents a j the x-axis and the y-axis are the rankings given by the PageRank approach and t model approach, respectively.Both approaches rank AoS, Biometrika, JASA, and JRSSB as the top 4 (Figure left). In particular, both approaches rank AoS as number 1 and Biometrika as num JASA and JRSSB, PageRank ranks them as numbers 2 and 4, respectively, while approach ranks them as numbers 4 and 2, respectively.The rankings by two methods are quite consistent with each other. A few exc CSDA, EJS, JMVA, JRSSA, JTSA, and SMed. For example, PageRank ranks CSDA 6 but Stigler's model ranks it as number 23; PageRank ranks JTSA as number 26, b model ranks it as number 12. In fact, PageRank weighs each citation equally, while model gives citations from higher-ranked journals greater weight than citations f ranked journals. The idea behind Stigler's model treats different journals as comp being cited is considered "winning"; being cited by more competitive journals is a nal of being competitive. For these reasons, the results of the PageRank approac close to that of ranking by citation numbers, but the results of the Stigler approa significantly different. A closer look at the citation counts reveals that a large pr  Next, consider the topic ranking. The export scores of 11 topics by TR-SCORE are shown in Figure 3 (right). Math.Stats. is the highest-ranked topic. This is reasonable, as the focus of Math.Stats. is mathematical analysis and probability, which may have a long-lasting impact on other topics in statistics. Regression and Mach.Learning are also highly ranked. This is also understandable, as the two topics cover many "hot" research topics; see Table 2. The rankings of Bio./Med. and Clinic. are relatively low; one reason is that a significant fraction of the impact these topics have may be over research areas that are outside our data range. citation to a single pair of topics. The two matrices P and P * are shown in Tables 10-11 of the supplementary material. Both definitions make sense, but the second one leads to a 'sparser' graph, which is presented in Figure 8 (the first one is relegated to Figure 13).In Figure 8 (left), the width of the edge from node k to node ℓ is proportional to P * kℓ , and the edge is presented only when P * kℓ ≥ 0.09. We have interesting observations. First, Exp.Design has relatively few citation exchanges with other topics and the majority of the citations it receives are from the topic itself. Since a one-way edge from node k to node ℓ is presented when P * kℓ ≥ 0.09, no edge from or to Exp.Design is shown in Figure 8. Second, Regression and Math.Stat. are the two topics that have attracted the most citations from other topics, and Bio./Med. and Inference are the two that have cited other topics most often. Third, each of Bayes, Variable Selection, and Mach.Learn. has considerably many outgoing and incoming citations. Last, Hypo.Test and Inference form a close pair, and most in-between citations are from Inference to Hypo.Test; Clinic. and Bio./Med. form a close pair, and the citation exchanges are relatively balanced between them.We then consider topic ranking. Figure 8 (right) shows the export scores of 11 topics by TR-SCORE. Math.Stats. is the highest-ranked topic. This is reasonable, as the focus of Math.Stats. is mathematical analysis and probability, which may have a long-lasting impact on other topics in statistics. Regression and Mach.Learning are also highly ranked. This is also understandable, as the two topics cover many "hot" research topics (see Table 2).The rankings of Bio./Med. and Clinic. are relatively low; one reason is that a significant fraction of their impacts are over research areas outside our data range.
ConclusionText analysis is a rapidly developing research area in data science. In this paper, we have surveyed recent methods for text analysis, ranging from topic modeling to neural language models. For topic modeling, we have discussed the anchor word condition, several different algorithms, optimal rates, and extensions to bigram and supervised models. In particular, we focus on Topic-SCORE, a fast algorithm that enjoys appealing theoretical properties.For neural language models, we provided a brief introduction to its key components, reviewed the popular BERT and word embedding models, and discussed how to apply them to solve downstream NLP tasks.We have also presented a data set, MADStat, about academic publications in statistics.It was collected and cleaned by ourselves with substantial efforts. We have made it publicly available at http://zke.fas.harvard.edu/MADStat.html. In this paper, we analyzed text abstracts of the papers in MADStat, using the Topic-SCORE algorithm. We discovered 11 representative topics and visualized the trends and pattens in statistical research. We also proposed the Hoffman-Stigler model to jointly model text abstracts and citation data and the TR-SCORE algorithm for ranking the citation impacts of 11 topics. These results are not only applications of text analysis but also can be viewed as a data-driven review of the academic statistical community.Nowadays, a vast amount of text data are generated on a daily basis. Recent advancements in Natural Language Processing (NLP) have revolutionized our everyday lives.This also provides a big opportunity to statistics. The statistical approaches to NLP are typically transparent, sample-efficient, fast-to-compute, and theoretically tractable, mak-ing them a suitable choice for many ordinary NLP users (who may have a moderate-size domain-specific corpus but cannot access the data and resources owned by those tech giants). On the other hand, statistical text analysis is still quite under-developed. Even for topic modeling, there are still many unresolved problems, such as how to estimate the number of topics and how to improve the accuracy when the documents are extremely short.We hope that this review article provides useful information to researchers interested in this area. We also hope that the MADStat dataset, which we collected and shared with public, serves as a good platform for testing existed methods and inspiring new research in text analysis. Table 3: For each of the 36 journals, we present the full name, abbreviated name, starting time, total number of authors, total number of papers, and impact factors in 2014 and 2015. For each journal, our data set consists of all papers between a certain year (i.e., the starting time) and 2015. The starting time is not necessarily the year the journal was launched. comparisons between ranking with our data set and ranking with the Google Scholar data). 
Abbrev
F The sleeping beauty citation patternsThe "sleeping beauty" pattern is especially interesting. To identify papers with such a pattern, we need a metric. We adapt the approach in [25]. Fix a paper i. Suppose T i years (or months/quarters) have passed since its publication by the end of 2015. Let n i (t), 1 ≤ t ≤ T i , be the number of citations the paper receives in year t. Suppose the citation counts reach the peak at year t = t * i . The sleeping beauty metric is defined to beB i = t:1≤t≤t * i n i (t * i )/t * i − n i (t)/t /[(n i (t) ∨ 1)/t]. (F.1)Intuitively, between Year 1 and t 0 , the citation counts may grow superlinearly, linearly, or sublinearly, and B i is positive, approximately 0, or negative, respectively. If paper i is a sleeping beauty, then we expect that (a) n i (t * i ) (maximum number of yearly citations) is large, and (b) B i is large (i.e., we expect the citation counts to grow superlinearly between Year 1 and t * i so B i is large). Note also that for a sleeping beauty, the citation counts may drop after Year t * i but should remain at a relatively high level for at least a few more years. Since "sleeping beauty" is a special kind of highly cited papers, we start by selecting the 300 papers with the largest maximum number of yearly citations. We then arrange all papers according to the sleeping beauty measure B i . Table 5 presents the 14 papers (among the 300) with the largest B i , and Figure 10 of the supplement presents the citation curve n i (t) for the first 8 papers on the list. All of these papers show a clear sleeping beauty pattern, suggesting that the introduced measure is reasonable. qq qq q qqq q q q q q q q q q qq q 1995 2005 2015 0 50 100 200 qq qq q qqq q q q q q q q q q qq q B = 145 q qq qqqqqqqq q qqq q q q q q q q q q q q q q q q q 1985 1995 2005 2015 0 10 20 30 q qq qqqqqqqq q qqq q q q q q q q q q q q q q q q q B = 138.7 qqq qqq q q qqq q q q q q q qq q q q q qq q q q q q q 1985 1995 2005 2015 0 5 10 15 20 qqq qqq q q qqq q q q q q q qq q q q q qq q q q q q q B = 114.8 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 1980 2000 0 5 10 15 20 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q B = 82.3 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 1980 2000 0 5 10 15 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q B = 80.3 q q q q q q q q q q q q q q q q 2000 2010 0 10 20 30 q q q q q q q q q q q q q q q q B = 78.9 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 1985 2000 2015 0 10 30 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q B = 77.9 qqq q qq q q qq qq q q q q q qq q 1995 2005 2015 0 5 10 15 20 qqq q qq q q qq qq q q q q q qq q B = 77.5  5.
G Pre-processing of the abstract dataThe standard preprocessing includes: (i) tokenization, which breaks each abstract into a bag of words; (ii) removing numbers and punctuations; (iii) removing stop words, such as a, the, this, those, me; and (iv) stemming, which helps unify different forms of the same word, such as testing, test, and tests. The default functions in the R package tm are not customized for the content of statistical abstracts. We thus add some manual adjustment.First, our dictionary only allows single words, and for important phrases we must include, we have to suppress them first. For example, when tokenizing the documents, we on the output of topic-SCORE for K = 4 and K = 5, respectively. We compare the two outputs and re-order the topics for K = 5 so that the first 4 topics have a one-to-one correspondence to the topics for K = 4. After checking the anchor words of the 5th topic for K = 5 and using our knowledge of the field of statistics, we think this topic can be interpreted as "Regression" and is meaningful. We thus prefer K = 5 to K = 4.Similarly, we successively compare each pair of nested values of K. For each of 5 ≤ k ≤ 11, we find that increasing K from k − 1 to k leads to the discovery of new topics that are meaningful. However, when we increase K from 11 to 12, it is not the case. Table 7 displays the 20 most frequent anchor words for each topic in the output of K = 12. We use the anchor word list to match each topic with one of the 11 topics in the output of K = 11 (see Figure 4 of the main article). We find that 11 out of the 12 discovered topics can be matched to one of 11 topics in Figure 4. The 12th discovered topic (last row of Table 7) is not very meaningful to be listed as a new topic (the 'anchor words' such as rootn, longmemori, censorship may be used by abstracts in different research areas of statistics).We thus prefer K = 11 to K = 12. We also investigate 12 < K ≤ 16 and find that these results are all less interpretable than that of K = 11. We decide that K = 11 is the most appropriate choice.How to select K in a topic model is a well-known challenging problem. To our best knowledge, there exists no method that works universally well. In theory, the singular values of D (i.e., the scree plot) contain information of K [28], but the scree plot of our data set is not informative enough for us to pin down the exact value of K (see Figure 11, Table 6: The 20 most frequent anchor words of each topic when K = 4 (top) or K = 5 (bottom). We have re-ordered topic labels so that the first 4 topics for K = 5 have similar interpretations as the topics for K = 4. where we only use the plot to determine a range of possible K). The perplexity [5] is a commonly used metric to assess the goodness-of-fit of a topic model. We may select K by minimizing the perplexity, but this approach is known to be unstable [46]. It tends to select a very large K on our data set, making the interpretation/labeling of topics difficult.Other ideas of estimating K include the Bayesian approach which puts a prior on K and computes the posterior, but it is unclear how to combine this idea with the topic-SCORE algorithm. We have tried many different approaches and found that the most satisfactory one is investigating the interpretability of discovered topics using our knowledge of the field, as described above.Table 7: The 20 most frequent anchor words of each topic when K = 12. We have reordered topic labels so that the first 11 topics have similar interpretations as the topics for K = 11 (see Figure 4 of the main article). 
I High-weight papers in each of the 11 topicsIn Section 5, we perform topic learning using the abstracts of 56, 500 papers and identify 11 topics. We propose a label for each topic using the topic loading vectors (see Figure 4 of the main article). The short label is often insufficient to describe all the research topics that this topic covers. We further study each topic by investigating papers with high weights on this topic. For each 1 ≤ k ≤ 11, we sort the paper abstracts in the descending order of ŵi (k).Table 8 shows the titles of the three abstracts with the largest ŵi (k). The results are largely consistent with the proposed topic labels. Moreover, for each topic k, by reading the titles of the 300 papers with highest weights on this topic, we come up with a list of suggested research topics umbrellaed by this topic. See Table 2 of the main article.
J The topic interests of 80 representative authorsIn Section 5, we use the output of topic learning to define a centered topic interest vector z a ∈ R 11 for each author a. To recap, for each author a, let N a ⊂ {1, 2, . . . , n} be the collection of papers published by this author in our data range, where each paper i has an estimated topic weight vector ŵi for its abstract. The centered topic interest vector z a isz a = wa − w,where wa is the average of ŵi over all abstracts in N a and w be the average of ŵi over all (n = 56, 500) abstracts. The entries of z a sum to 0, and so it has both positive and negative entries. We are interested in positive entries of z a : Author a has greater-thanaverage weight on topic k if z a (k) > 0, for 1 ≤ k ≤ 11. See Figure 5 and details therein.We now use z a to define the "major topics" of author a and show the results for 80 representative authors. Fix an author a. We call topic k a "major topic" of author a if z a (k) > 50% × max 1≤ℓ≤11 {z a (ℓ)}.We may change 50% to (50% ± 5%) but the results are similar.Table 9 presents the major topics of 80 authors with highest citations (ordered alphabetically). We remark again that the short topic labels may not be accurate for all research areas each topic covers, and it is always useful to consult Table 2 of the main article.
K Topic trends in 7 representative journalsIn Section 5.3, we have selected a few journals and study how the evolution of the yearly average topic weights for each journal. Based on the journal ranking by the Stigler's model and PageRank (see Section 4.4), we select the 7 journals with highest average ranks: AoS, Bka, JASA, JRSSB, Bcs, JMLR, and Sini. For each journal, we obtain the yearly average topic weight (i.e., the average of ŵi among papers published in this journal each year) and smooth the curves as before. The results are in Figure 12. While we may plot the average weights of different topics in the same journal, we choose to plot the average weights of the same topic in different journals. In Figure 12, each panel corresponds to a topic, and different curves in each panel represent different journals.q q q q q q q q q q q q q q q q q q q q q q q q q q 1990 1995 2000 2005 2010 2015 0.08 0.10 0.12 0.14 0.16 Bayes q q q q q q q q q q q q q q q q q q q q q q q q q q 1990 1995 2000 2005 2010 2015 0.06 0.08 0.10 0.12 0.14 0.16Bio./Med.q q q q q q q q q q q q q q q q q q q q q q q q q q 1990 1995 2000 2005 2010 2015 0.04 0.06 0.08 0.10 0.12 Clinic.q q q q q q q q q q q q q q q q q q q q q q q q q q 1990 1995 2000 2005 2010 2015 0.05 0.10 0.15Exp.Design q q q q q q q q q q q q q q q q q q q q q q q q q q 1990 1995 2000 2005 2010 2015 0.08 0.10 0.12 0.14 Hypo.Test q q q q q q q q q q q q q q q q q q q q q q q q q q 1990 1995 2000 2005 2010 2015 0.05 0.06 0.07 0.08 0.09 0.10 Inference q q q q q q q q q q q q q q q q q q q q q q q q q q 1990 1995 2000 2005 2010 2015 0.04 0.05 0.06 0.07 0.08 0.09 0.10 Latent.Var. q q q q q q q q q q q q q q q q q q q q q q q q q q 1990 1995 2000 2005 2010 2015 0.05 0.10 0.15 0.20 0.25 Mach.Learn. q q q q q q q q q q q q q q q q q q q q q q q q q q 1990 1995 2000 2005 2010 2015 0.08 0.10 0.12 0.14 0.16 0.18Math.Stats. q q q q q q q q q q q q q q q q q q q q q q q q q q 1990 1995 2000 2005 2010 2015 0.10 0.12 0.14 0.16 Regression q q q q q q q q q q q q q q q q q q q q q q q q q q 1990 1995 2000 
L The cross-topic citation weightsIn Section 6.3, we have introduced two definitions of the cross-topic-citation graph. In the first definition, for each 1 ≤ k ̸ = ℓ ≤ K, there is a directed edge from topic k to topic ℓ with weight P kℓ = N kℓ /( K m=1 N km ), whereN kℓ = n i,j=1ŵi (k) ŵj (ℓ)C ij .In the second definition, for each 1  It is seen from Tables 10-11 that distribution of elements in P are more heavy tailed.≤ k ̸ = ℓ ≤ K,As a result, if we apply the same threshold P to P * to get two binary matrices, the one Table 10: The cross-topic citation matrix P * (by dominant topics). The diagonal elements show the proportions of within-topic-citations. The off-diagonal elements that are ≥ 0.09 are marked grey. This matrix is used to construct the graph in Figure 8  associated with P is sparser and may be more interesting. For this reason, we choose to present the graph associated with P (thresholded at 0.09) in the main text; see Figure 8.The graph associated with P * (thresholded at 0.11) is shown in Figure 13.The diagonal elements of P and P * show the proportion of within-topic-citations for each topic. We observe that Exp.Design, Hypo.Test, Math.Stats. and Regression are the topics whose proportions of within-topic-citations are relatively high, and that Bio./Med., Inference and Latent.Var are the topics whose proportions of within-topic-citations are relatively low.6CTR-SCORE: an extension of Topic-SCORE for topic ranking 6.1 The Hofmanm-Stigler model for abstract and citation data . . . . . . . . . 6.2 Topic-Ranking SCORE (TR-SCORE) . . . . . . . . . . . . . . . . . . . . . 6.3 Topic-ranking and a cross-citation graph . . . . . . . . . . . . . . . . . . . The list of 36 journals D Additional results on paper counts E Additional results on network centrality F The sleeping beauty citation patterns G Pre-processing of the abstract data H Selection of the number of topics K I High-weight papers in each of the 11 topics J The topic interests of 80 representative authors K Topic trends in 7 representative journals L The cross-topic citation weights 1 Introduction
A 1 ,1. . . , A K ∈ R p , we assume: (a) w i (k) is document i's 'weight' on topic k, 1 ≤ k ≤ K, and (b) given that the document is (purely) discussing topic k, the population word frequency vector is A k . Combining (a)-(b) and (2.1), it is reasonable to assumeΩ i = n k=1 w i (k)A k . Write Ω = [Ω 1 , Ω 2 , . . . , Ω n ], A = [A 1 , . . . , A K ],and W = [w 1 , w 2 , . . . , w n ]. It follows that Ω = AW.
curves, we notice a sharp increase near 2005-2006, possibly because several new journals (AoAS, Bay, EJS) were launched between 2006 and 2008; see Table3of the supplementary material. The middle panel of Figure1presents the yearly paper counts, defined as the average number of papers per active author. We consider both standard count and fractional count, where for an m-author paper, each author is counted as published 1 and 1/m papers, respectively. In the standard count, the yearly paper counts increase between 1975 and 2009, from about 1.2 paper per author to about 1.4 paper per author, and decrease after 2009, to about 1.3 paper per author in 2015. In the fractional count, the yearly paper counts always decrease, from about 0.85 paper per author in 1975 to about 0.5 paper per author in 2015. This can be explained by that the average number of authors per paper has been steadily increaseing over the years. See the right panel of Figure1, where we present the average number of authors per paper; the curve is seen to be steadily increasing.The above counts can be further explained by Figure9of the supplementary material, in which (a) the paper count each year is partitioned into the counts of m-author papers for different m and (b) the author count each year is partitioned into the counts of k-yearsenior author for different k. The results show some interesting patterns, and we refer the readers to Section D of the supplementary material for details.
Figure 3 ≤ 10 Figure 7 Figure 1 :31071Figure S1: Left: total numbers of papers and active authors in each year; middle: average number of papers per author in each year; right: average number of authors per paper in each year.
4. 33Citation patterns and the sleeping beautiesIdentification of representative citation patterns is an interesting problem, as it helps distinguish short-term citation effects from long-lasting citation effects. By a careful study of the yearly citation curves of individual papers, we identify four representative citation patterns: "sleeping beauty," "transient," "steadily increasing," and "sudden fame." "Sleeping beauty" refers to the papers that receive low citations within a few years after publication but become frequently cited after a certain point (a.k.a. "waking up"). Representative papers include the lasso paper, Tibshirani (1996), and the FDR paper, Benjamini and Hochberg(1995). "Transient" refers to the papers that receive a good number of citations for a few years shortly after publication, but then their citations drop sharply and remain low for years. "Steadily increasing" refers to those papers whose citations have been increasing at a modest rate for many years, with a large number of citations over a relatively long time period. Representative papers include Dempster et al. (1977) on EM algorithm. "Sudden fame" refers to papers that receive a large number of citations shortly after publication and the citations remain high for many years. Representative papers include Liang and Zeger (1986) on longitudinal data, Gelfand and Smith (1990) on marginal densities, and Efron et al. (2004) on LARS. See Figure 2. longitudinal data, Gelfand and Smith (1990) on marginal densities, and Efron et al. (2004)
Figure 2 :2Figure 2: Yearly citation curves for 4 papers. Left to right: "sleeping beauty" (Tibshirani (1996) on Lasso), "transient", "steadily increasing" (Dempster, Laird and Rubin (1977) on EM algorithm), and "sudden fame" (Liang and Zeger (1986) on GLM).
Figure 10 in10Figure 10 in the supplementary material show the papers with largest B i , such as Tibshirani (1996), Azzalini (1985), Hubert & Arabie (1985), Hill (1975), Marcus et al. (1976), Lunn et al. (2000), Rosenbaum & Rubin (1983), Bai & Saranadasa (1996), Holm (1979), Clayton (1978), and Fan & Li (2001).
Figure 3 :3Figure 3: Journal ranking. Each point is a journal (x-axis: ranking by PageRank, y-axis: ranking by Stigler's model). See Table3of the supplement for the full journal names.
Figure 4 :4Figure 4: For 1 ≤ k ≤ K (where K = 11), Panel k is the barplot of the 20 words j that have the largest weight a j (k) among all words (the length of each bar is the value of a j (k)).
Figure 5 :5Figure 5:The overall topic interests of some authors. For interpretation purpose, we select some authors we are familiar with, but similar figures can be generated for other authors.
Figure 6 :6Figure 6: The yearly average topic weights (averaged for all 33 journals), 1990 -2015.
have higher-than-average weights, suggesting that they have attracted more attention; from 1990 to 2015, the weight of Bio./Med. increases relatively fast, the weights of Math.Stat. and Hypo.Test gradually decrease, and the weights of Regression and Bayes are relatively flat. Among the remaining 6 topics, Mach.Learn. increases quickly; its weight has passed the overall average starting from 2014 (Latent.Var. is another topic where the weight is steadily increasing).Second, we select a few journals and study how the evolution of the yearly average topic weights for each journal. In Section 4.4 we have ranked the 33 journals (excluding 3 probability journals) by the Stigler's model and PageRank. We select the 7 journals with highest average ranks: AoS, Bka, JASA, JRSSB, Bcs, JMLR, and Sini. For each journal, we obtain the yearly average topic weight (i.e., the average of ŵi among papers published in this journal each year) and smooth the curves as before. The results are in Figure12of the supplementary material. A partial result is shown in Figure7. Each panel corresponds to a topic. Fixing a topic k, for each journal, we plot the kth entry (subject to smoothing over time) in the yearly average of ŵi 's among papers published in this journal. These curves of different journals for the same topic can be used to study journal friendliness to this topic.
Figure 3 :3Figure3: Left: The weighted directed graph for cross-topic citations. The diameter of a node (topic) is proportional to the total citations the topic has received from other topics, and the width of an edge is proportional to the weight defined in the text. An edge is presented if the weight is bigger than 0.09. Right: The estimated export scores of 11 topics (subject to median(μ 1 , . . . , μ11 ) = 0).
edge from node k to node `is presented when P k` 0.09, no edge from or to Exp.Design is shown in Figure 3. Second, Regression and Math.Stat. are the two topics that have attracted the most citations from other topics, and Bio./Med. and Inference are the two topics that have cited other topics most often. Third, each of the three topics, Bayes, Variable Selection, and Mach.Learn. has significantly cited and been cited by other topics. Last, Hypo.Test and Inference form a close pair, and most citations between them are from Inference to Hypo.Test. Clinic. and Bio./Med. also form a close pair, and the citation exchanges are relatively balanced between them.
Figure 8 :8Figure 8: Left: The weighted directed graph for cross-topic citations. The diameter of a node (topic) is proportional to the total citations the topic has received from other topics, and the width of an edge is proportional to the weight defined in the text. An edge is presented if the weight is bigger than 0.09. Right: The estimated export scores of 11 topics (subject to median(μ 1 , . . . , μ11 ) = 0).
Figure 10 :10Figure 10: The yearly citation curves for the first 8 papers in Table5.
Figure 11 :11Figure 11: Scree plot of the text corpus matrix D. Left: top 30 singular values. Right: omitting first two singular values for a better visualization.
Figure 13 :13Figure 13: The cross-topic citation graph associated with P * (the width of edge if proportional to the weight of this edge; only edges with a weight ≥ 0.11 are shown).
Table 44cleaning such citation data. Compared to Google Scholar, our citation data are of higherquality, so our results on network centrality shed new light that Google Scholar cannotprovide., if we instead use the citation counts by Google Scholar on December 31, 2022, then the papers Benjamini & Hochberg (1995) on FDR, Donoho & Johnstone (1994) on wavelets, and Efron et al. (2004) on LARS will receive better rankings, as these papers have many citations from papers outside our data range. Despite this, our approach is still valuable. For example, using our data, we can provide the ranking (e.g., by number of citations) for any author or any paper in our data set, but how to do this using Google Scholar is unclear: We need to build a large database for the citation relationships between many authors and papers and spend substantial time
Table 1 :1The top 10 authors ordered by the number of coauthors, citers, and citations, respectively (we only count co-authors and citations within the range of MADStat).Author name#Coauthors Author name#Citers Author name#CitationsRaymond Carroll234Donald B. Rubin5337Peter Hall6847Peter Hall222Nan Laird5079Donald B. Rubin6825N. Balakrishnan186Bradley Efron4500Jianqing Fan5726Jeremy Taylor159Robert Tibshirani4076Robert Tibshirani5074Joseph Ibrahim158Peter Hall3789Nan Laird5040Geert Molenberghs146Arthur P. Dempster3406Bradley Efron4589James S. Marron130Scott Zeger3311Raymond Carroll4415Malay Ghosh119Kung Yee Liang3231Scott Zeger3802Emmanuel Lesaffre119Trevor Hastie3174Trevor Hastie3582Xiaohua Zhou119Raymond Carroll3110Kung Yee Liang3366
Table 2 :2Interpretation of the 11 estimated topics.Topic LabelAbbreviation Corresponding Research Topics1 Bayesian statistics BayesBayesian methods2 Bio & medicalBio/Med. Observational studies, genetics, genomicsstatistics3 Clinical trialsClinic.Clinical trials, causal inference4 ExperimentalExp.Design Experimental designdesign5 Hypothesis testing Hypo.Test Hypothesis testing, goodness of fit6 StatisticalInference Confidence intervals, bootstrapping, empirical likelihoodinference7 Latent variablesLatent.Var. Latent variable model, incomplete data, mixtures, clustering, factormodel, graphical model, variable selection, categorial data analysis,dimension reduction8 Machine learningMach.Learn.Machine learning, computation, EM algorithm, Monte Carlomethods, clustering9 MathematicalMath.Stats. Asymptotics, mathematical statistics, probability, stochasticstatisticsprocess10 RegressionRegression Linear models, nonparametric regression, quantile regression,analysissemi-parametric models11 Time seriesTime Se-ries Time series, longitudinal data, stochastic processes, survival analysis topics umbrellaed by each of the brief topic label. See the third column of Table2.
presents z a for 12 representative authors. We have some interesting findings. 1) James Berger has a prominently high weight on Bayes; Raymond Carroll and Jianqing Fan have prominently high weights on Regression; and Michael Jordan and Jun Liu have prominently high weights on Mach.Learn. These results are reasonable: Berger has many works in Bayesian statistics and decision theory; Carroll has many works in semiparametric models; Fan has many works in nonparametric regression and high dimensional variable selection; Jordan has many works in machine learning, nonparametric Bayes, and Bayesian computation; and Liu has many works in Bayesian computation and MCMC. 2) Peter Hall has notably high weights on Inference, Mach.Learn., and Regression; Xihong Lin has notably high weights on Clinic., Regression, and Bio./Med.; Larry Wassermann has notably high weights on Inference, Mach.Learn., and Bayes; and Cun-Hui Zhang has notably high weights on Inference, Regression, and Math.Stat.. 3) Figure
Table 22(e.g. Time Series includes longitudinal data and survival analysis, and it is why this topic has a high weight in the journal Bcs). Among the 7 journals, JMLR has a significantly Furthermore, the 4 journals, AoS, Bka, JASA and JRSSB, are traditionally considered the leading journals in statistical method and theory. Among these 4 journals, AoS is friendlier to Math.Stat., Inference, Hypo.Test, Regression, and Exp.Design; JASA is friendlier to Mach.Learn., Bio./Med., Clinic. and Time Series; JRSSB is friendlier to Mach.Learn., Bayes, and Var.Select.; and Bka is friendlier to Bayes and Regression (JASA publishes more on Clinic. and Bio./Med. than Bka; this is possibly due to that JASA has a casestudy sector).BayesBio./Med.0.180.120.140.160.110.120.140.100.100.120.100.090.080.08199019952000200520102015199019952000200520102015199019952000200520102015higher weight on Mach.Learn. than on the other topics, Bcs has a significantly higher weight on Bio./Med. and Clinic., and AoS has a considerably higher weight on Math.Stat..
Table 4 :4The most-cited papers (only the citations within MADStat are counted).Rank AuthorYear Title
Table 5 :5The 14 papers with the largest sleeping beauty measures B (among the 300 papers that have the largest maximum yearly citation counts). TC is total citation counts.PaperJournal T CBPaperJournal TC B1. Tibshirani (1996)JRSSB 1327 145 8. Bai & Saranadasa (1996)Sini862. Azzalini (1985)ScaJS288 139 9. Holm (1979)ScaJS2653. Hubert & Arabie (1985)JClas179 115 10. Clayton (1978)Bka3934. Hill (1975)AoS280 8211. Fan & Li (2001)JASA7755. Marcus et al. (1976)Bka218 8012. Turnbull (1976)JRSSB 3466. Lunn et al. (2000)SCmp198 7913. Pickands (1975)AoS2347. Rosenbaum & Rubin (1983) Bka413 7814. Benjamini & Hochberg (1995) JRSSB 695
Table 8 :8For each of the 11 topics, the titles of the three papers that have the highest topic weight in that topic (last column: topic weight in that topic).TopicTitle
there is a directed edge from topic k to topic ℓ with weight P * kℓ = N * ∈ {e 1 , e 2 , . . . , e K }. Here, ŵ * i encodes the 'dominant topic', i.e., ŵ * i = e k if and only if k = argmax{1 ≤ m ≤ K : ŵi (m)}. The two 11 × 11 weighted adjacency matrices P and P * are presented in Table 10 and Table 11, respectively.kℓ /( K m=1 N  *  km ), wherenN  *  kℓ =ŵ *i,j=1InferenceMach.Learn.Exp.DesignHypo.TestMath.Stats.RegressionBio..Med.BayesClinic.Time.SeriesLatent.Var.i (k) ŵ * j (ℓ)C ij ,with ŵ * i
Table 11 :11of the main article. Bayes Bio./Med Clinic. Exp.Des Hypo.Test Inference Latent.Var Mach.Learn Math.Stats Regression Time Seri. The cross-topic citation matrix P (by topic weights). The diagonal elements show the proportions of within-topic-citations. The off-diagonal elements that are ≥ 0.11 are marked grey. This matrix is used to construct the graph in Figure 13. Bayes Bio./Med Clinic. Exp.Des Hypo.Test Inference Latent.Var Mach.Learn Math.Stats Regression Time Seri.Bayes.230.057.046.013.070.056.066.127.130.134.072Bio./Med..096.143.099.029.081.048.070.081.081.169.101Clinic..076.090.339.050.064.034.060.061.036.098.091Exp.Design .029.049.079.562.056.030.034.034.039.064.024Hypo.Test.062.048.038.019.454.049.038.041.092.112.048Inference.088.054.034.026.103.242.064.063.124.148.054Latent.Var. .092.053.047.014.048.046.256.116.079.203.046Mach.Learn. .123.055.039.017.048.044.097.312.087.122.056Math.Stats. .102.041.018.013.068.071.077.073.347.126.064Regression.073.047.030.015.055.050.096.061.087.431.055Time Series .089.072.066.013.057.045.046.076.090.141.303Bayes.125.101.076.031.107.072.068.092.123.138.068Bio./Med..113.108.084.034.099.072.070.090.117.139.073Clinic..111.107.108.040.096.068.071.088.109.128.073Exp.Design .088.090.080.207.086.063.056.075.099.108.048Hypo.Test.119.099.075.033.130.073.063.083.126.138.063Inference.108.099.073.034.101.100.074.091.118.138.064Latent.Var. .110.100.077.031.090.077.101.099.112.138.064Mach.Learn. .115.102.076.034.094.076.077.110.114.135.068Math.Stats. .116.100.073.033.107.073.067.086.135.144.065Regression.113.100.072.032.100.074.070.088.125.159.066Time Series .112.107.083.028.092.069.067.091.112.132.106



AoSJRSSB Bka JASA JMLR StSci Bcs Biost Bern JSPI Sini JTSA JCGS AoAS JRSSA ScaJS JRSSC CanJS JMVA JNS EJS SMed CSDA AISM SPLet Extrem SCmp JClas ISRe AuNZ Bay CSTM JoAS


The data and code for text analysis conducted in this article can be found at multiple repositories, including the journal website (https://www.annualreviews.org/ doi/abs/10.1146/annurev-statistics-040522-022138), GitHub (https://github.com/ ZhengTracyKe/MADStat-Text), and Harvard Dataverse (https://dataverse.harvard. edu/dataset.xhtml?persistentId=doi:10.7910/DVN/YIXS6B).


TR-SCORE: an extension of Topic-SCORE for topic rankingTopic-SCORE is a flexible idea and can be extended in many directions. In this section, we extend Topic-SCORE by proposing Topic-Ranking-SCORE (TR-SCORE) as new approach to ranking the citation impacts of different topics. Since TR-SCORE is directly motivated by the analysis of MADStat, we focus our discussion on the MADStat dataset in this section but keep in mind that the idea is useful in other applications.In Section 4, we have discussed how to use citation exchanges to rank different journals. We can extend the idea to topic ranking, but there is a major challenge: citation exchanges between papers or journals are well-defined and directly observable, but citation exchanges between research topics are not well-defined and directly observable. We tackle this by combining the abstracts and the citation data: we first propose a model that jointly models text abstracts and citations, including an idea to measure the (unobserved) citation exchanges between research topics. We then introduce TR-SCORE, and use it to rank different topics and to construct a knowledge graph visualizing the cross-topic citation exchanges.
The Hofmanm-Stigler model for abstract and citation dataConsider n papers in MADStat, where the abstract data are summarized in a p × n word- We assume that all the paper abstracts focus onInspired by the Stigler's model, we introduce µ = (µ 1 , µ 2 , . . . , µ K ) ′ , where µ k is the export score associated with topic k, 1 ≤ k ≤ K. Intuitively, a topic with a larger export score means that it has larger impacts. Now, fix 1 ≤ i ≤ n and consider paper i. Similarly as in Section 2, let w i ∈ R K be the weight vector of document i (i.e., w i (k) is the weight that abstract i puts on topic k). When paper i is cited by another paper j, we have two different ways to attribute this particular citation count.• (Orthodox Citation Attribution (OCA)). We simply attribute the citation to paper i.• (Topic Weight Citation Attribution (TWCA)). We attribute the citation to each of the K topics, with weights w i (1), . . . , w i (K), respectively (note that K k=1 w i (k) = 1).
A Data collection and cleaningOne might think that our data sets is easy to obtain, as it seems that BibTeX and citation data are easy to download. Unfortunately, when we need a large-volume high-quality data set, this is not the case. For example, the citation data by Google Scholar is not very accurate, and many online resources do not allow for large volume downloads. Our data are downloaded using a handful of techniques including, but not limited to, web scraping.The data set was also carefully cleaned by a combination of manual efforts and computer algorithms we developed. Both data collection and cleaning are sophisticated and timeconsuming processes, during which we have encountered a number of challenges.The first challenge is that, for many papers, we need multiple online resources to acquire the complete information. For example, to download complete information of a paper, we might need online resources 1, 3, and 5 for paper 1, whereas online resources 2, 4, and 6for paper 2. Also, each online resource may have a different system to label their papers.As a result, we also need to carefully match papers in one online resource to the same ones in another online resource. These make the downloading process rather complicated.The second challenge is name matching and cleaning. For example, some journals list the authors only with the last name and first initial, so it is hard to tell whether "D.Rubin" is Donald Rubin or Daniel Rubin. Also, the name of the same author may be spelled differently in different papers (e.g., "Kung-Yee Liang" and "Kung Yee Liang"). A more difficult case is that different authors may share the same name (e.g., Hao Zhang at Purdue University and Hao Zhang at Arizona State University). To correctly match the names and authors, we have to combine manual efforts with some computer algorithms.Last, an online resource frequently has internal inconsistencies, syntax errors, encoding issues, etc. We need a substantial amount of time and efforts to fix these issues.
B DisclaimerIt is not our intention to rank a researcher (or a paper, or an area) over others. For example, when we say a paper is "highly cited," we only mean that the citation counts are high, and we do not intend to judge how important or influential the paper is. Our results on journal ranking are based on journal citation exchanges, but we do not intend to interpret the ranking more than the numerical results we obtain from the algorithms we use.As our data set is drawn from real-world publications, we have to use real names, but we have not used any information that is not publicly available. For interpretation purposes, we frequently need to suggest a label for a research group or a research area, and we wish to clarify that the labels do not always accurately reflect all the authors/papers in the group.Our primary interest is the statistics community as a whole, and it is not our intention to label a particular author (or paper, or topic) as belonging to a certain community (group, area).While Due to the limited scope of our data set, some of our results may be biased. For example, for the citations a paper has received, we count only those within our data range, so the resultant citation counts may be lower than the real counts the paper has received.Alternatively, for each paper, we can count the citation by web searching (e.g., GoogleScholar, which is known to be not very accurate), or by reference matching (e.g., Web of Science and Scopus). Our approach allows us to perform advanced analysis (e.g., ranking authors/papers by citation counts, reporting the most cited authors and papers, excluding self-citations, and calculating cross-journal citation). For such analysis, it is crucial that we know the title, author, author affiliation, references, and time and place where it is published for each paper under consideration. For each of the two alternative approaches, we can gather such information for a small number of papers, but it is hard to obtain such information for 83,336 papers as in our data set.A full scope study of a scientific community is impossible to accomplish in one paper.The primary goal of our paper is to serve as a starting point for this ambitious task by creating a template where researchers in other fields (e.g., physics) can use statisticians'expertise in data analysis to study their fields. For these reasons, the main contributions of our paper are still valid, despite some limitations discussed above.
C The list of 36 journalsThe 36 journals are selected as follows. We start with the 175 journals in the 2010 ranked list of statistics journals provided by the Australian Research Council (ARC). 
D Additional results on paper countsFigure 1 presents the number of papers per year and the number of active authors per year.These results can be further explained using Figure 9. In Figure 9 (left), we present the number of m-authored papers in each year for m = 1, 2, 3 and m ≥ 4, respectively. It is seen that the fraction of single author papers have been steadily decreasing, and the fraction of papers with 3 or more authors have been steadily increasing. One possible reason is that, as statistics becomes increasingly more interdisciplinary, publishing in statistical journals has been increasingly more challenging, as statisticians need to coauthor with researchers from other scientific areas, for their data sets or expertise in their areas, and often works on methods and theory alone are not adequate for publication. Figure 9 (right) presents the number of active authors with k-year seniority in each year for k in some different ranges. We say that an author is k-year-senior in year t if this author's first paper appears in year t − k in our data set. The plot shows a significant increase of authors with seniority < 3 years, suggesting that the statistics community has attracted more and more junior authors. The cohort with seniority < 3 years and the cohort with seniority > 10 years have the largest and second largest fractions. One possible explanation is that a more senior author tends to have more junior collaborators (e.g., a senior professor tends to have more Ph.D students than a less senior professor); such forged collaborations have improved the productivity of both the senior cohort and the junior cohort.  
E Additional results on network centralityTable 4 presents the top 10 most-cited papers. Note that the numbers of coauthors, citers, and citations here are all counted using only the papers in our data range, so there may be some biases in our ranking. See Section 4.2 for more discussion on ranking (and especially encounter phrases such as test error and monte carlo. We suppress them by testerror and montecarlo respectively, before we insert them to the dictionary. Second, stemming may sometimes mistakenly combine words with significantly different meanings. For example, the words measurement and measure have the same stem measur, but very different meaning in our context. To make sure that they are stemmed differently, we replace measurement by measurement1 before stemming, so the stems of measurement and measure become mea-sur1 and measur respectively. Third, the default stop word list in the R package tm does not cover all "topic-irrelevant words" for the analysis of statistical abstracts. We manually add a list of 289 words (some overlap with the default stop words) to the stop word list.These words include (a) common words used in statistical abstracts, such as data, estimation, paper, method, propose, and discuss; (b) words related to the copyright information of the journal or the press, such as springer, wiley, royal, and sinica; and (c) words arising from citing references in the abstract, such as bickel, berger, and fan.After the above steps, the vocabulary contains more than 60, 000 words, the majority of which have extremely low frequencies in the corpus. Additionally, some abstracts become quite short after removing stop words. As argued in [28], removing low-frequency words and short documents can increase the signal-to-noise ratio. To this end, we first remove all words that appear in fewer than 100 abstracts. This reduces the vocabulary to p = 2, 106.We then remove approximately the 10% shortest abstracts and retain a total of n = 56, 500 abstracts.
H Selection of the number of topics KFirst, we check the scree plot of the text corpus matrix D; see Figure 11. The elbow points are 4 and 16. We thus consider the range of 4 ≤ K ≤ 16.Next, for each 4 ≤ K ≤ 16, we run topic-SCORE (Step 1 of TR-SCORE) to obtain Â and then follow the approach in Section 5 to find the most frequent anchor words for each topic. We use these anchor words to investigate the research areas covered by each discovered topic.For example, Table 6 displays the 20 most frequent anchor words of each topic, based   




A practical algorithm for topic modeling with provable guarantees

SArora


RGe


YHalpern


DMimno


AMoitra


DSontag


YWu


MZhu



International conference on machine learning

PMLR
2013






Learning topic models-going beyond SVD

SArora


RGe


AMoitra



IEEE 53rd Annual Symposium on Foundations of Computer Science

IEEE
2012






Neural machine translation by jointly learning to align and translate

DBahdanau


KCho


YBengio

arXiv:1409.0473

2014


arXiv preprint



A fast algorithm with minimax optimal guarantees for topic models with an unknown number of topics

XBing


FBunea


MWegkamp



Bernoulli

26
3
2020





Latent dirichlet allocation

DMBlei


AYNg


MIJordan



Journal of Machine Learning Research

3

2003






TTCai


ZTKe


PTurner

arXiv:2301.01381
Testing high-dimensional multinomials with applications to text analysis

2023


arXiv preprint



Indexing by latent semantic analysis

SDeerwester


STDumais


GWFurnas


TKLandauer


RHarshman



Journal of the American Society for Information Science

41
6

1990






JDevlin


M.-WChang


KLee


KToutanova

arXiv:1810.04805
BERT: Pre-training of deep bidirectional transformers for language understanding

2018


arXiv preprint



50 years of data science

DDonoho



Journal of Computational and Graphical Statistics

26
4

2017






DDonoho


JJin



Higher criticism for large-scale inference, especially for rare and weak effects

2015
30






When does non-negative matrix factorization give a correct decomposition into parts?

DDonoho


VStodden



Advances in Neural Information Processing Systems

16
2003





Deep convolutional neural networks for sentiment analysis of short texts

CDos Santos


MGatti



Proceedings of COLING 2014, the 25th international conference on computational linguistics: technical papers
COLING 2014, the 25th international conference on computational linguistics: technical papers

2014






Experiments in automatic phrase indexing for document retrieval: A comparison of syntactic and nonsyntactic methods

JLFagan


1988


Cornell University





Fast and robust recursive algorithmsfor separable nonnegative matrix factorization

NGillis


SAVavasis



IEEE Transactions on Pattern Analysis and Machine Intelligence

36
4

2013





The first text retrieval conference (TREC-1)

DKHarman


1993
500


US Department of Commerce, National Institute of Standards and Technology





Long short-term memory

SHochreiter


JSchmidhuber



Neural computation

9
8

1997





Probabilistic latent semantic indexing

THofmann



Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval

ACM
1999







RAHorn


CRJohnson

Matrix Analysis

Cambridge University Press
2013


2nd ed.



Co-citation and co-authorship networks of statisticians

PJi


JJin


ZTKe


WLi



Journal of Business & Economic Statistics

40
2

2022





Fast community detection by SCORE

JJin



The Annals of Statistics

43
1

2015





Network global testing by counting graphlets

JJin


ZKe


SLuo



International Conference on Machine Learning

PMLR
2018






Optimal adaptivity of signed-polygon statistics for network testing

JJin


ZTKe


SLuo



The Annals of Statistics

49
6

2021





Mixed membership estimation for social networks

JJin


ZTKe


SLuo



Journal of Econometrics

2023





A convolutional neural network for modelling sentences

NKalchbrenner


EGrefenstette


PBlunsom

arXiv:1404.2188

2014


arXiv preprint



Defining and identifying sleeping beauties in science

QKe


EFerrara


FRadicchi


AFlammini



Proceedings of the National Academy of Sciences

112
24

2015





Special invited paper: The SCORE normalization, especially for heterogeneous network and text data

ZTKe


JJin



Stat

12
1
2023





Predicting returns with text data

ZTKe


BTKelly


DXiu


2019


National Bureau of Economic Research


Technical report



Using SVD for topic modeling

ZTKe


MWang



Journal of the American Statistical Association October


2022





Assigning topics to documents by successive projections

OKlopp


MPanov


SSigalla


ABTsybakov



The Annals of Statistics

51
5

2023





Discussion of "Coauthorship and citation networks for statisticians

MKolar


MTaddy



Annals of Applied Statistics

10
4

2016





Learning the parts of objects by non-negative matrix factorization

DDLee


HSSeung



Nature

401
6755

1999





BioBERT: A pre-trained biomedical language representation model for biomedical text mining

JLee


WYoon


SKim


DKim


SKim


CHSo


JKang



Bioinformatics

36
4

2020





Supervised topic models

JMcauliffe


DBlei



Advances in Neural Information Processing Systems

2007. 20





A note on EM algorithm for probabilistic latent semantic analysis

QMei


CZhai



Proceedings of the International Conference on Information and Knowledge Management
the International Conference on Information and Knowledge Management

CIKM
2001






TMikolov


KChen


GCorrado


JDean

arXiv:1301.3781
Efficient estimation of word representations in vector space

2013


arXiv preprint



A survey of the usages of deep learning for natural language processing

DWOtter


JRMedina


JKKalita



IEEE Transactions on Neural Networks and Learning Systems

32
2

2020





Improving language understanding by generative pre-training

ARadford


KNarasimhan


TSalimans


ISutskever


2018





End-to-end transformer-based models in textual-based

ARahali


MAAkhloufi



NLP. AI

4
1

2023





Weaving the fabric of science: Dynamic network models of science's unfolding structure

FShi


JGFoster


JAEvans



Social Networks

43

2015





Citation patterns in the journals of statistics and probability

SMStigler



Statistical Science

9

1994





On estimation and selection for topic models

MTaddy



Artificial Intelligence and Statistics

PMLR
2012






Statistical modeling of citation exchange between statistics journals (with discussions)

CVarin


MCattelan


DFirth



Journal of the Royal Statical Society: Series A

179
1

2016





Attention is all you need

AVaswani


NShazeer


NParmar


JUszkoreit


LJones


ANGomez


LKaiser


IPolosukhin



Advances in Neural Information Processing Systems

2017
30





Topic modeling: beyond bag-of-words

HMWallach



Proceedings of the 23rd international conference on Machine learning
the 23rd international conference on Machine learning

2006






Sparse topic modeling: Computational efficiency, near-optimal algorithms, and statistical inference

RWu


LZhang


TTonyCai



Journal of the American Statistical Association


2022





A heuristic approach to determine an appropriate number of topics in topic modeling

WZhao


JJChen


RPerkins


ZLiu


WGe


YDing


WZou



BMC bioinformatics

Springer
2015
16






Aligning books and movies: Towards story-like visual explanations by watching movies and reading books

YZhu


RKiros


RZemel


RSalakhutdinov


RUrtasun


ATorralba


SFidler



Proceedings of the IEEE international conference on computer vision
the IEEE international conference on computer vision

2015








