



Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders




7 Jan 2025





XuemeiGu
xuemei.gu@mpl.mpg.de

Max Planck Institute for the Science of Light

Staudtstrasse 2
91058
Erlangen
Germany




MarioKrenn
mario.krenn@mpl.mpg.de

Max Planck Institute for the Science of Light

Staudtstrasse 2
91058
Erlangen
Germany



Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders



7 Jan 2025


9E5816D9228E2D42D755F0D87434466D
arXiv:2405.17044v3[cs.AI]






GROBID - A machine learning software for extracting information from scholarly documents






The rapid growth of scientific literature makes it challenging for researchers to identify novel and impactful ideas, especially across disciplines. Modern artificial intelligence (AI) systems offer new approaches, potentially inspiring ideas not conceived by humans alone. But how compelling are these AI-generated ideas, and how can we improve their quality? Here, we introduce SciMuse, which uses 58 million research papers and a large-language model to generate research ideas. We conduct a large-scale evaluation in which over 100 research group leaders -from natural sciences to humanities -ranked more than 4,400 personalized ideas based on their interest. This data allows us to predict research interest using (1) supervised neural networks trained on human evaluations, and (2) unsupervised zero-shot ranking with large-language models. Our results demonstrate how future systems can help generating compelling research ideas and foster unforeseen interdisciplinary collaborations.





INTRODUCTIONAn interesting idea is often at the heart of successful research projects, crucial for their success and impact. However, with the accelerating growth in the number of scientific papers published each year [1][2][3], it becomes increasingly difficult for researchers to uncover novel and interesting ideas. This challenge is even more pronounced for those seeking interdisciplinary collaborations, who have to navigate an overwhelming volume of literature.Automated systems that extract insights from millions of scientific papers present a promising solution [2,4,5]. Recent advances have demonstrated that analyzing relationships between research topics across vast scientific literature can reliably predict future research directions [6][7][8][9][10], forecast the potential impact of emerging work [11,12], and identify unconventional avenues for discovery [13]. With the advent of powerful large-language models (LLMs), it is now possible to leverage knowledge from millions of scientific papers to generate concrete research ideas [14][15][16].Yet, a crucial question remains: Are AI-generated research ideas compelling to experienced scientists? Previous studies have only conducted small-scale evaluations with six natural language processing (NLP) PhD students [14], three social science PhD students [15] and ten PhD students in computer science and biomedicine [16]. However, perspectives from experienced researchers -who define and evaluate research projects through grant applications and shape their group's research agendaare essential for assessing the value of new ideas. Involving a larger group of more experienced evaluators could offer deeper insights into what makes a research idea compelling, how to generate and predict them.Here, we introduce SciMuse, a system designed to suggest new personalized research ideas for individual scientists or collaborations. By using 58 million papers and their citation history, and leveraging automated access to GPT-4 [17], SciMuse formulates comprehensive research suggestions. The suggestions were evaluated by more than 100 research group leaders from the Max Planck Society across natural sciences and technology (e.g., from the Institutes for Biogeochemistry, Astrophysics, Quantum Optics, and Intelligent Systems) as well as social sciences and humanities (e.g., from the Institutes for Geoanthropology, Demographic Research, and Human Development). These experienced researchers rank the interest-level of more than 4,400 research ideas generated by SciMuse. This large dataset not only allows us to identify connections between properties of ideas and their interest-level, but also enables us to accurately predict the level of interest of new ideas with two fundamentally different methods: (1) training supervised neural networks and (2) using LLMs for zero-shot prediction without access to human evaluations, which will be important when expensive human-expert data is unavailable. Our results highlight SciMuse's potential to suggest compelling research directions and collaborations, revealing opportunities that might not be readily apparent and positioning AI as a source of inspiration in scientific discovery [18][19][20][21].
RESULTSKnowledge graph generation -While we could directly use publicly available large language models such as GPT-4 [17] or Gemini [26] or Claude [27] to suggest new research ideas and collaborations, our control over the generated ideas would be limited to the structure of the prompt. Therefore, we decided to build a large knowledge graph from the scientific literature to identify the personalized research interests of scientists.The knowledge graph, depicted in Fig. 1(a), consists of vertices, representing scientific concepts, and edges are drawn when two concepts jointly appear in a title or abstract of a scientific paper. The concept list is generated from the titles and abstracts of approximately Fig. 1. SciMuse suggests research ideas or collaborations using a knowledge graph and GPT-4. (a), Knowledge graph generation. Nodes represent scientific concepts extracted from 2.44 million paper titles and abstracts using the RAKE algorithm [22], further refined with custom NLP techniques, manual review, GPT, and Wikipedia (to restore mistakenly removed concepts), resulting in a final list of 123,128 concepts. Edges are formed when two concepts co-occur in titles or abstracts of over 58 million papers from OpenAlex [23], augmented with citation data as a proxy for impact. A mini-knowledge graph illustrates the connections for two example papers [24,25]. (b), AI-generated research collaborations. We extract concepts from the publications of Researchers A and B, refine them using GPT-4, and identify relevant sub-networks in the knowledge graph. GPT-4 then uses these concept pairs, along with the researchers' research information, to generate personalized research ideas or collaboration projects.2.44 million papers from arXiv, bioRxiv, ChemRxiv, and medRxiv, with a data cutoff in February 2023. Rapid Automatic Key-word Extraction (RAKE) algorithm based on statistical text analysis is used to extract candidate concepts [22]. Those candidates are further refined using GPT, Wikipedia, and human annotators, resulting in 123,128 concepts in the natural and social sciences. We then use more than 58 million scientific papers from the open-source database OpenAlex [23] to create edges. These edges contain information about the co-occurrence of concepts in scientific papers (in titles and abstracts) and their subsequent citation rates. This new knowledge graph representation was recently introduced in [12] to predict the impact of future research topics. As a result, we have an evolving knowledge graph that captures part of the evolution of science from 1665 (a text by Robert Hooke on the observation of a great spot on Jupiter [29]) to April 2023. Details of the knowledge graph generation are shown in Fig. 1(a) and Supplementary Information.Personalized research suggestions -We generate personalized research proposals for collaborations between two Max Planck Society group leaders, with one researcher evaluating the AI-suggested proposal.To generate suggestions (Fig. 1(b)), we first identify each researcher's interests by analyzing all their publications from the past two years. Specifically, we extract their concepts from the titles and abstracts of these papers using the full concept list shown in Fig. 1(a). The extracted concepts are further refined by GPT-4, allowing us to build personalized subgraphs in the knowledge graph for each researcher.With the researchers' subgraphs, we then generate a prompt for GPT-4 to create a research project (details in the Supplementary Information). In the prompt, we provide the titles of up to seven papers from each researcher and ask GPT-4 to create a research project based on two Each suggestion represents a potential collaboration between the evaluating researcher (Researcher A) and another researcher (Researcher B) from the Max Planck Society, visualized as bi-colored edges (orange for Researcher A, green for Researcher B). A purple circle indicates collaborations within the same institute, and edge transparency reflects the number of evaluated suggestions. Blue dots denote natural sciences (nat) and red dots represent social sciences (soc). (c): Distribution of interest ratings on a scale from 1 ('not interesting') to 5 ('very interesting'), with 394 suggestions rated as very interesting and 713 rated 4. Ratings are further categorized by whether the collaborations are within or across institutes, and by research field affiliation in either the natural or social sciences.selected scientific concepts. These concepts are chosen in one of three ways: (1) a randomly selected concept pair, (2) the concept pair with the highest predicted impact, or (3) no specific concept pair, relying only on paper titles. We predict the impact (in terms of expected future citations) by adapting the computational methods from [12], applying them to a different and much larger knowledge graph. While the third method does not directly use the knowledge graph features, it serves as a valuable sanity check for our approach (see Supplementary Information). The prompt incorporates self-reflection techniques [30], where GPT-4 generates three ideas, iteratively refines them twice, and then selects the most suitable one as the final result.Large-scale human evaluation -To evaluate the interest level of AI-generated research ideas, we invited 110 research group leaders, who regularly evaluate research proposals and act upon research ideas, from 54 Max Planck Institutes within the Max Planck Society (one of the largest research organizations worldwide), to participate in the evaluation (see Fig. 2(a) and (b)). Each leader evaluated up to 48 personalized research projects on a scale from 1 ('not interesting') to 5 ('very interesting'). Of the 110 researchers, 104 were from natural science institutes and 6 from social science institutes. On average, evaluators had published 59.7 papers (range: 9 to 402) and received 3,759.7 citations (range: 20 to 85,778). In total, we received 4,451 responses. As shown Fig. 3. Analysis of interest levels versus knowledge graph features. We analyzed how eight features of the knowledge graph correlate with researchers' interest levels. After normalizing these features using z-scores, we arranged them from lowest to highest and divided the data into 50 equal groups. For each group, we plotted the average feature value (x-axis) against the average interest level (y-axis) with standard deviations, to identify trends. (a) and (b) show node features, (c)-(e) show node citation metrics, (f ) shows an edge feature, (g) an edge citation metric, and (h) represents semantic distance between researchers' sub-networks (higher values indicate that the researchers' scientific fields are further apart). Data points include all 2,996 responses (blue), the top 50% of concept pairs by predicted impact (green), and the top 25% (red), using the neuralnetwork based impact prediction presented in [12].in Fig. 2(c), 1,107 projects (nearly 25%) received a rating of 4 or 5, with 394 rated as very interesting.Properties of interesting research suggestions -On average, we find no significant difference in interest levels between projects generated using random concept pairs, high-impact concept pairs, or without concept pairs. The similarity in results for the sanity test (projects generated without a concept pair used in the knowledge graph) and those using concept pairs enables us to analyze which knowledge graph features most strongly influence the perceived interest of a research project. Identifying these features can help us suggest future research projects with higher interest levels.We use the 2,996 suggested research projects, which were created using concept pairs from the knowledge graph, and sort them by various knowledge graph features. Then we group them into 50 equal bins and calculated the mean interest and standard deviation for each bin. Fig. 3 shows these correlations and highlights several notable trends. For instance, the vertex degree and PageRank of the first concept, selected from the evaluating researcher's concept list, are strongly negatively correlated with human-assessed interest levels. This indicates that the more connected a concept is within the knowledge graph, the less appealing the research project appears. A similar trend is observed for citation rates: the more frequently a concept has been cited in the past, the less interesting the project is evaluated. Additionally, the semantic distance feature shows a negative correlation in Fig. 3(h), suggesting that research proposals involving researchers from similar fields are perceived as more interesting than those from more distant fields. This finding aligns with Fig. 2(c), where proposals from the same institute are generally rated higher than those from different institutes with distinct research focuses. We present these correlations for all 2,996 responses (blue), as well as for the top 50% and top 25% of concept pairs with the highest predicted impact (green and red, respectively) in Fig. 3, indicating that some correlations are more pronounced for suggestions using highimpact concept pairs. Predicting interest -We set out to predict which suggestions would receive high interest ratings (4 or 5 out of 5) using two fundamentally different methods. First, we trained a neural network on researchers' responses, using knowledge graph properties and human-evaluation rankings, without incorporating the GPT-generated text. Second, we employed GPT in a zero-shot manner to rank the 2,996 suggestions independently of human evaluations. Remarkably, both method showed high prediction accuracy despite the absence of crucial information (see Fig. 4). This suggests that intelligent concept pair selec-Fig. 4. Predicting Scientific Interest. We use two distinct methods to predict interest levels: (1) a supervised neural network trained on human evaluations using only knowledge graph data (not the text of the actual suggestion), and (2) GPT in a zero-shot setting, ranking suggestions without getting any feedback from human evaluations. Both methods classify suggestions as highly interesting (ratings of 4 or 5) or not (below 4). The neural network uses 25 knowledge graph features and employs Monte Carlo cross-validation for accuracy estimation. For GPT, we conduct pairwise comparisons using personalized research details and rank suggestions through an ELO-based tournment system. (a), The ROC curve shows prediction accuracy of 64.5% for the neural network and 67.3% for GPT-4o. (b), The precision for top-N suggestions is significantly higher than random selection, with the top-1 precision reaching 70% for the neural network (51.0% for GPT-4o and 52.9% for GPT-3.5) and top-5 precision at 60.4% (46.7% for GPT-4o, 43.7% for GPT-3.5). (c), The probability of having at least one high-interest suggestion among the top N recommendations is significantly higher for the supervised neural network compared to random selection. Practically, evaluation data from experienced researchers may not always be available, thus it is very encouraging that LLMs, even without human evaluation, can rank suggestions effectively such that the highest interesting ones appear first.tion alone can significantly influence interest rankings in the graph-based approach, while GPT's zero-shot ranking is valuable when human evaluations are unavailable.For the supervised neural network, we used knowledge graph features to predict whether a research proposal would receive a high rating (4 or 5) or below 4. Given the limited training data -each of the 2,996 data point representing a research group leader's evaluation of a proposal's interest level -we employed a low-data machine learning approach with a small neural network (25 highperforming input features, 50 neurons in a single hidden layer, and one output neuron). Dropout was used for training [31], and Monte Carlo cross-validation [32] (which is also known as repeated random sub-sampling validation) was applied to ensure robust evaluation and maximize the utility of our limited data (see the Supplementary Information). Decision trees [33] were not able to outperform the quality of neural networks (see Supplementary Information).In the second approach, we tasked GPT-3.5 and GPT-4o to rank all 2,996 suggestions from most to least interesting. Because the suggestions are personalized, we included relevant research details, such as recent paper titles, in the prompts. Specifically, GPT is asked to compare pairs of randomly selected suggestions and determine which is more interesting based on the personalized research interests of the evaluating researcher. This comparison was repeated between 22,000 and 45,000 times (depending on the GPT version), and suggestions were ranked using an ELO system, with each suggestion starting at an initial ELO score of 1400. The model's choices adjusted the rankings, resulting in a final sorted list of suggestions based on their predicted interest level.For the binary classification task -ranking research ideas in as highly interesting (4 or 5 out of 5) or lowinterest (3 or below) -, both methods achieve an average Area Under the Curve (AUC) of the receiver operating characteristic (ROC) curve [34] of nearly 2/3 (Fig. 4(a)). More importantly, high precision is more relevant for our task, as we want to suggest highly interesting projects within a small subset of overall suggestions. To evaluate this, we calculate the precision for the top-N predicted concept pairs. For small N (e.g., N=3), the supervised approach achieves 66.4% while GPT-4o and GPT-3.5 reached 45.0% and 47.2%, respectively. This means that 66.4% of the top-3 suggestions were rated as highly interesting, significantly higher than random selection (23%), as shown in Fig. 4(b). Additionally, we also measured the probability of finding at least one highly interesting suggestion within the top-N suggestions. As shown in Fig. 4(c), our machine learning method offers a significantly higher probability of identifying interesting suggestions within the first few recommendations compared to random sampling. Surprisingly, GPT -without access to human evaluations -also ranks interesting suggestions much higher than a random approach. This capability is highly valuable in scenarios where human evaluations are costly or unavailable. Furthermore, it is encouraging that newer, more powerful models (e.g., GPT-4o) perform better at predicting human interests than earlier versions like GPT-3.5.
DISCUSSIONWe demonstrate the largest human evaluation of AIgenerated research ideas to date, involving over 100 research group leaders across diverse scientific domains, including the natural sciences and humanities.Beyond understanding the correlations between the properties of the generated research ideas and their interest rankings, we introduce two distinct methods to predict interest levels of AI-generated research ideas. First, we use thousands of human evaluations to train a supervised neural network that can predict interest values based on knowledge graph data alone -without needing the full suggestion text. This approach enables us to select more compelling abstract topics before generating specific ideas. Second, we demonstrate that LLMs can autonomously rank the interest levels of ideas with high quality, even without human evaluation data. This capability is especially valuable when human evaluations are unavailable. Furthermore, we observe that ranking quality improves with more advanced LLMs, which is promising for future developments. As publicly available models such as GPT [17], Gemini 1.5 [26], LLaMa3 [35], and Claude [27] continue to evolve at an accelerated pace [36] -especially when it comes to scientific domain knowledge [37,38] -we expect personalized research ideas to become increasingly targeted and relevant.The methodologies employed by SciMuse have the potential to inspire novel and unexpected cross-disciplinary research on a large scale. By providing a broad view through the analysis of millions of scientific papers, SciMuse facilitates the discovery of interesting research collaborations between scientists from different fields that might otherwise remain undiscovered. Research in distant fields has been shown to have great potential for impactful, award-winning results [1,2,6,39]. Therefore, large scientific organizations, national funding agencies, and other stakeholders may find value in adopting methodologies in the line of SciMuse to foster new highly interdisciplinary and interesting collaborations and ideas that might otherwise remain untapped. This, hopefully, could advance the progress and impact of science at a large scale.One exciting possibility is in automated scientific experimentation [40]. Currently, while large-language models have been integrated into laboratories [41,42], the main idea of the experiment has been provided by human scientists. In the future, one might envision the entire scientific process becoming fully automated -from the generation of an interesting idea, as we demonstrate here, to its automated execution and implementation.We compiled a list of scientific concepts using metadata from arXiv, bioRxiv, medRxiv, and chemRxiv. The arXiv data is available on Kaggle, while metadata for other preprint sources can be accessed through their respective APIs. Our dataset includes ∼2.44 million prapers, with a cutoff date of February 2023.For edge generation, we used the OpenAlex database snapshot (available on the OpenAlex bucket) with a cutoff date of April 2023. For more details, refer to the OpenAlex website [23]. The original dataset was filtered to entries of journal papers that contain titles, abstracts, and citation data, resulting roughly 92 million papers. From these 92 million papers, 58 million contain at least two concepts of our concept list and can therefore for an edge in the knowledge graph.
B. The concept listWe analyzed the titles and abstracts of ∼2.44 million papers from four preprint datasets using the RAKE algorithm, enhanced with additional stopwords, to extract potential concept candidates. Initial filtering retained two-word concepts (e.g. gouy phase) appearing in at least nine articles and concepts with more than three words (e.g. recurrent neural network ) appearing in six or more, reducing the list to 726,439 concepts.To further improve the quality of the identified concepts, we developed a suite of automated tools to eliminate domain-independent errors commonly associated with RAKE and performed a manual review to remove inaccuracies like non-conceptual phrases, verbs, and conjunctions. This step refined the list to 368,825 concepts.Next, we used GPT-3.5 to further refine the concepts, which resulted in the removal of 286,311 concepts. We then employed Wikipedia to restore 40,614 mistakenly removed entries, resulting in a final refined list of 123,128 concepts.
C. Classification of Max Planck InstitutesWe classified all 87 Max Planck Institutes into two categories: Class 1, abbreviated as nat, includes natural sciences, technology, mathematics, and medicine (68 institutes), while Class 2, abbreviated as soc, includes social sciences and humanities (19 institutes). The initial classification was done manually based on each institute's title and research field. To validate this, we further used GPT-4o for automatic classification, which perfectly matched with our manual classification.
D. Researcher StatisticsOver 100 highly experienced researchers, spanning fields from the natural sciences to the humanities, participated in evaluating the personalized research ideas. Table . S1 summarizes the researchers' publication and citation statistics as of January 1, 2024, when the evaluations were conducted. On average, the researchers had published 59 papers and received over 3,750 citations. The prompt to refine the researchers' concept list is shown below:
Prompt to Refine the Researchers' Concept ListA scientist has written the following papers: 0) title1 1) title2 2) title3 ... I have a noisy list of the researchers' topics of interest, and I would like your help in filtering them. Please look at the list below and return all concepts that are relevant to the scientist's research (based on their paper titles) and meaningful in the context of their research direction. The concepts can be detailed; I mainly want you to filter out concepts that are not meaningful, words that are not concepts, or concepts that are too general for the direction of the scientist (e.g., "artificial intelligence" might be a meaningful concept for a geologist, but not for a machine learning researcher). Do not change or add any conceptsonly remove or keep them. concept list=[c1, c2, c3, c4, c5, c6, ...]
F. Prompt to GPT-4 for project idea generationThe prompt used to suggest research ideas based on concept pairs from the knowledge graph is described as the follows:Prompt to GPT-4 for Project Idea Generation Two researchers A and B, with expertise in "con-cept1" and "concept2" respectively, are eager to collaborate on a novel interdisciplinary project that leverages their unique strengths and creates synergy between their fields.To better understand their backgrounds, here are the titles of recent publications from each researcher: Researcher A: 1: title1 2: title2 3: title3 ... Researcher B: 1: title1 2: title2 3: title3 ... Please suggest a creative and surprising scientific project that combines "concept1" and "con-cept2". In your response, follow this outline: First, explain "concept1" and "concept2" in one short sentence each.Then, do the following three steps 3 times, improving in each time the response: A) Describe 4 interesting and new scientific contexts, in which those two concepts might appear together in a natural and useful way. B) Criticize the 4 contexts (one short sentence each), on how well the contexts merge the idea of the two concepts. C) Give a 2 sentence summary of your reflections above, on how well one can combine these concepts naturally and interestingly.Then, start finding a project. Taking your reflections from (A-C) into account, define in your response a project title, followed by a brief explanation of the project's main objective.Finally, address the following questions (Take the full reflections (A-C) into account): What specific interesting research questions will this project address, that will lead to innovative novel results? [2 bullet points, one sentence each] Rather than relying on a knowledge graph to supply "concept1" and "concept2", it is also possible to direct GPT-4 to extract these concepts from the research paper titles of Researchers A and B, respectively. GPT-4 can then use these identified concepts within the same prompting context to generate innovative research ideas.
G. Interest evaluation for three different generation methodsFig. S1 presents the interest-level distributions for research suggestions generated using three different methods. The interest levels are notably similar between suggestions generated with and without concepts from the knowledge graph. This similarity enables us to analyze the correlations between knowledge graph properties and interest levels, and to use these properties for predicting the interest level of generated research proposals. (1) no concepts provided by the knowledge graph, (2) random concepts from the researchers' subnetwork, and (3) predicted high-impact concept pairs from the researchers' subnetwork. The figures displays: (a) overall interest levels (numbers within bars show the number of responses for that evaluation), (b) interest levels for ideas without using concepts from the knowledge graph, (c) interest levels with random concept pairs, and (d) interest levels using high-impact concept pairs (predicted by adapting the computational methods from [12], and applying them to a different and much larger knowledge graph).
H. Predicting high interest from knowledge graph features with neural networksIn Fig. 4 (main text), we aim to predict whether a research proposal will be rated with high interest. Specifically, using only data from the knowledge graph (excluding the final text generated by GPT), we predict if a proposal will receive an interest rating of 4 or 5 (on a scale of 1 to 5: not interesting to very interesting) or below 4. This is formulated as a binary classification task.The input to the neural networks (using PyTorch [44]) consists of network-theoretical features extracted from the knowledge graph. For each concept pair in a re- search project, we compute 144 features. The first 141 features are derived from those used to predict the future impact of concept pairs, as described in [12]. These features include node properties (e.g., node degree and PageRank [45]) and edge properties (e.g., Simpson similarity and Sørensen-Dice coefficient [46]). Several features also account for impact information, such as recent citation counts. The remaining three features include the predicted impact and two distance metrics between the researchers' subgraphs (Fig. 1(b)). The first distance metric measures the distance based solely on the concepts present in Researcher A and Researcher B's concept lists. In contrast, the second metric takes into account the entire neighborhood of these subgraphs by calculating semantic distances between all neighboring concepts and the concepts from the subgraphs. These features serve as the input to the neural network for predicting whether a proposal will achieve a high interest rating.Given the small dataset size (2,996 answers with properties from the knowledge graph), we use a data-efficient learning method -a small neural network with dropout. The input layer consists of the 25 best-performing features (see Table. S2), selected from the total 144 by independently analyzing the feature importance of each and choosing the top 25. The neural network has one hidden layer with 50 neurons and a single output neuron. Mean square error is used as the loss function.To ensure robust performance estimation for the small dataset, we use Monte Carlo cross-validation. The dataset is repeatedly split into training and validation sets, and the model is trained and evaluated on each split. This approach ensures that the performance metrics are robust and not dependent on a particular split of the data. This iterative process continues until the standard deviation of the mean AUC is less than 10 −2 3 , achieved after 130 iterations. This method provides a reliable estimate of the model's performance, which is crucial for small datasets where individual splits may lead to high variance in the evaluation metrics.The neural network performance is not specifically sensitive to hyperparameter choices, thus we refrained from hyperparameter optimization, and instead used a reasonable defaults: learning rate=0.003, dropout=20%, weight decay=0.0007, training dataset=75%, validation dataset=15%, test dataset=10%. In Fig. S2, we investigate alternative hyper-parameters of the training process, and find that the results are robust under variations of the hyper-parameters.
I. Predicting high interest from knowledge graph features with decision treesWe experimented with other data-efficient learning methods, specifically with decision trees [33] using [47]. However, decision trees did not outperform the neural network predictions, as can be seen in Fig. S3. These values confirm that neural networks are advantageous for the task of ranking research ideas by their interest value in a supervised way, which can also be confirmed in Fig. S5.
J. Zero-shot ranking of research suggestions by GPTWe ranked 2,996 research suggestions -previously evaluated by human experts -using GPT-3.5, GPT-4o, and GPT-4o-mini. For each pair of randomly selected suggestions, we asked the LLMs to rank which one was more interesting, considering the personalized research interests of the evaluating human expert. This pairwise comparison was repeated between 22,000 and 45,000 times (for GPT4o and GPT4o-mini, respectively). We treated this task as a tournament where all 2,996 suggestions compete pairwise against each other. Using the ELO ranking system, each suggestion started with an initial ELO score of 1400. Each comparison by GPT updated the ELO rankings based on the outcome, producing a final sorted list of suggestions from highest to lowest ELO score. We evaluated the ranking quality by calculating the AUC to determine how well the ranked list aligns with the human-expert evaluations of interest levels (Fig. S4).
K. Prediction of Interest with different methodsWe show the full data of all five methods (supervised training with neural networks and decision trees, as well as unsupervised zero-shot prediction with GPT3.5, GPT4o and GPT4o-mini), with their corresponding AUC, top-N precision and top-N success probability, in Fig. S5. We see that the neural network outperforms decision trees when trained in a supervised way, and that GPT4o is better than the other tested models, when the ranking is performed in a zero-shot manner without giv-Fig. S4. Zero-Shot ranking of research suggestions by LLMs. The research suggestions are generated using the knowledge graph together with GPT4. They are then ranked using GPT4o, GPT4o-mini and GPT3.5, without feedback from the human evaluation. The human evaluation is used to compute the final quality of the ranking. the ranking is performed in a pair-wise choice where we ask the LLM to select the more interesting one given the research background of the researchers. One match is one pairwise selection. The LLMs perform 10,000 of these pairwise selections, which allows us to compute ELO scores for each generated research idea.ing any information about the evaluations of humans.
L. Prompt engineeringWe have explored manual and automated improvements of the prompts, both for the research question design and the zero-shot prediction. Specifically, we attempted to improve the prompts for the idea generation using GPT-4o. While the prompts were more structured, a small-scale evaluation did not show any improvement in terms of more interesting results.
M. GPT-4o and GPT-o1 for idea generationWe conducted two small-scale tests where GPT-4 and GPT-4o generated ideas using the exact same settings described above.In the first test, three research group leaders evaluated 180 pairs of questions (one generated by GPT-4 and the other by GPT-4o using the same prompt). They found 31.1% of GPT-4 answers to be more interesting, while 60.56% favored GPT-4o answers (8.3% were draw). In the second test, a research group leader evaluated 11 pairs of questions (one generated by GPT-4o and the other by GPT-o1 with the same prompt), and all 11 ideas generated by GPT-o1 were ranked as more interesting.These additional small-scale tests suggest that improved models can enhance idea generation, thus directly improving the results of SciMuse.Prompt for Zero-Shot Ranking of Research Ideas I will present two research ideas. The first idea is for Researchers A1 and B1, and the second idea is for Researchers A2 and B2. Researchers A1 and A2 will evaluate how interesting they find the respective ideas. You will determine which of the two suggestions will be considered more interesting.  Fig. 2 .2Fig. 2. Large-scale human evaluation within the Max Planck Society. (a)-(b), The map of Germany, based on the GISCO statistical unit dataset from Eurostat [28], shows the locations of the Max Planck Institutes and the participating group leaders. A total of 4,451 personalized AI-generated research suggestions were evaluated by 110 research group leaders. Each suggestion represents a potential collaboration between the evaluating researcher (Researcher A) and another researcher (Researcher B) from the Max Planck Society, visualized as bi-colored edges (orange for Researcher A, green for Researcher B). A purple circle indicates collaborations within the same institute, and edge transparency reflects the number of evaluated suggestions. Blue dots denote natural sciences (nat) and red dots represent social sciences (soc). (c): Distribution of interest ratings on a scale from 1 ('not interesting') to 5 ('very interesting'), with 394 suggestions rated as very interesting and 713 rated 4. Ratings are further categorized by whether the collaborations are within or across institutes, and by research field affiliation in either the natural or social sciences.
Fig. S1 .S1Fig. S1. Interest levels across different generation methods. Research ideas are generated using three methods:(1) no concepts provided by the knowledge graph, (2) random concepts from the researchers' subnetwork, and (3) predicted high-impact concept pairs from the researchers' subnetwork. The figures displays: (a) overall interest levels (numbers within bars show the number of responses for that evaluation), (b) interest levels for ideas without using concepts from the knowledge graph, (c) interest levels with random concept pairs, and (d) interest levels using high-impact concept pairs (predicted by adapting the computational methods from[12], and applying them to a different and much larger knowledge graph).
Fig. S2 .S2Fig. S2. Choice of alternative hyper-parameters for training of neural network. We analyse the prediction of interest level quality (in terms of AUC) with different parameters of the neural network, such as different number of features, different number of layers and neurons, learning rate and drop-out rate. We see that the final results are robust under variations of the hyper-parameters.
Fig. S3 .S3Fig.S3. Choice of hyper-parameters for training of decision tree. The model is trained using Monte Carlo crossvalidation until a statistical uncertainty of σ=0.001 is reached. We find that no setting of number of features, maximum depth and minimal sample leaf can reach the performance of the data-efficient neural network.
The suggestions are randomly ordered, and you should evaluate each suggestion independently and without bias. ### Researcher A1 Context and Suggestion 1: Here are a few papers of Researcher A1: (papersA1) Suggestion 1: [suggestion1] **Summary for Researcher A1**: Provide a one-sentence summary of Suggestion 1 in the context of Researcher A1. ### Researcher A2 Context and Suggestion 2: Here are a few papers of Researcher A2: (papersA2) Suggestion 2: [suggestion2] **Summary for Researcher A2**: Provide a one-sentence summary of Suggestion 2 in the context of Researcher A2.### Evaluation:Based on the summaries and the research interests of A1 and A2, evaluate which suggestion is more likely to be ranked higher in terms of interest. **Result**: If Suggestion 1 is ranked higher by Researcher A1 than Suggestion 2 is by Researcher A2, write 'RESULT: SUGGESTION 1'. Otherwise, write 'RESULT: SUGGESTION 2'. Remember, the suggestions are randomly ordered, and your evaluation should be impartial and based solely on the research interests of A1 and A2.
Fig. S5 .S5Fig.S5. Interest predictions with five methods. We reproduce Fig.4(main text), and add results from a supervised decision tree training, and an unsupervised GPT4o-mini.



Table S1 .S1Summary statistics of researchers' publications and citations.Mean Median Min MaxNumber of papers59.736.09402Number of citations 3759.7 1630.020 85778E. Prompt to GPT-4 for concept refinement



ACKNOWLEDGEMENTSThe authors wholeheartedly thank all the researchers who spent the time participating in our study. The authors also thank the organizers of OpenAlex, arXiv, bioRxiv, medRxiv, and chemRxiv for making scientific resources freely accessible. X.G. acknowledges support from the Alexander von Humboldt Foundation.


DATA AND CODE AVAILABILITYData for the knowledge graph is accessible on Zenodo at https://doi.org/10.5281/zenodo.13900962 [43]. Codes and evaluation data for this work are available on GitHub at https://github.com/artificial-scientist-lab/SciMuse.


ETHICS STATEMENTThe research was reviewed and approved by the Ethics Council of the Max Planck Society.
Supplementary InformationA. Datasets for creating knowledge graph 






SFortunato


CTBergstrom


KBörner


JAEvans


DHelbing


SMilojević


AMPetersen


FRadicchi


RSinatra


BUzzi

10.1126/science.aao0185


Science of science

359
185
2018


Science




DWang


A.-LBarabási

The science of science

Cambridge University Press
2021





Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases

LBornmann


RHaunschild


RMutz

10.1057/s41599-021-00903-w


Humanities and Social Sciences Communications

8
1
2021







JAEvans


JGFoster


Metaknowledge

10.1126/science.1201765


Science

331
721
2011





Sciscinet: A largescale open data lake for the science of science research

ZLin


YYin


LLiu


DWang

10.1038/s41597-023-02198-9


Scientific Data

10
315
2023





Choosing experiments to accelerate collective discovery

ARzhetsky


JGFoster


ITFoster


JAEvans

10.1073/pnas.150975711


Proc. Natl. Acad. Sci. USA

112
14569
2015





Predicting research trends with semantic and neural networks with an application in quantum physics

MKrenn


AZeilinger

10.1073/pnas.1914370116


Proc. Natl. Acad. Sci. USA

117
1910
2020





Agatha: automatic graph mining and transformer based hypothesis generation approach

JSybrandt


ITyagin


MShtutman


ISafro

10.1145/3340531.3412684


Proceedings of the 29th ACM International Conference on Information & Knowledge Management
the 29th ACM International Conference on Information & Knowledge Management

2020







RNadkarni


DWadden


IBeltagy


NASmith


HHajishirzi


THope

arXiv:2106.09700
Scientific language models for biomedical knowledge base completion: an empirical study

2021





Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network

MKrenn


LBuffoni


BCoutinho


SEppel


JGFoster


AGritsevskiy


HLee


YLu


JPMoutinho


NSanjabi

10.1038/s42256-023-00735-0


Nature Machine Intelligence

5
1326
2023





Surprising combinations of research contents and contexts are related to impact and emerge with scientific outsiders from distant disciplines

FShi


JEvans

10.1038/s41467-023-36741-4


Nature Communications

14
1641
2023






XGu


MKrenn

arXiv:2402.08640
Forecasting high-impact research topics via machine learning on evolving knowledge graphs

2024





Accelerating science with human-aware artificial intelligence

JSourati


JAEvans

10.1038/s41562-023-01648-z


Nature Human Behaviour

7
1682
2023






QWang


DDowney


HJi


THope

Scimon: Scientific inspiration machines optimized for novelty, Annual Meeting of the Association for Computational Linguistics

2024





Large language models for automated open-domain scientific hypotheses discovery

ZYang


XDu


JLi


JZheng


SPoria


ECambria

arXiv:2309.02726

2023






JBaek


SKJauhar


SCucerzan


SJHwang

arXiv:2404.07738
Researchagent: Iterative research idea generation over scientific literature with large language models

2024







JAchiam


SAdler


SAgarwal


LAhmad


IAkkaya


FLAleman


DAlmeida


JAltenschmidt


SAltman


SAnadkat

arXiv:2303.08774


Gpt-4 technical report

2023





On scientific understanding with artificial intelligence

MKrenn


RPollice


SYGuo


MAldeghi


ACervera-Lierta


PFriederich


GDos Passos


FGomes


AHäse


AJinich


Nigam

10.1038/s42254-022-00518-3


Nature Reviews Physics

4
761
2022





A computational inflection for scientific discovery

THope


DDowney


DSWeld


OEtzioni


EHorvitz

10.1145/3576896


Communications of the ACM

66
62
2023





Scientific discovery in the age of artificial intelligence

HWang


TFu


YDu


WGao


KHuang


ZLiu


PChandak


SLiu


PVan Katwyk


ADeac

10.1038/s41586-023-06221-2


Nature

620
47
2023





The impact of large language models on scientific discovery: a preliminary study using gpt-4

MRAi4science


MAQuantum

arXiv:2311.07361

2023





Automatic keyword extraction from individual documents, Text mining: applications and theory

SRose


DEngel


NCramer


WCowley

10.1002/9780470689646.ch1

2010
1






JPriem


HPiwowar


ROrr

arXiv:2205.01833
Openalex: A fullyopen index of scholarly works, authors, venues, institutions, and concepts

2022







KWakonig


ADiaz


ABonnin


MStampanoni


ABergamaschi


JIhli


MGuizar-Sicairos


AMenzel


X-Ray FourierPtychography

10.1126/sciadv.aav0282


Science advances

5
282
2019





Ultrafast x-ray imaging of the light-induced phase transition in vo2

ASJohnson


DPerez-Salinas


KMSiddiqui


SKim


SChoi


KVolckaert


PEMajchrzak


SUlstrup


NAgarwal


KHallman

10.1038/s41567-022-01848-w


Nature Physics

19
215
2023






MReid


NSavinov


DTeplyashin


DLepikhin


TLillicrap


J-B. Alayrac


RSoricut


ALazaridou


OFirat


JSchrittwieser

arXiv:2403.05530
Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context

2024





The claude 3 model family: Opus, sonnet, haiku

Anthropic



Papers with Code

2024


available at https://paperswithcode.com/paper/ the-claude-3-model-family-opus-sonnet-haiku




ECommission

Eurostat gisco -nuts geodata

2024





A spot in one of the belts of jupiter

RHooke

10.1098/rstl.1665.0005


Philosophical Transactions of the Royal Society of London

1
3
1665





Self-refine: Iterative refinement with self-feedback

AMadaan


NTandon


PGupta


SHallinan


LGao


SWiegreffe


UAlon


NDziri


SPrabhumoye


YYang



Advances in Neural Information Processing Systems

36
2024





Dropout: A simple way to prevent neural networks from overfitting

NSrivastava


GHinton


AKrizhevsky


ISutskever


RSalakhutdinov



Journal of Machine Learning Research

15
1929
2014





Monte carlo cross validation

Q.-SXu


Y.-ZLiang

10.1016/S0169-7439(00)00122-2


Chemometrics and Intelligent Laboratory Systems

56
1
2001






LBreiman

Classification and regression trees

2017


Routledge



Roc graphs: Notes and practical considerations for researchers

TFawcett



Machine learning

31
1
2004





Llama 3: Open foundation and fine-tuned chat models

MAi



2024






W.-LChiang


LZheng


YSheng


ANAngelopoulos


TLi


DLi


HZhang


BZhu


MJordan


JEGonzalez

arXiv:2403.04132
Chatbot arena: An open platform for evaluating llms by human preference

2024






GPWellawatte


PSchwaller

arXiv:2311.04047
Extracting human interpretable structure-property relationships in chemistry using xai and large language models

2023





Can large language models provide useful feedback on research papers? a large-scale empirical analysis

WLiang


YZhang


HCao


BWang


DYDing


XYang


KVodrahalli


SHe


DSSmith


YYin

10.1056/AIoa2400196


NEJM AI

1
AIoa2400196. 2024





Atypical combinations and scientific impact

BUzzi


SMukherjee


MStringer


BJones

10.1126/science.1240474


Science

342
468
2013





Self-driving laboratories for chemistry and materials science

GTom


SPSchmid


SGBaird


YCao


KDarvish


HHao


SLo


SPablo-García


EMRajaonson


MSkreta

10.1021/acs.chemrev.4c00055


Chemical Reviews

124
9633
2024





Autonomous chemical research with large language models

DABoiko


RMacknight


BKline


GGomes

10.1038/s41586-023-06792-0


Nature

624
570
2023





Augmenting large language models with chemistry tools

AMBran


SCox


OSchilter


CBaldassari


ADWhite


PSchwaller

10.1038/s42256-024-00832-8


Nature Machine Intelligence

1
2024





Interesting scientific idea generation using knowledge graphs and llms: Evaluations with 100 research group leaders [data set

XGu

10.5281/zenodo.13900962


2024





Pytorch: An imperative style, high-performance deep learning library

APaszke


SGross


FMassa


ALerer


JBradbury


GChanan


TKilleen


ZLin


NGimelshein


LAntiga



Advances in neural information processing systems

32
2019





The pagerank citation ranking : Bringing order to the web

LPage


SBrin


RMotwani


TWinograd


1999
Stanford InfoLab






A.-LBarabási

Network Science

Cambridge University Press
2016





Scikit-learn: Machine learning in python

FPedregosa


GVaroquaux


AGramfort


VMichel


BThirion


OGrisel


MBlondel


PPrettenhofer


RWeiss


VDubourg


JVanderplas


APassos


DCournapeau


MBrucher


MPerrot


EDuchesnay



Journal of Machine Learning Research

12
2825
2011







