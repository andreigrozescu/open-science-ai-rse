



Forecasting high-impact research topics via machine learning on evolving knowledge graphs




7 Jan 2025





XuemeiGu
xuemei.gu@mpl.mpg.de

Max Planck Institute for the Science of Light

Staudtstrasse 2
91058
Erlangen
Germany




MarioKrenn
mario.krenn@mpl.mpg.de

Max Planck Institute for the Science of Light

Staudtstrasse 2
91058
Erlangen
Germany



Forecasting high-impact research topics via machine learning on evolving knowledge graphs



7 Jan 2025


ED608FBF2983FD865ACA1A6182B1E593
arXiv:2402.08640v3[cs.DL]






GROBID - A machine learning software for extracting information from scholarly documents






The exponential growth in scientific publications poses a severe challenge for human researchers. It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one's own field. While there are ways to predict a scientific paper's future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived. Here we show how to predict the impact of onsets of ideas that have never been published by researchers. For that, we developed a large evolving knowledge graph built from more than 21 million scientific papers. It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers. Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy (AUC values beyond 0.9 for most experiments), and thereby the impact of new research directions. We envision that the ability to predict the impact of new ideas will be a crucial component of future artificial muses that can inspire new impactful and interesting scientific ideas.





INTRODUCTIONAs we see an explosion in the number of scientific articles [1][2][3][4], it becomes increasingly challenging for researchers to find new impactful research directions beyond their own expertise. Consequently, researchers might have to focus on narrow subdisciplines. A tool that can read and intelligently act upon scientific literature could be an enormous aid to individual scientists in choosing their next new and high-impact research project, which -on a global scale -could significantly accelerate science itself.These days, a natural first choice for an AI-assistant would be powerful large-language-models (LLMs) such as GPT-4 [5], Gemini [6], LLaMA-2 [7] or custom-made models [8]. However, these models often struggle in scientific reasoning, and it remains unclear how they can suggest new scientific ideas or evaluate their impact in a reliable way in the near term.An alternative and complementary approach is to build scientific semantic knowledge graphs. Here, the nodes represent scientific concepts and the edges are formed when two concepts are researched together in a scientific paper [2]. While this approach extracts only small amounts of information from each paper, surprisingly non-trivial conclusions can be drawn if the underlying dataset of papers is large. An early example of this is a work in biochemistry [9]. The authors use their semantic network, where nodes represent biomolecules, to find new potentially more efficient exploration strategies for the bio-chemistry community on a global scale. In these semantic networks, an edge between two concepts indicates that researchers have jointly investigated these research concepts. The edges are drawn from papers, thus they are created at a specific time when the paper was published. In this way, one creates an evolving semantic network that captures what researchers have investigated in the past. With such an evolving network, one can ask how the network might evolve in the future. In the scientific context, this question can be reformulated into what scientists will research in the future. For example, if two nodes do not share an edge, one can ask whether they will share an edge in the next three years -or, alternatively, whether scientists will investigate these two concepts jointly within three years. This question, denoted as a link-prediction problem in network theory [10], has been successfully demonstrated with high prediction quality for semantic networks in the field of quantum physics [11] and artificial intelligence [4]. These works focus on the question what scientists will work on, completely leaving out which of these topics will be impactful.Impact in the scientific community is often approximated (for lack of better metrics [12,13]) by citations [1,2,14,15], including exciting results that find interpretable mathematical models to describe citation evolution [16][17][18][19]. Beside concrete mathematical modelling, impact of scientific papers has also been predicted using advanced statistical and machine-learning methods that use meta-data such as including authors and affiliations [20], the content and the references of the paper [21,22]. Techniques employed for the predictions of individual paper impact using a combination of characteristics include support-vector machines [23], regression [24][25][26], dense [27] or graph neural networks [28].The prediction of a paper's impact however is only possible after the research is completed, and long after its underlying idea is created. A true scientific assistant or muse however should contribute at the earliest stage of the scientific cycle, when the idea for the next impactful research project is born. One solution is the prediction at the concept level. Specifically, we can ask the question Which scientific concepts, that have never been investigated jointly, will lead to the most impactful research?.In this work, we answer this question by combining semantic networks and citation networks that are purely The edges are augmented with citation information, which acts as a proxy for impact in our work. A mini-knowledge graph (blue edges) is constructed from four randomly selected papers (p1-p4) [29][30][31][32] from OpenAlex as an example. Here, cp4 represents the total citations of paper p4 since its publication, and cp4(y) is its annual citations from 2018 to 2022 (e.g., cp4(2018) = 4). The citation value of the edge is the sum of the all papers creating the edge.based on the level of scientific concepts 1 . Specifically, we develop a large evolving knowledge graph using more than 21 million scientific papers, from 1709 (starting with a letter by Antoni van Leeuwenhoek [38]) to April 2023. The vertices of the knowledge graph are scientific concepts and the edges between two concepts contain information about when these topics have been investigated and how often they have been cited subsequently. We then train a machine learning model on the historic evolution of the knowledge graph. We find that the neural network can predict with high accuracy which concept pairs, that have never been jointly investigated before in any scientific paper, will be highly cited in the future.Being able to predict the potential impact of new research ideas -before the paper is written or the research is done or even started -could be a cornerstone in future scientific AI-assistants that help humans broadening their horizon of possible new research endeavours [39].1 GitHub: Impact4Cast
RESULTSCreating a list of scientific concepts -At the heart of our knowledge graph are scientific concepts, as depicted in Fig. 1. We chose not to rely on existing concept lists, such as the APS or computer science ontology [40], for several reasons. Firstly, our goal is to ultimately cover all natural sciences comprehensively, and a universal list encompassing this breadth doesn't currently exist. Secondly, we want to capture the most recent concepts that might be absent from existing lists. Lastly, generating our list ensures that we have a granular understanding and control over the concepts.To build our concept list, we started with 2,444,442 papers from four publicly available preprint servers: arXiv, bioRxiv, medRxiv, and chemRxiv. We use papers from preprint servers for two reasons: (1) It contains papers that are not published yet in journals, thus our dataset also contains state-of-the-art concepts; (2) they associate . We find many revolutionary topics in the realm of quantum physics and optics research in the last decade, including Perovskite devices [33], the emergence of complex and nonhermitian topology [34], the introduction of advanced concepts of machine learning in physics [35][36][37] and quasi-BIC (bound state in continuum) resonances.papers to research categories, which can be used to focus on specific scientific domains (as we do, with the field of quantum physics and optics). The data cutoff is February 2023. From these, we extracted titles and abstracts of the papers. To single out concept candidates from this extensive collection, we applied the Rapid Automatic Keyword Extraction (RAKE) algorithm based on statistical text analysis to automatically detect important keywords [41]. Concepts with two words, like phase transition, were retained if they appear in at least 9 papers, while longer concepts, such as single molecule localization microscopy, needed to appear in at least 6 papers. In this way, we can increase the fraction of high-quality concepts. We further developed a suite of natural language processing tools to refine the concepts, followed by manual inspection to remove any incorrectly identified ones. Finally, we got a list which contains over 368,000 concepts. We focus here on concepts specific to the sub-field of optics and quantum physics (representing roughly 10% of the entire concepts), but our method can immediately be translated to any other domain. This refined domain-specific concept list serves as the vertices of our knowledge graph.Creating an evolving, citation-augmented knowledge graph -Now that we have the vertices, we can create edges that contain information from the scientific literature. We get the citation information from papers in OpenAlex [42], an open-source database containing detailed information on more than 92 million publications. Edges are drawn when two concepts co-occur in the title or abstract of a scientific paper. If a paper connects two vertices, the weight of the newly formed edge is the paper's annual citation numbers from 2012 to 2023 together with the total citation number since its publication. If more than one paper creates an edge, then the edge contains the sum of the annual citations (as well as the sum of the total citations) gained by all papers. As research papers appear over time, and their citations are created in time, we effectively build an evolving, citation-augmented knowledge graph that evolves in time (see Fig. 1). From these 92 million papers, 21 million contain at least two concepts of our concept list and can therefore for an edge in the knowledge graph.The final constructed knowledge graph has 37,960 vertices with more than 26 million edges (built from 190 million concept pairs, containing multi-edges when multiple papers create the same edge) from the OpenAlex dataset, with a data cutoff at April 2023. In Fig. 2, we show the fastest growing (in terms of citation) concepts and concept pairs since 2012, where we can recognize many highly influential topics in quantum physics and optics research. the past, we can formulate the prediction of impact for new concept pairs as a supervised learning task, as illustrated in Fig. 3. For a vertex pair that has not had any connection in the year 2016, we predict whether three years later this pair accumulated more than a certain number of citations. Using the historical knowledge graph, we possess an ideal supervision signal for our binary classification task. During the training phase, we selected pairs of vertices that were not connected and calculated 141 features for each pair. These features include 41 network features, divided into 20 node features (such as the number of neighbors and PageRank [43] over the past three years) and 21 edge features (including cosine, geometric, and Simpson similarities [44]). Additionally, we incorporated 100 impact features: 58 of these are node citation features, covering total citations and yearly citations within the last three years. The other 42 features are about vertex pairs and include measures such as the citation ratio between them. Detailed feature description are available in GitHub: Impact4Cast. The network features are inspired by the winner of the Science4Cast competition [11,45], and the citation features are developed empirically and could potentially be improved by careful feature importance analysis. Our neural network is a fully connected feed-forward network with four hidden layers of 600 neurons each. The exploration of more advanced architectures might improve the prediction qualities further. The neural network has to predict whether the unconnected vertex pair in 2019 will have at least IR citations (IR stands for the impact range). The impact range (IR) is a threshold representing the minimum number of citations a concept pair must accumulate within a specified time frame (e.g., three years) to be classified as "high-impact". For instance, an IR = 100 means that only concept pairs with at least 100 citations during the defined period are considered impactful. This binary threshold simplifies the problem into a classification task, making it computationally tractable while providing a clear measure of success. Predicting individual citation counts is inherently noisy due to the stochastic nature of citation dynamics. By using IR, we focus on identifying high-impact trends, avoiding fluctuations of precise citation counts. IR provides a clear and measurable target for classification. This allows us to use metrics like the Area Under the Curve (AUC) of Receiver Operating Characteristics (ROC) curves to evaluate the prediction quality [46].
Forecasting impact of newly created concept connections -With an evolving knowledge graph fromWe perform the training for different values of the impact range IR from IR = 1 to IR = 200, and then quantify the quality with AUC of the ROC curves [46]. The AUC gives a measure of classification quality and stands for the probability that a randomly chosen true example is ranked higher than a randomly chosen false example. A random classifier has AU C = 0.5. We measure the AUC for a test set (which contains unconnected pairs not in the training set) for a prediction from 2016 to 2019, and for an evaluation dataset, with 10 million random data from 2019 to 2022 (while keeping the training data of the neural network from 2016 to 2019). The evaluation dataset shows how well the neural network performs on future, never-seen datasets. This is motivated by our goal that ultimately we want to train a neural network with all available data (let's say, until January 2023) and predict what happens until the future in 2026. In Fig. 4(a), we find that the AUC scores for both the test set and the evaluation set are beyond 0.8, in most of the cases beyond 0.9, for different IR. We can conclude that the neural network can forecast a high impact of previously never-investigated concept connections to a high degree. In Fig. 4(b), we sort the concept pairs of the evaluation dataset with the neural network (IR = 100), and plot their true citation counts. We further divide the 10 million evaluation dataset into 20 equal parts and plot their average citation count (represented by green bars) for each 5% segment. This clearly demonstrates good predictions at the individual concept pair level. As seen in Fig. 4(c), the highest predicted concept pairs indeed get more than 3 orders of magnitude more citations than the average citation of all 10 million pairs.Forecasting genuine impact beyond link prediction -Next, we perform an even more challenging, genuine impact prediction task that goes beyond link prediction (i.e., predicting which concept pairs will be investigated in the future by a scientific paper). Concretely, in this task training data is conditioned on unconnected vertex pairs in 2016 which are actually connected in 2019. We quantify the quality using the AUC of the ROC curve. For example, IR = 100, i.e, (< 100, >= 100), refers to whether the 3-year citation counts after 2016 (test) or after 2019 (eval) is at least 100. TPR (true positive rate) measures how often a test correctly identifies a true positive, while FPR (false positive rate) measures how often it correctly identifies a true negative. (b): Sorted predictions of the neural network on the evaluation set (blue curve in (a)) shows the very high quality prediction at the level of individual concept pairs. The y-axis stands for the respective fraction of the evaluation dataset (10 7 data points). The histogram is separated into 20 equal bins. No fitting is involved. In (c), we show the average citation of the first N highest predicted concept pairs. This plot shows impressively that the highest predicted concept pairs indeed have very high citation, more than 3 orders of magnitude higher than the average citation of all 10 7 pairs (0.029 citations). (d): This more challenging step shows that citation prediction goes beyond link predictions. Here we take unconnected vertex pairs, conditioned on a connection 3 years later. The neural network is tasked to classify these concept pairs in low or high citations, revealing that it is not just predicting new links, but is learning intrinsic citation features. Here IR = [5, 100], i.e, (0 − 5, >= 100), means whether the 3-year citation count after 2016 (test) or after 2019 (eval) is at most 5 or at least 100.The neural network only gets citation information from 2016 and has to predict whether the newly generated concept pair will be highly impactful or not in the future. For that, our classification task asks whether the newly generated edge will receive citations within 0-5 or above 100 (Fig. 4(d)) in 2019. We see that the AUC score is beyond 0.7 (for the test set) and beyond 0.67 for the evaluation set, clearly indicating that the neural network can predict impact properties that go beyond the simple link-prediction task.Highly predicted impact pair and potential applications -We can now investigate the largest predicted pairs of concepts, by taking all unconnected vertex pairs (∼694 million pairs) until January 2023, and let the neural network (trained with all unconnected pairs in 2019 with supervision signal in 2022) sort them by impact predictions. We find that the highest predicted pair is renewable energy and cancer cell. This prediction is a very high-risk bet. For more practical, personalized suggestions, one can restrict the unconnected concept pairs to those related to specific scientists or research groups, aiming for high-impact collaboration suggestions. By examining the published works of scientists to identify their research interests, it becomes possible to identify concept pairs where one aligns with one scientist's specialty and the other with another scientist's. Thereby, one can suggest potential collaborations of high impact. As an example, by constraining the personalized research interests of scientists in experimental quantum optics and one researcher in biophysics, the highest predicted impact concepts pairs are 'microfluidic channel ' with 'Kerr resonator ', 'SARS CoV' with 'quantum enhanced sensitivity' or 'electron microscopy' with 'quantum vacuum field '. These suggestions can be further refined based on their similarity (e.g., represented by the cosine similarity) or the prominence of the concepts (indicated by the node degree), as we show in Fig. 5. Here, we plot 100,000 concept pairs that have not been studied together until January 2023 and use the neural network trained on 2019 dataset to predict their impact. The points are plotted based on various properties, such as the similarity between concepts, their prominence within the network, their growth rate in the network (reflected in newly acquired neighbors), and how often the concepts have been cited previously. Plotting in this way allows us to identify rare outliers -concept pairs with high predicted impact that have unique properties, such as the bright yellow spots highlighted in the insets of Fig. 5. These methods help us narrow down the enormously large number of possibilities into a small number of personalized and targeted suggestions, which could inspire new ideas. In practical application, it will be useful to update the knowledge graph regularly, and train the machine learning models on the latest knowledge graph  data, so it can better incorporate the latest trends and discoveries for its predictions. Training from 2014 -> 2017 FIG. 7. Predictions for varying intervals without retraining. The four ML models were trained using data from 2014 to 2017 and then used to predict outcomes 1 to 5 years into the future without retraining. For one task, the number of positive cases was insufficient for meaningful classification (we set a threshold of at least 10 positive cases out of 1 million total cases). FIG. 8. Prediction of higher IR. The models are tasked with predicting whether concept pairs will receive at least 50 citations. Training is performed on data from 2014 to 2017, with evaluation conducted over intervals of 2, 3, 4, or 5 years (a 1-year interval lacks sufficient data). The blue line represents the performance of a fully connected neural network trained specifically for this task. In contrast, the red line represents a fully connected neural network (with identical architecture and training parameters) trained to solve the task for IR = 10 instead. Interestingly, the red model achieves slightly higher AUC values, indicating that predictions for higher impact ranges (IR = 50) benefit from training on more diverse data, including those from lower impact ranges.Benchmarking different models and different time intervals -So far, we have only focused on a specific case: a training interval from 2016 to 2019 (3 years) and an evaluation interval from 2019 to 2022 (3 years). Additionally, we have primarily investigated the performance of feed-forward neural networks. Exploring other models and examining their predictive capabilities across various training and evaluation intervals could provide deeper insights into model performance. To do so, we expanded our study to include benchmarking on a small dataset of 1 million pairs. This benchmarking incorporates the previously used fully connected neural network alongside additional models, including a transformer architecture [47,48], random forest [49], and XGBoost [50].The feed-forward neural network, implemented using PyTorch [51], consisted of three hidden layers, each with 600 neurons and ReLU activations [52], resulting in ap-proximately 800,000 trainable parameters. Similarly, the transformer architecture [47,48] also implemented via PyTorch, was designed with 4 layers, a hidden size of 128, 4 attention heads, and a feedforward dimension of 512, resulting in approximately 800,000 trainable parameters. Positional encodings were added to the input features. Both neural network models were trained using Adam optimizer [53] with a batch size of 2048 and a learning rate of 0.0001. The random forest classifier, implemented with scikit-learn [54], was trained with 300 trees, a minimum of 25 samples required to split a node, and 10 samples per leaf. The XGBoost model was trained using up to 2000 boosting rounds, a learning rate of 0.01, and a maximum tree depth of 10. Hyperparameters for all models were selected via a hyperparameter search for a single benchmark task (training: 2016-2019, evaluation: 2019-2022, with IR = 10) and kept constant for all tasks.In all tasks, the models are provided with 141 input features of a specific concept pair and have to predict whether this pair will receive more or fewer IR citations (IR = 10 or IR = 50) in certain future years. To achieve this, the four models are trained on 2-, 3-, and 4-year intervals and evaluated on intervals ranging from 1 to 5 years. For example, if the training interval spans 2 years and the evaluation interval spans 5 years, the models are trained using data from 2015 to 2017 to predict whether unconnected concept pairs will receive IR citations. They are then evaluated using 2017 data to make predictions for 2022. After training, the models are evaluated on 1 million concept pairs, predicting the likelihood of each pair receiving more than IR citations. These predictions are ranked (from high to low likelihood) and compared against the ground truth to compute the ROC curve and the AUC score, which measures prediction quality. As shown in Fig. 6, the models achieve AUC values exceeding 90% in these tasks. In a slightly modified task, we train the models on the data from 2014 to 2017 and evaluate them from 2017 to 1-5 years into the future (2018 to 2022). This test analyzes how well the prediction perform for intervals on which the models have not been trained and how difficult it is to predict further into the future. As shown in Fig. 7, the quality of the predictions indeed decreases for larger intervals.In Fig. 8, we show that models trained on smaller impact ranges can predict higher impact ranges even slightly better than those trained exclusively on higher impact ranges. This might be due to more diverse training data (there are many more examples of IR = 10 than of IR = 50, because many more concept pairs achieve at least 10 citations rather than 50). It might also be explained by a systematic drift in citation patterns between the years the models were trained and the years they are applied, potentially due to the growing number of overall citations. It will be very interesting in the future to explore and understand this effect further and to develop new ML methods that could leverage this dynamic.All of our models use the same set of input features and do not directly access the full knowledge graph. Developing techniques that can leverage more general graph properties -such as automatically learning features or generating embeddings -would be an interesting avenue to explore. A related approach was demonstrated in a previous competition [4], where the task was to predict the future state of a knowledge graph. There, the graph's edges represented co-occurrences of concepts in scientific papers. In contrast, the knowledge graph used in our study is more complex, with edges also weighted by the number of citations received by concept pairs. Exploring more end-to-end approaches that integrate more information from the entire knowledge graph could reveal whether such methods can outperform the models and hand-crafted features demonstrated in this work.
DISCUSSIONWe show how to forecast the impact of future research topics. Although we view this as a significant step towards developing truly useful AI-driven assistants, achieving this goal requires numerous further advancements. Firstly, developing methods to extract more complex information from each paper will be crucial, for instance by employing hyper-graph structures that carry more information from each paper [55,56], which has already been demonstrated to lead to exciting results in other domains [2,57,58]. This might also allow for the forecast of new concepts [59,60] and their impact. Incorporating the recent dataset [61,62] into our research could also allow us to explore more complex data structures than those used in our paper. Secondly, it will be interesting to approximate impact with metrics that go beyond citations -which is a crucial topic in computational sociology and the study of the science of science [1,2]. Additionally, introducing metrics of surprise, as discussed in [63,64], could serve as a complementary metric to citation prediction for ranking suggestions. Finally, while the suggestion of impactful new ideas might be a key component of future AI assistants, it will be crucial to study its relation to the scientific interest of working researchers [65].
MATERIALS AND METHODSDatasets for knowledge graph construction -To compile a list of scientific concepts in natural science, we used metadata from four major preprint servers: arXiv, bioRxiv, medRxiv, and chemRxiv. The arXiv dataset can be directly downloaded from Kaggle, while metadata from bioRxiv, medRxiv, and chemRxiv are accessible through their APIs. The full methodology and codes are available on the GitHub: Impact4Cast. Our comprehensive dataset encompasses approximately 2.44 million papers, including 78,084 from arXiv's physics.optics and quant-ph categories, which were specifically utilized for identifying domain concepts.For edge generation, we used the OpenAlex database snapshot, available for download in OpenAlex bucket. More details can be found at the OpenAlex documentation site. The complete dataset occupies around 330 GB, expanding to approximately 1.6 TB when decompressed. Our interest was specifically in scientific journal papers that include publication time, title, abstract, and citation information. By focusing on these criteria, we managed to reduce the dataset to a more manageable gzip-compressed size of 68 GB, comprising around 92 million scientific papers. From these 92 million papers, 21 million contain at least two concepts of our final concept list and can therefore for an edge in the knowledge graph.Details on concept and edge generation -From the preprint dataset of ∼2.44 million papers, we analyzed each article's title and abstract using the RAKE algo-rithm, enhanced with additional stopwords, to extract potential concept candidates. These candidates were stored for subsequent analysis. We filtered out concepts to retain only those with two words that appeared in nine or more articles, and those with three or more words that appeared in six or more articles. This step significantly reduced the noise from the RAKE-generated concepts, yielding a refined list of 726,439 relevant concepts. To further enhance the quality of the identified concepts, we developed a suite of automatic tools designed to identify and eliminate common, domain-independent errors often associated with RAKE. In addition, we conduct a manual review to identify and eliminate any inaccuracies in the concepts. The entire process, which included eliminating non-conceptual phrases, verbs, ordinal numbers, conjunctions, and adverbials, resulted in a full list of 368,825 concepts.We then specifically focused on articles within the physics.optics and quant-ph categories from arXiv to extract domain-specific concepts. Iterating this entire list of concepts to these domain-specific articles, we identified 87,741 relevant concepts. Employing our specially designed automated filtering tool for initial refinement and then conducting a thorough manual review to remove inaccuracies, we narrowed the list down to 37,960 high-quality, domain-specific concepts.As an example, we show the extraction of concepts for the four papers used in Fig. 1  We created concept pairs, or edges, from the Ope-nAlex dataset, by detecting when domain-specific concepts co-occurred in paper titles or abstracts. This yielded 193,977,096 concept pairs (including multi-edges) across about 21 million papers. Each edge receives a time-stamp based on its paper's publication date, converted to the number of days since January 1, 1990. The final full knowledge graph comprises 26,010,946 unique edges after merging multiple edges between the same concept pairs. The citation information for an edge includes the paper's yearly citations from 2012 to 2023, alongside its total citation since publication. The OpenAlex dataset excludes yearly citations older than ten years, hence the focus on this specific ten-year time frame due to the absence of data prior to 2012. For edges formed by multiple papers, the edge weight combines the annual and total citations from all contributing papers.Consider the edge formed by the concepts 'single molecule localization microscopy' and 'neural net', generated from paper p1 [29] published on January 7, 2020. FIG. 11. ROC curve explanation. The ROC curve are plotted for the evaluation data in Fig. 4, which illustrates the performance of binary classifiers discussed in our paper. On the left, for IR = 10, the area under the curve (AUC) is 0.94. The x-axis represents the False Positive Rate (FPR), while the y-axis represents the True Positive Rate (TPR). The curve demonstrates how the classifier's performance changes with variations in the classification threshold, which determines whether a case is classified as Class 0 or Class 1. For instance, at a threshold that gives a classifier with FPR=0.1, the TPR is 0.83, and at FPR=0.3, the TPR is 0.96. Therefore, the ROC curve is more informative than just a single pair of FPR and TPR. On the right, the ROC curves for IR=10, 50, and 100 are shown.The time-stamp for this edge is derived from the days elapsed since January 1, 1990. The citation metrics for this edge includes the total and yearly citations from each contributing paper. Paper p1 with 38 citations (c p1 =38), contributes yearly citations represented as c p1 (y i ) for i=2023,2022,..., 2012, with actual values {5, 8, 16, 9} for 2023 to 2020, and zeros for previous years, culminating in a citation sequence {5, 8, 16, 9, 0, 0, 0, 0, 0, 0, 0, 0}. Similarly, paper p2 [30], published on March 20, 2020, with 43 citations (c p2 =43), adds its yearly citations {6, 16, 14, 7} for the same period (2023 to 2020). The aggregated citation data for this edge, combining c p1 and c p2 , yields a total of 81 citations, with an annual citation sequence of {11, 24, 30, 16, 0, 0, 0, 0, 0, 0, 0, 0}.Training -Our neural network consists of six fully connected layers, which include four hidden layers with 600 neurons each. The network inputs are 141 features for each unconnected concept pair (v i , v j ), denoted as p i,j = (p 1 i,j , p 2 i,j , ..., p 141 i,j ), where each p i,j ∈ R. For instance, p 1 i,j and p 2 i,j represent the vertex degree of concepts v i and v j for the current year, y. Detailed feature description and feature generation code are available in GitHub: Impact4Cast.In our training process for the year 2016 to predict impact in 2019, we prepared a dataset comprising approximately 689 million unconnected concept pairs. The goal was to evaluate these pairs to determine whether their 3-year citation counts would have at least IR citations (IR is impact range) or not. From this extensive collection, we selected all positive samples (the 3-year citation counts are at least IR). An equivalent number of negative samples were then randomly chosen to match across the years y, y−1, and y−2, and the cosine similarity coefficient for unconnected pairs (u, v) in year y (i.e., y=2016), with AUC scores of 0.8880, 0.8795, 0.8720, and 0.8683, respectively. In contrast, the lowest predictive three features are the average total citation count up to year y for vertex v, and the total citation count for the pair (u, v) up to years y − 1 and y − 2, with AUC scores of 0.5219, 0.5234, and 0.5285. Using all 141 features together leads to a significant improvement in the AUC score to 0.948, showing that the combination of all features works better.the size of the positive set. The refined dataset was subsequently divided, allocating 85% for training and 15% for testing purposes. For the evaluation dataset in 2019, which aims to predict the impact in 2022, we randomly selected 10 million unconnected pairs. Our neural network was trained using the Adam optimizer with a learning rate of 3 × 10 −5 and a mini-batch size of 1000. In every training batch, we randomly chose an equal number of positive and negative samples from the training set. This approach was also applied to our 2019 training process for predictions into 2022, where the trained neural network is used for future forecasting. An example loss curve is shown in Fig. 9. An example for the number of positive and negative cases of the evaluation dataset (for the experiments in Fig. 4) is shown in Fig. 10. In Fig. 11 we explain more details of the ROC curve, specifically its relation to false positive rate (FPR) and true positive rate (TPR).The full dynamic knowledge graph, along with the data required for feature preparation and evaluation, was processed on an Intel Xeon Gold 6130 CPU with 1 TiB of RAM. However, it is not strictly necessary to have 1 TiB of RAM for this process with the relatively small concept list of 37,960; the high memory capacity was utilized for efficiency and to handle additional operations concurrently. The final domain knowledge graph in this work occupies approximately 23.12 GB of storage. It is worth noting that knowledge graphs built from larger concept lists will require more memory, as the data size and complexity increase.The neural network training was conducted on a standard single GPU (Nvidia Quadro RTX 6000), with each training run for different impact ranges taking approximately 1.5 hours. For benchmarking, all models -except the transformer model -were run on a standard CPU (Intel Xeon Gold 6130) with memory usage below 15 GB. Each benchmarking task took roughly one hour to complete. The transformer model, however, was run on a single GPU (Nvidia Quadro RTX 6000), taking approximately 3 hours per task.Individual feature's predictive ability -In Fig. 4 (a), we observe an AUC score of 0.948 for the 2019 evaluation dataset with the neural network that uses all 141 features, trained on 2016 dataset and impact range IR = 100. To explore the predictive ability of individual features, we trained separate neural networks on each feature using the same 2016 dataset, and then applied the 2019 evaluation dataset to these models. This resulted in 141 individual predictions, each from a network trained on a single feature. The features were ranked by their impact predictions, shown in the Fig. 12.FIG. 1 .1FIG.1. Generation of the knowledge graph with time and citation information. Vertices are formed by scientific concepts, which are extracted from scientific papers (titles and abstracts) from prominent academic preprint servers. Edges are formed when concepts are investigated jointly in a scientific publication. There are 21,165,421 out of 92,764,635 papers from OpenAlex which form at least one edge. The edges are augmented with citation information, which acts as a proxy for impact in our work. A mini-knowledge graph (blue edges) is constructed from four randomly selected papers (p1-p4)[29][30][31][32] from OpenAlex as an example. Here, cp4 represents the total citations of paper p4 since its publication, and cp4(y) is its annual citations from 2018 to 2022 (e.g., cp4(2018) = 4). The citation value of the edge is the sum of the all papers creating the edge.
FIG. 2 .2FIG.2. Fastest growing citations of concepts and concept pairs: Evolution of citations over three years for the topfastest growing, previously uncited concepts (a) and concept pairs (b). We find many revolutionary topics in the realm of quantum physics and optics research in the last decade, including Perovskite devices[33], the emergence of complex and nonhermitian topology[34], the introduction of advanced concepts of machine learning in physics[35][36][37] and quasi-BIC (bound state in continuum) resonances.
FIG. 4 .4FIG. 4. Evaluating the machine-learning-based impact forecast. (a): Classification of unconnected pairs, whether they will exceed a certain threshold three years later. Training data contains unconnected vertex pairs from 2016 and the supervision signal according to their impact range IR 3 years later. The test dataset also includes pairs from 2016, but excludes those in the training set. A more challenging evaluation set contains unconnected pairs from 2019, with outcomes verified in 2022, importantly noting that the neural network was only trained on data from 2016 to 2019, not 2019 to 2022.We quantify the quality using the AUC of the ROC curve. For example, IR = 100, i.e, (< 100, >= 100), refers to whether the 3-year citation counts after 2016 (test) or after 2019 (eval) is at least 100. TPR (true positive rate) measures how often a test correctly identifies a true positive, while FPR (false positive rate) measures how often it correctly identifies a true negative. (b): Sorted predictions of the neural network on the evaluation set (blue curve in (a)) shows the very high quality prediction at the level of individual concept pairs. The y-axis stands for the respective fraction of the evaluation dataset (10 7 data points). The histogram is separated into 20 equal bins. No fitting is involved. In (c), we show the average citation of the first N highest predicted concept pairs. This plot shows impressively that the highest predicted concept pairs indeed have very high citation, more than 3 orders of magnitude higher than the average citation of all 10 7 pairs (0.029 citations). (d): This more challenging step shows that citation prediction goes beyond link predictions. Here we take unconnected vertex pairs, conditioned on a connection 3 years later. The neural network is tasked to classify these concept pairs in low or high citations, revealing that it is not just predicting new links, but is learning intrinsic citation features. Here IR = [5, 100], i.e, (0 − 5, >= 100), means whether the 3-year citation count after 2016 (test) or after 2019 (eval) is at most 5 or at least 100.
FIG. 5 .5FIG. 5. Network features vs. predicted impact. A randomly selected set of 100,000 unconnected pairs until January 2023 is used. The color represents the neural network's prediction of each concept pair's impact. (a): The y-axis shows cosine similarity, indicating the semantic similarity between concepts; lower values represent concept pairs that are semantically distinct. The x-axis is the average vertex degree of the two concepts in the knowledge graph, reflecting their overall prominence. Concepts with low similarity and low degree yet predicted to have high impact could be surprising and offer interesting suggestions. (b): The x-axis represents the average number of new neighbors each concept gained over the last three years. Concept pairs with low similarity and few new neighbors but high impact predictions might highlight potentially overlooked but intriguing ideas. (c): The x-axis denotes citation density (average citations per paper mentioning the concepts). Pairs with low similarity and citation density but high predicted impact could again indicate overlooked potential ideas. (d): Citation counts for concept 1 (x-axis) and concept 2 (y-axis) over last three years are plotted on a logarithmic scale. We can easily identify concept pairs predicted to have high impact in the future, even though they have individually received few citations in the past.
5 (5FIG.6. Benchmarking fully connected NNs, Transformers, Random Forest, and XGBoost on 27 variations of the prediction task, with 2-4 year training and 1-5 year evaluation intervals, across two different impact ranges (IR).
FIG. 9 .9FIG.9. Loss curves for one typical example. The training and test loss curves correspond to a fully connected neural network trained on data from 2017 to 2020, with an impact range (IR) of 10.
FIG. 12 .12FIG. 12. Neural network performance across individual features.The highest-performing four features are the Simpson similarity coefficient for the unconnected pair (u, v) across the years y, y−1, and y−2, and the cosine similarity coefficient for unconnected pairs (u, v) in year y (i.e., y=2016), with AUC scores of 0.8880, 0.8795, 0.8720, and 0.8683, respectively. In contrast, the lowest predictive three features are the average total citation count up to year y for vertex v, and the total citation count for the pair (u, v) up to years y − 1 and y − 2, with AUC scores of 0.5219, 0.5234, and 0.5285. Using all 141 features together leads to a significant improvement in the AUC score to 0.948, showing that the combination of all features works better.



ACKNOWLEDGEMENTSThe authors thank Burak Gurlek for interesting discussions at the start of this project, and the organizers of OpenAlex, arXiv, bioRxiv, chemRxiv, and medRxiv for making scientific resources freely and readily accessible. X.G acknowledges the support from the Alexander von Humboldt Foundation.


Data and Code availability: Data is accessible on Zenodo at https://doi.org/10.5281/zenodo.10692137 [66]. Benchmark data used in our work is also available on Zenodo at https://doi.org/10.5281/zenodo.14527306 [67]. Codes for this work are available on GitHub at https://github.com/artificial-scientist-lab/Impact4Cast.


Author Contributions X.G. and M.K. designed research; X.G. performed research and analyzed data; and X.G. and M.K. wrote the manuscript. Competing Interests The authors declare that they have no competing financial interests. 






SFortunato


CTBergstrom


KBörner


JAEvans


DHelbing


SMilojević


AMPetersen


FRadicchi


RSinatra


BUzzi

10.1126/science.aao0185


Science of science

359
185
2018


Science




DWang


A.-LBarabási

The science of science

Cambridge University Press
2021





Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases

LBornmann


RHaunschild


RMutz

10.1057/s41599-021-00903-w


Humanities and Social Sciences Communications

8
1
2021





Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network

MKrenn


LBuffoni


BCoutinho


SEppel


JGFoster


AGritsevskiy


HLee


YLu


JPMoutinho


NSanjabi

10.1038/s42256-023-00735-0


Nature Machine Intelligence

5
1326
2023





arXiv:2303.08774
OpenAI, Gpt-4 technical report

2023






GeminiGoogle

arXiv:2312.11805
family of highly capable multimodal models

2023






HTouvron


LMartin


KStone


PAlbert


AAlmahairi


YBabaei


NBashlykov


SBatra


PBhargava


SBhosale

arXiv:2307.09288
Llama 2: Open foundation and finetuned chat models

2023





Learning to generate novel scientific directions with contextualized literature-based discovery

QWang


DDowney


HJi


THope

arXiv:2305.14259

2023





Choosing experiments to accelerate collective discovery

ARzhetsky


JGFoster


ITFoster


JAEvans

10.1073/pnas.150975711


Proc. Natl. Acad. Sci. USA

112
14569
2015





A survey of link prediction in complex networks

VMartínez


FBerzal


J.-CCubero

10.1145/3012704


ACM computing surveys (CSUR)

49
1
2016





Predicting research trends with semantic and neural networks with an application in quantum physics

MKrenn


AZeilinger

10.1073/pnas.1914370116


Proc. Natl. Acad. Sci. USA

117
1910
2020





Challenge the impact factor
10.1038/s41551-017-0103


Nature Biomedical Engineering

1
103
2017





The many facets of impact
10.1038/s42254-024-00696-2


Nature Reviews Physics

6
71
2024





The formula: The universal laws of success

A.-LBarabási


2018
Hachette UK





The evolution of citation graphs in artificial intelligence research

MRFrank


DWang


MCebrian


IRahwan

10.1038/s42256-019-0024-5


Nature Machine Intelligence

1
79
2019





Quantifying longterm scientific impact

DWang


CSong


A.-LBarabási

10.1126/science.1237825


Science

342
127
2013





Defining and identifying sleeping beauties in science

QKe


EFerrara


FRadicchi


AFlammini

10.1073/pnas.1424329112


Proc. Natl. Acad. Sci. USA
Natl. Acad. Sci. USA

2015
112
7426





Quantifying the evolution of individual scientific impact

RSinatra


DWang


PDeville


CSong


A.-LBarabási

10.1126/science.aaf5239


Science

354
5239
2016





Large teams develop and small teams disrupt science and technology

LWu


DWang


JAEvans

10.1038/s41586-019-0941-9


Nature

566
378
2019





Learning on knowledge graph dynamics provides an early warning of impactful research

JWWeis


JMJacobson

10.1038/s41587-021-00907-6


Nature Biotechnology

39
1300
2021





An overview on evaluating and predicting scholarly article impact

XBai


HLiu


FZhang


ZNing


XKong


ILee


FXia

10.3390/info8030073


Information

8
73
2017





A review of scientific impact prediction: tasks, features and methods

WXia


TLi


CLi

10.1007/s11192-022-04547-8


Scientometrics

128
543
2023





Using content-based and bibliometric features for machine learning models to predict citation counts in the biomedical literature

LFu


CAliferis

10.1007/s11192-010-0160-5


Scientometrics

85
257
2010





Citation impact prediction for scientific papers using stepwise regression analysis

TYu


GYu


P.-YLi


LWang

10.1007/s11192-014-1279-6


Scientometrics

101
1233
2014





Predicting the long-term citation impact of recent publications

CStegehuis


NLitvak


LWaltman

10.1016/j.joi.2015.06.005


Journal of informetrics

9
642
2015





Learning to predict citationbased impact measures

LWeihs


OEtzioni



2017 ACM/IEEE joint conference on digital libraries (JCDL)

IEEE
2017






Predicting the citation counts of individual papers via a bp neural network

XRuan


YZhu


JLi


YCheng

10.1016/j.joi.2020.101039


Journal of Informetrics

14
101039
2020






GHe


ZXue


ZJiang


YKang


SZhao


WLu

arXiv:2305.01572
H2cgl: Modeling dynamics of citation network for impact prediction

2023





Accurate and rapid background estimation in singlemolecule localization microscopy using the deep neural network bgnet

LMöckl


ARRoy


PNPetrov


WMoerner

10.1073/pnas.1916219117


Proc. Natl. Acad. Sci. USA

117
60
2020





Machine learning for cluster analysis of localization microscopy data

DJWilliamson


GLBurn


SSimoncelli


JGriffié


RPeters


DMDavis


DMOwen

10.1038/s41467-020-15293-x


Nature communications

11
1493
2020





Constraints on cosmic strings using data from the first advanced ligo observing run

BPAbbott

LIGO Scientific Collaboration ; Virgo Collaboration



RAbbott

LIGO Scientific Collaboration ; Virgo Collaboration



TDAbbott

LIGO Scientific Collaboration ; Virgo Collaboration



FAcernese

LIGO Scientific Collaboration ; Virgo Collaboration



KAckley

LIGO Scientific Collaboration ; Virgo Collaboration



CAdams

LIGO Scientific Collaboration ; Virgo Collaboration



TAdams

LIGO Scientific Collaboration ; Virgo Collaboration



PAddesso

LIGO Scientific Collaboration ; Virgo Collaboration



RXAdhikari

LIGO Scientific Collaboration ; Virgo Collaboration



VBAdya

LIGO Scientific Collaboration ; Virgo Collaboration


10.1103/PhysRevD.97.102002


Phys. Rev. D

97
102002
2018





Learning phase transitions from dynamics

EVan Nieuwenburg


EBairey


GRefael

10.1103/PhysRevB.98.060301


Phys. Rev. B

98
60301
2018





Challenges for commercializing perovskite solar cells

YRong


YHu


AMei


HTan


MISaidaminov


SISeok


MDMcgehee


EHSargent


HHan

10.1126/science.aat8235


Science

361
8235
2018





Exceptional topology of non-hermitian systems

EJBergholtz


JCBudich


FKKunst

10.1103/RevModPhys.93.015005


Rev. Mod. Phys

93
15005
2021





Machine learning and the physical sciences

GCarleo


ICirac


KCranmer


LDaudet


MSchuld


NTishby


LVogt-Maranto


LZdeborová

10.1103/RevModPhys.91.045002


Rev. Mod. Phys

91
45002
2019





Artificial intelligence and machine learning for quantum technologies

MKrenn


JLandgraf


TFoesel


FMarquardt

10.1103/PhysRevA.107.010101


Phys. Rev. A

107
10101
2023





Scientific discovery in the age of artificial intelligence

HWang


TFu


YDu


WGao


KHuang


ZLiu


PChandak


SLiu


PVan Katwyk


ADeac

10.1038/s41586-023-06221-2


Nature

620
47
2023





microscopical observations on the blood vessels and membranes of the intestines. in a letter to the royal society from mr. anthony van leeuwenhoek, frs

AVLeeuwenhoek


Ii

10.1098/rstl.1708.0007


Philosophical Transactions of the Royal Society of London

26
53
1709





On scientific understanding with artificial intelligence

MKrenn


RPollice


SYGuo


MAldeghi


ACervera-Lierta


PFriederich


GDos Passos


FGomes


AHäse


AJinich


Nigam

10.1038/s42254-022-00518-3


Nature Reviews Physics

4
761
2022





The cso classifier: Ontology-driven detection of research topics in scholarly articles

AASalatino


FOsborne


TThanapalasingam


EMotta

10.1007/978-3-030-30760-8_26


Digital Libraries for Open Knowledge: 23rd International Conference on Theory and Practice of Digital Libraries, TPDL 2019
Oslo, Norway

Springer
September 9-12, 2019. 2019
23






Automatic keyword extraction from individual documents, Text mining: applications and theory

SRose


DEngel


NCramer


WCowley

10.1002/9780470689646.ch1

2010
1






JPriem


HPiwowar


ROrr

arXiv:2205.01833
Openalex: A fullyopen index of scholarly works, authors, venues, institutions, and concepts

2022





The pagerank citation ranking : Bringing order to the web

LPage


SBrin


RMotwani


TWinograd


1999
Stanford InfoLab






A.-LBarabási

Network Science

Cambridge University Press
2016





Predicting research trends in artificial intelligence with gradient boosting decision trees and time-aware graph neural networks

YLu



2021 IEEE International Conference on Big Data (Big Data

IEEE
2021






Roc graphs: Notes and practical considerations for researchers

TFawcett



Machine learning

31
1
2004





Neural machine translation by jointly learning to align and translate

DBahdanau


KCho


YBengio



International Conference on Learning Representations (ICLR

2015





Attention is all you need

AVaswani


NShazeer


NParmar


JUszkoreit


LJones


ANGomez


LUKaiser


IPolosukhin



Advances in Neural Information Processing Systems

IGuyon


UVLuxburg


SBengio


HWallach


RFergus


SVishwanathan


RGarnett


Curran Associates, Inc
2017
30





Induction of decision trees

JRQuinlan

10.1007/BF00116251


Machine Learning

1
81
1986





Xgboost: A scalable tree boosting system

TChen


CGuestrin



Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining
the 22nd acm sigkdd international conference on knowledge discovery and data mining

2016






Pytorch: An imperative style, highperformance deep learning library

APaszke


SGross


FMassa


ALerer


JBradbury


GChanan


TKilleen


ZLin


NGimelshein


LAntiga


ADesmaison


AKopf


EYang


ZDevito


MRaison


ATejani


SChilamkurthy


BSteiner


LFang


JBai


SChintala



Advances in Neural Information Processing Systems

HWallach


HLarochelle


ABeygelzimer


FAlché-Buc


EFox


RGarnett


Curran Associates, Inc
2019
32





Rectified linear units improve restricted boltzmann machines

VNair


GEHinton

10.5555/3104322.3104425


International conference on machine learning (ICML)

2010





Adam: A method for stochastic optimization

DPKingma



International Conference on Learning Representations (ICLR)

2015





Scikit-learn: Machine learning in python

FPedregosa


GVaroquaux


AGramfort


VMichel


BThirion


OGrisel


MBlondel


PPrettenhofer


RWeiss


VDubourg



Journal of machine learning research

12
2825
2011





The physics of higher-order interactions in complex systems

FBattiston


EAmico


ABarrat


GBianconi


GFerraz De Arruda


BFranceschiello


IIacopini


SKéfi


VLatora


YMoreno

10.1038/s41567-021-01371-4


Nature Physics

17
1093
2021





Prediction of robust scientific facts from literature

AVBelikov


ARzhetsky


JEvans

10.1038/s42256-022-00474-8


Nature Machine Intelligence

4
445
2022





Tradition and innovation in scientists' research strategies

JGFoster


ARzhetsky


JAEvans

10.1177/0003122415601618


American Sociological Review

80
875
2015





Accelerating science with human-aware artificial intelligence

JSourati


JAEvans

10.1038/s41562-023-01648-z


Nature Human Behaviour

7
1682
2023





How are topics born? understanding the research dynamics preceding the emergence of new areas

AASalatino


FOsborne


EMotta

10.7717/peerj-cs.119


PeerJ Computer Science

3
e119
2017





Augur: forecasting the emergence of new research topics

AASalatino


FOsborne


EMotta

10.1145/3197026.3197052


Proceedings of the 18th ACM/IEEE on joint conference on digital libraries
the 18th ACM/IEEE on joint conference on digital libraries

2018






Sciscinet: A largescale open data lake for the science of science research

ZLin


YYin


LLiu


DWang

10.1038/s41597-023-02198-9


Scientific Data

10
315
2023





A dataset of publication records for nobel laureates

JLi


YYin


SFortunato


DWang

10.1038/s41597-019-0033-6


Scientific Data

6
33
2019





Surprise! measuring novelty as expectation violation

JGFoster


FShi


JEvans

10.31235/osf.io/2t46f


SocArXiv

2
46
2021





Surprising combinations of research contents and contexts are related to impact and emerge with scientific outsiders from distant disciplines

FShi


JEvans

10.1038/s41467-023-36741-4


Nature Communications

14
1641
2023





Interesting scientific idea generation using knowledge graphs and llms

XGu


MKrenn

arXiv:2405.17044


Evaluations with 100 research group leaders

2024





Impact4cast: Forecasting high-impact research topics via machine learning on evolving knowledge graphs [data set

XGu

10.5281/zenodo.10692137


2024





Benchmark dataset for impact4cast: Forecasting high-impact research topics via machine learning on evolving knowledge graphs [data set

XGu

10.5281/zenodo.14527306


2024







