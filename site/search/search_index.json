{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AI and Open Science in Research Software Engineering \u00b6 This project, developed as part of my Computer Science coursework, focuses on the intersection of Artificial Intelligence and Open Science in the field of Research Software Engineering. he objective is to apply text analysis techniques to open-access research articles using Grobid. Key tasks include: Extracting and processing the full text from a set of 10 open-access research papers. Generating insightful keyword clouds based on abstracts to visualize key themes. Visualizing the number of figures per article to understand the use of visuals in research. Compiling a comprehensive list of links extracted from the papers, offering a deeper look into the references and connections within the research. The project is designed to extract the data such as figures, links, and metadata from research papers, and it can be run either through Docker or a virtual environment. Recommended \u00b6 Ubuntu 22.04 License \u00b6 Distributed under the MIT License . See LICENSE for more information. Author: Andrei Iulian Grozescu Contact: (andrei.grozescu@alumnos.upm.es)","title":"Home"},{"location":"#ai-and-open-science-in-research-software-engineering","text":"This project, developed as part of my Computer Science coursework, focuses on the intersection of Artificial Intelligence and Open Science in the field of Research Software Engineering. he objective is to apply text analysis techniques to open-access research articles using Grobid. Key tasks include: Extracting and processing the full text from a set of 10 open-access research papers. Generating insightful keyword clouds based on abstracts to visualize key themes. Visualizing the number of figures per article to understand the use of visuals in research. Compiling a comprehensive list of links extracted from the papers, offering a deeper look into the references and connections within the research. The project is designed to extract the data such as figures, links, and metadata from research papers, and it can be run either through Docker or a virtual environment.","title":"AI and Open Science in Research Software Engineering"},{"location":"#recommended","text":"Ubuntu 22.04","title":"Recommended"},{"location":"#license","text":"Distributed under the MIT License . See LICENSE for more information. Author: Andrei Iulian Grozescu Contact: (andrei.grozescu@alumnos.upm.es)","title":"License"},{"location":"installation/","text":"Setting Up the Environment \u00b6 Virtual Environment with Venv \u00b6 To use venv for creating the virtual environment, first, you need to install the tool by running: python3 -m pip install --user virtualenv Once installed, create the virtual environment with: python3 -m venv your_environment_name Then, activate it: source your_environment_name/bin/activate With the environment activated, you need to install the necessary modules for running the program. To do so, use the provided requirements.txt file to install all the dependencies: pip install -r requirements.txt Virtual Environment with Conda \u00b6 If you prefer to use conda, you need to install Anaconda first. After installing, create the environment using the environment.yml file with: conda env create -f environment.yml Then, activate the environment: conda activate your_environment_name Once everything is installed, you can run pip freeze to check that all dependencies have been installed correctly and proceed to execution. Docker Installation Guide \u00b6 Prerequisites \u00b6 Before running the project using Docker, ensure you have the following installed: Docker : Install it from the official Docker website . 1. Clone the Repository \u00b6 Clone the project repository from GitHub: git clone https://github.com/andreigrozescu/open-science-ai-rse.git cd open-science-ai-rse 2. Build the Docker Image \u00b6 Since the Dockerfile is already provided in the repository, run the following command to build the Docker image: docker build -t open-science-ai-rse . This command: Reads the Dockerfile. Pulls the Python 3.10 base image. Installs dependencies. Copies the project files. Creates the image open-science-ai-rse . 3. Verify the Image \u00b6 Once the build is complete, verify that the image was created successfully: docker images You should see open-science-ai-rse listed. 4. Run the Grobid Server (Required for Processing) \u00b6 Start the Grobid server before running the analysis: docker run -d --name grobid-server -p 8070 :8070 -p 8071 :8071 lfoppiano/grobid:0.8.0 This command runs the Grobid server in the background ( -d ). 5. Run the Analysis Container \u00b6 Now, run the project container, connecting it to the Grobid server: docker run --rm --name paper_analysis --network = \"host\" -v $( pwd ) /papers:/app/papers -v $( pwd ) /output:/app/output open-science-ai-rse Explanation: \u00b6 --rm : Deletes the container after execution. --network=\"host\" : Ensures the container can communicate with Grobid. -v $(pwd)/papers:/app/papers : Maps the local papers/ folder to the container. -v $(pwd)/output:/app/output : Stores results in the local output/ folder.","title":"Installation"},{"location":"installation/#setting-up-the-environment","text":"","title":"Setting Up the Environment"},{"location":"installation/#virtual-environment-with-venv","text":"To use venv for creating the virtual environment, first, you need to install the tool by running: python3 -m pip install --user virtualenv Once installed, create the virtual environment with: python3 -m venv your_environment_name Then, activate it: source your_environment_name/bin/activate With the environment activated, you need to install the necessary modules for running the program. To do so, use the provided requirements.txt file to install all the dependencies: pip install -r requirements.txt","title":"Virtual Environment with Venv"},{"location":"installation/#virtual-environment-with-conda","text":"If you prefer to use conda, you need to install Anaconda first. After installing, create the environment using the environment.yml file with: conda env create -f environment.yml Then, activate the environment: conda activate your_environment_name Once everything is installed, you can run pip freeze to check that all dependencies have been installed correctly and proceed to execution.","title":"Virtual Environment with Conda"},{"location":"installation/#docker-installation-guide","text":"","title":"Docker Installation Guide"},{"location":"installation/#prerequisites","text":"Before running the project using Docker, ensure you have the following installed: Docker : Install it from the official Docker website .","title":"Prerequisites"},{"location":"installation/#1-clone-the-repository","text":"Clone the project repository from GitHub: git clone https://github.com/andreigrozescu/open-science-ai-rse.git cd open-science-ai-rse","title":"1. Clone the Repository"},{"location":"installation/#2-build-the-docker-image","text":"Since the Dockerfile is already provided in the repository, run the following command to build the Docker image: docker build -t open-science-ai-rse . This command: Reads the Dockerfile. Pulls the Python 3.10 base image. Installs dependencies. Copies the project files. Creates the image open-science-ai-rse .","title":"2. Build the Docker Image"},{"location":"installation/#3-verify-the-image","text":"Once the build is complete, verify that the image was created successfully: docker images You should see open-science-ai-rse listed.","title":"3. Verify the Image"},{"location":"installation/#4-run-the-grobid-server-required-for-processing","text":"Start the Grobid server before running the analysis: docker run -d --name grobid-server -p 8070 :8070 -p 8071 :8071 lfoppiano/grobid:0.8.0 This command runs the Grobid server in the background ( -d ).","title":"4. Run the Grobid Server (Required for Processing)"},{"location":"installation/#5-run-the-analysis-container","text":"Now, run the project container, connecting it to the Grobid server: docker run --rm --name paper_analysis --network = \"host\" -v $( pwd ) /papers:/app/papers -v $( pwd ) /output:/app/output open-science-ai-rse","title":"5. Run the Analysis Container"},{"location":"installation/#explanation","text":"--rm : Deletes the container after execution. --network=\"host\" : Ensures the container can communicate with Grobid. -v $(pwd)/papers:/app/papers : Maps the local papers/ folder to the container. -v $(pwd)/output:/app/output : Stores results in the local output/ folder.","title":"Explanation:"},{"location":"scripts/","text":"Document Processing and Analysis Script \u00b6 Description \u00b6 This Python script is designed to process a set of PDFs, extract relevant information, and generate visualizations based on the extracted data. The script performs the following tasks: Process PDF files using Grobid to extract metadata and abstracts. Generate a keyword word cloud from the extracted abstracts. Count and visualize the number of figures in each article. Extract and save links found within the bibliographic structure of each article. Tools Used \u00b6 requests : To interact with the Grobid API for PDF processing. xml.etree.ElementTree : To parse and work with XML documents. matplotlib : For creating visualizations (word cloud and bar chart). wordcloud : For generating the word cloud. tqdm : For displaying a progress bar during the processing of files. Steps: \u00b6 1. Load Configuration \u00b6 The script first loads configuration settings from a config.json file. The configuration contains the URL for the Grobid API and the directories for input, output, and results. with open ( 'scripts/config.json' , 'r' ) as config_file : config = json . load ( config_file ) GRODIB_URL = config . get ( \"grobid_url\" , \"http://localhost:8070/api/processFulltextDocument\" ) 2. Process PDF Files \u00b6 The script processes each PDF file in the papers folder, sending each one to the Grobid service to extract metadata and abstracts. The extracted XML content is saved in the output directory. def process_pdf ( pdf_path ): with open ( pdf_path , 'rb' ) as f : files = { 'input' : f } response = requests . post ( GRODIB_URL , files = files ) if response . status_code == 200 : return response . text else : print ( f \"Error al procesar el archivo { pdf_path } : { response . status_code } \" ) return None 3. Generate Word Cloud \u00b6 Once the XML files are saved, the script parses them to extract abstracts. It concatenates the text of all abstracts to generate a word cloud. The word cloud is then saved as an image in the results directory. def generate_wordcloud (): all_text = \"\" for xml_file in os . listdir ( OUTPUT_DIR ): if xml_file . endswith ( \".xml\" ): tree = ET . parse ( os . path . join ( OUTPUT_DIR , xml_file )) root = tree . getroot () abstracts = root . findall ( './/tei:abstract' , { 'tei' : 'http://www.tei-c.org/ns/1.0' }) for abstract in abstracts : div = abstract . find ( 'tei:div' , { 'tei' : 'http://www.tei-c.org/ns/1.0' }) if div is not None : p = div . find ( 'tei:p' , { 'tei' : 'http://www.tei-c.org/ns/1.0' }) if p is not None and p . text : all_text += p . text + \" \" if all_text : wordcloud = WordCloud ( width = 800 , height = 800 , background_color = 'white' ) . generate ( all_text ) plt . figure ( figsize = ( 8 , 8 )) plt . imshow ( wordcloud , interpolation = 'bilinear' ) plt . axis ( \"off\" ) plt . title ( \"Keyword Cloud\" ) plt . savefig ( os . path . join ( RESULTS_DIR , \"wordcloud.png\" ), format = 'png' ) plt . close () print ( \"Nube de palabras clave generada.\" ) 4. Count and Visualize the Number of Figures \u00b6 The script then counts the number of figures in each article by parsing the XML files and finding tei:figure elements. A bar chart is created to visualize the number of figures per article, which is saved in the results directory. def count_figures (): figures_count = {} for xml_file in os . listdir ( OUTPUT_DIR ): if xml_file . endswith ( \".xml\" ): tree = ET . parse ( os . path . join ( OUTPUT_DIR , xml_file )) root = tree . getroot () figures = root . findall ( './/tei:figure' , { 'tei' : 'http://www.tei-c.org/ns/1.0' }) figures_count [ xml_file ] = len ( figures ) if figures_count : labels = [ ' '.join(textwrap.wrap(label.replace(\".xml\", \"\"), 20)) for label in figures_count.keys()] plt . bar ( range ( len ( figures_count )), list ( figures_count . values ()), align = 'center' ) plt . xticks ( range ( len ( figures_count )), labels , rotation = 90 ) plt . tight_layout () plt . title ( \"Number of Figures per Article\" ) plt . savefig ( os . path . join ( RESULTS_DIR , \"figures_count.png\" ), format = 'png' ) plt . close () print ( \"Visualizaci\u00f3n del n\u00famero de figuras generada.\" ) 5. Extract and Save Links \u00b6 Finally, the script extracts URLs from the bibliographic structures in each XML file. These URLs are saved in a text file in the results directory. def extract_links (): links = {} for xml_file in os . listdir ( OUTPUT_DIR ): if xml_file . endswith ( \".xml\" ): tree = ET . parse ( os . path . join ( OUTPUT_DIR , xml_file )) root = tree . getroot () biblStructs = root . findall ( './/tei:biblStruct' , { 'tei' : 'http://www.tei-c.org/ns/1.0' }) url_list = [] for biblStruct in biblStructs : ptr = biblStruct . find ( './/tei:ptr' , { 'tei' : 'http://www.tei-c.org/ns/1.0' }) if ptr is not None and 'target' in ptr . attrib : url_list . append ( ptr . attrib [ 'target' ]) links [ xml_file ] = url_list with open ( os . path . join ( RESULTS_DIR , \"extracted_links.txt\" ), \"w\" ) as f : for article , urls in links . items (): f . write ( f 'Articulo: { article } Links : { \" \" . join ( urls )} ') print ( \"Lista de enlaces extra\u00edda y guardada.\" ) Final Steps: \u00b6 The functions generate_wordcloud() , count_figures() , and extract_links() are executed to process the data and generate the desired results. generate_wordcloud () count_figures () extract_links () print ( \"Procesamiento completo.\" ) Files Generated: \u00b6 wordcloud.png : A word cloud image showing the most common words in the abstracts. figures_count.png : A bar chart showing the number of figures in each article. extracted_links.txt : A text file containing a list of URLs found in each article.","title":"Scripts"},{"location":"scripts/#document-processing-and-analysis-script","text":"","title":"Document Processing and Analysis Script"},{"location":"scripts/#description","text":"This Python script is designed to process a set of PDFs, extract relevant information, and generate visualizations based on the extracted data. The script performs the following tasks: Process PDF files using Grobid to extract metadata and abstracts. Generate a keyword word cloud from the extracted abstracts. Count and visualize the number of figures in each article. Extract and save links found within the bibliographic structure of each article.","title":"Description"},{"location":"scripts/#tools-used","text":"requests : To interact with the Grobid API for PDF processing. xml.etree.ElementTree : To parse and work with XML documents. matplotlib : For creating visualizations (word cloud and bar chart). wordcloud : For generating the word cloud. tqdm : For displaying a progress bar during the processing of files.","title":"Tools Used"},{"location":"scripts/#steps","text":"","title":"Steps:"},{"location":"scripts/#1-load-configuration","text":"The script first loads configuration settings from a config.json file. The configuration contains the URL for the Grobid API and the directories for input, output, and results. with open ( 'scripts/config.json' , 'r' ) as config_file : config = json . load ( config_file ) GRODIB_URL = config . get ( \"grobid_url\" , \"http://localhost:8070/api/processFulltextDocument\" )","title":"1. Load Configuration"},{"location":"scripts/#2-process-pdf-files","text":"The script processes each PDF file in the papers folder, sending each one to the Grobid service to extract metadata and abstracts. The extracted XML content is saved in the output directory. def process_pdf ( pdf_path ): with open ( pdf_path , 'rb' ) as f : files = { 'input' : f } response = requests . post ( GRODIB_URL , files = files ) if response . status_code == 200 : return response . text else : print ( f \"Error al procesar el archivo { pdf_path } : { response . status_code } \" ) return None","title":"2. Process PDF Files"},{"location":"scripts/#3-generate-word-cloud","text":"Once the XML files are saved, the script parses them to extract abstracts. It concatenates the text of all abstracts to generate a word cloud. The word cloud is then saved as an image in the results directory. def generate_wordcloud (): all_text = \"\" for xml_file in os . listdir ( OUTPUT_DIR ): if xml_file . endswith ( \".xml\" ): tree = ET . parse ( os . path . join ( OUTPUT_DIR , xml_file )) root = tree . getroot () abstracts = root . findall ( './/tei:abstract' , { 'tei' : 'http://www.tei-c.org/ns/1.0' }) for abstract in abstracts : div = abstract . find ( 'tei:div' , { 'tei' : 'http://www.tei-c.org/ns/1.0' }) if div is not None : p = div . find ( 'tei:p' , { 'tei' : 'http://www.tei-c.org/ns/1.0' }) if p is not None and p . text : all_text += p . text + \" \" if all_text : wordcloud = WordCloud ( width = 800 , height = 800 , background_color = 'white' ) . generate ( all_text ) plt . figure ( figsize = ( 8 , 8 )) plt . imshow ( wordcloud , interpolation = 'bilinear' ) plt . axis ( \"off\" ) plt . title ( \"Keyword Cloud\" ) plt . savefig ( os . path . join ( RESULTS_DIR , \"wordcloud.png\" ), format = 'png' ) plt . close () print ( \"Nube de palabras clave generada.\" )","title":"3. Generate Word Cloud"},{"location":"scripts/#4-count-and-visualize-the-number-of-figures","text":"The script then counts the number of figures in each article by parsing the XML files and finding tei:figure elements. A bar chart is created to visualize the number of figures per article, which is saved in the results directory. def count_figures (): figures_count = {} for xml_file in os . listdir ( OUTPUT_DIR ): if xml_file . endswith ( \".xml\" ): tree = ET . parse ( os . path . join ( OUTPUT_DIR , xml_file )) root = tree . getroot () figures = root . findall ( './/tei:figure' , { 'tei' : 'http://www.tei-c.org/ns/1.0' }) figures_count [ xml_file ] = len ( figures ) if figures_count : labels = [ ' '.join(textwrap.wrap(label.replace(\".xml\", \"\"), 20)) for label in figures_count.keys()] plt . bar ( range ( len ( figures_count )), list ( figures_count . values ()), align = 'center' ) plt . xticks ( range ( len ( figures_count )), labels , rotation = 90 ) plt . tight_layout () plt . title ( \"Number of Figures per Article\" ) plt . savefig ( os . path . join ( RESULTS_DIR , \"figures_count.png\" ), format = 'png' ) plt . close () print ( \"Visualizaci\u00f3n del n\u00famero de figuras generada.\" )","title":"4. Count and Visualize the Number of Figures"},{"location":"scripts/#5-extract-and-save-links","text":"Finally, the script extracts URLs from the bibliographic structures in each XML file. These URLs are saved in a text file in the results directory. def extract_links (): links = {} for xml_file in os . listdir ( OUTPUT_DIR ): if xml_file . endswith ( \".xml\" ): tree = ET . parse ( os . path . join ( OUTPUT_DIR , xml_file )) root = tree . getroot () biblStructs = root . findall ( './/tei:biblStruct' , { 'tei' : 'http://www.tei-c.org/ns/1.0' }) url_list = [] for biblStruct in biblStructs : ptr = biblStruct . find ( './/tei:ptr' , { 'tei' : 'http://www.tei-c.org/ns/1.0' }) if ptr is not None and 'target' in ptr . attrib : url_list . append ( ptr . attrib [ 'target' ]) links [ xml_file ] = url_list with open ( os . path . join ( RESULTS_DIR , \"extracted_links.txt\" ), \"w\" ) as f : for article , urls in links . items (): f . write ( f 'Articulo: { article } Links : { \" \" . join ( urls )} ') print ( \"Lista de enlaces extra\u00edda y guardada.\" )","title":"5. Extract and Save Links"},{"location":"scripts/#final-steps","text":"The functions generate_wordcloud() , count_figures() , and extract_links() are executed to process the data and generate the desired results. generate_wordcloud () count_figures () extract_links () print ( \"Procesamiento completo.\" )","title":"Final Steps:"},{"location":"scripts/#files-generated","text":"wordcloud.png : A word cloud image showing the most common words in the abstracts. figures_count.png : A bar chart showing the number of figures in each article. extracted_links.txt : A text file containing a list of URLs found in each article.","title":"Files Generated:"},{"location":"usage/","text":"Execution Instructions \u00b6 To run the program, simply place the articles you want to process in the papers/ folder. The script will analyze these papers and store the xml results in the output/ directory, and the keyword clouds, figures and links in the results/ directory. Once everything is set up, navigate to the main project folder by running: cd open-science-ai-rse After that, you can execute the main script with the following command: python3 scripts/main.py This will run all the necessary scripts. After the execution, the results will be available in the output/ directory, and additional processed results like figures and links can be found in the results/ directory.","title":"Usage"},{"location":"usage/#execution-instructions","text":"To run the program, simply place the articles you want to process in the papers/ folder. The script will analyze these papers and store the xml results in the output/ directory, and the keyword clouds, figures and links in the results/ directory. Once everything is set up, navigate to the main project folder by running: cd open-science-ai-rse After that, you can execute the main script with the following command: python3 scripts/main.py This will run all the necessary scripts. After the execution, the results will be available in the output/ directory, and additional processed results like figures and links can be found in the results/ directory.","title":"Execution Instructions"}]}